{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boris/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import sys \n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import scipy.special\n",
    "from scipy.stats import linregress\n",
    "from scipy.spatial.distance import jaccard\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from scipy.cluster import hierarchy as hrc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "jaccard_index = jaccard_similarity_score\n",
    "\n",
    "import copy\n",
    "\n",
    "sys.path.append(\"./src/\")\n",
    "import lumberjack\n",
    "import tree_reader as tr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/boris/taylor/vision/rust_prototype/2_forest/trees/johnston_retina/deeper/run.0.compact'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-84ab7557860c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombined_tree_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mraw_forest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./trees/johnston_retina/deeper/run.prediction_truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/boris/taylor/vision/rust_prototype/2_forest/trees/johnston_retina/deeper/run.0.compact'"
     ]
    }
   ],
   "source": [
    "johnston_counts = np.loadtxt(\"/Users/boris/taylor/johnston_retina/single_cell/dmel-retina-scRNA/exploration/2018.07.19_Scanpy/counts.txt\")\n",
    "johnston_header = np.loadtxt(\"/Users/boris/taylor/johnston_retina/single_cell/dmel-retina-scRNA/exploration/2018.07.19_Scanpy/header.txt\",dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts.shape)\n",
    "print(header.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[:10,:10]\n",
    "np.var(counts,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_encoding = node_sample_encoding(first_forest.leaves(),first_forest.dim[0]).T\n",
    "# print(sample_encoding.shape)\n",
    "\n",
    "# distances = coocurrence_distance(sample_encoding)\n",
    "# print(distances.shape)\n",
    "# embedding_model = MDS(n_components=2,dissimilarity='precomputed',metric=False)\n",
    "# coordinates = embedding_model.fit_transform(distances)\n",
    "\n",
    "# distances = coocurrence_matrix(sample_encoding)\n",
    "# print(distances.shape)\n",
    "# embedding_model = KernelPCA(n_components=2,kernel='precomputed')\n",
    "# coordinates = embedding_model.fit_transform(distances)\n",
    "\n",
    "\n",
    "\n",
    "# print(coordinates.shape)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(coordinates[:,0],coordinates[:,1],s=.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsne_coordinates = sample_tsne(first_forest.leaves(),first_forest.dim[0])\n",
    "\n",
    "# hd_clusters = sample_hdbscan(first_forest.leaves(),first_forest.dim[0])\n",
    "# a_clusters = sample_agglomerative(first_forest.leaves(),first_forest.dim[0],13)\n",
    "l_clusters = hacked_louvain(first_forest.leaves(),first_forest.samples)\n",
    "# e_clusters = embedded_hdbscan(tsne_coordinates)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(tsne_coordinates[:,0],tsne_coordinates[:,1],s=.5,c=(l_clusters+1),cmap='rainbow')\n",
    "# plt.scatter(tsne_coordinates[:,0],tsne_coordinates[:,1],s=.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2585, 3228)\n"
     ]
    }
   ],
   "source": [
    "# sample_encoding = node_sample_encoding(first_forest.leaves(),first_forest.dim[0]).T\n",
    "\n",
    "print(sample_encoding.shape)\n",
    "\n",
    "# np.savetxt(\"/Users/boris/haxx/python/sample_encoding.txt\",sample_encoding)\n",
    "\n",
    "# print(squareform(pdist(sample_encoding,metric='cityblock')).shape)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(pdist(sample_encoding,metric='cityblock'),bins=np.arange(0,100,2))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cascading_node_slices_x(node,depth=0):\n",
    "\n",
    "    splits = []\n",
    "    \n",
    "    leaves = node.leaves()\n",
    "    \n",
    "    print \"Cascading\"\n",
    "    print len(leaves)\n",
    "    \n",
    "    total_counts = np.zeros((0,len(node['features'])))\n",
    "\n",
    "    for leaf in leaves:\n",
    "        if len(leaf['children']) > 0:\n",
    "            node_counts = leaf.singly_sorted_counts()\n",
    "            splits.append(total_samples + split_index)\n",
    "        else:\n",
    "            node_counts = \n",
    "        total_samples += node_counts.shape[0]\n",
    "        total_counts = np.concatenate((total_counts,node_counts), axis=0)\n",
    "    \n",
    "    return total_counts, splits\n",
    "\n",
    "\n",
    "    \n",
    "def make_tree_movie(node,counts,header,feature_sort,title,max_depth=1):\n",
    "\n",
    "    splits = []\n",
    "    \n",
    "    for i in range(max_depth):\n",
    "        \n",
    "        split_map, new_splits = cascading_node_slices_x(node,counts,header,depth=i)\n",
    "\n",
    "        plt.figure(figsize=(8,4))\n",
    "        im = plt.imshow(split_map.T[feature_sort].T,aspect='auto')\n",
    "        for split in splits:\n",
    "            plt.plot([0,split_map.shape[1]-1],[split,split],c='xkcd:cyan',linewidth=.25)\n",
    "        for split in new_splits:\n",
    "            plt.plot([0,split_map.shape[1]-1],[split,split],c='blue',linewidth=.25)\n",
    "        splits.extend(new_splits)\n",
    "        plt.savefig('figures/tree_movie/%s.png' % (title + \".\" + str(i)),dpi=500)\n",
    "\n",
    "           \n",
    "node_counts = node_to_naive_counts(first_forest[0],counts,header)\n",
    "\n",
    "gene_linked = hrc.linkage(node_counts.T, method='average', metric='correlation')\n",
    "gene_dendrogram = hrc.dendrogram(gene_linked,no_plot=True)\n",
    "\n",
    "# make_tree_movie(first_forest[0],counts,header,gene_dendrogram['leaves'],\"tree1\",max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len([x for y in tree_to_leaves(first_forest[0],0,) for x in y['samples']])\n",
    "\n",
    "# combined_2nd_tree_files = open('trees_for_clustering/combined/2nd_paths.txt').readlines()\n",
    "\n",
    "# second_layer = []\n",
    "\n",
    "# for tree_file in combined_2nd_tree_files:\n",
    "#     second_layer.append(json.load(open(tree_file.strip())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_forest_leaves = forest_to_leaves(first_forest)\n",
    "# second_forest_leaves = forest_to_leaves(second_layer)\n",
    "\n",
    "print len(first_forest_leaves)\n",
    "# print len(second_forest_leaves)\n",
    "\n",
    "# print [len(x['features']) for x in first_forest_leaves]\n",
    "\n",
    "# print list(alt_node_sample_encoding(first_forest_leaves)[0])\n",
    "# print list(first_forest_leaves[0]['samples'])\n",
    "\n",
    "# print np.sum(alt_node_sample_encoding(first_forest_leaves)[0])\n",
    "# print len(first_forest_leaves[0]['samples'])\n",
    "\n",
    "# for sample in first_forest_leaves[0]['samples']:\n",
    "#     print alt_node_sample_encoding(first_forest_leaves)[0][int(sample)]\n",
    "\n",
    "\n",
    "# print len(first_forest_leaves[0]['samples'])\n",
    "\n",
    "ff_sample_coordinates = node_sample_tsne(first_forest_leaves)\n",
    "# sf_sample_coordinates = node_sample_mds(second_forest_leaves)\n",
    "\n",
    "print \"Tsne is done\"\n",
    "\n",
    "ff_sample_feature_cross_clusters = node_sample_hdbscan(first_forest_leaves)\n",
    "# fs_sample_feature_cross_clusters = node_feature_dbscan(second_forest_leaves)\n",
    "\n",
    "# print ff_sample_coordinates.shape\n",
    "# print sf_sample_coordinates.shape\n",
    "\n",
    "plt.figure(\"first_forest_sample_pca\")\n",
    "plt.title(\"First Layer Forest Node Sample Membership\")\n",
    "plt.scatter(ff_sample_coordinates[:,0],ff_sample_coordinates[:,1],s=.03,c=ff_sample_feature_cross_clusters,cmap=\"rainbow\")\n",
    "plt.savefig(prefix + \"ff_sample_scatter.png\", dpi=300)\n",
    "\n",
    "# plt.figure(\"second_forest_sample_pca\")\n",
    "# plt.title(\"Second Layer Forest Node Sample Membership\")\n",
    "# plt.scatter(sf_sample_coordinates[:,0],sf_sample_coordinates[:,1],s=.03,c=fs_sample_feature_cross_clusters)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute_error = np.loadtxt('trees_for_clustering/combined/impute_error.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.hist([np.sum(clusters == cluster) for cluster in set(list(clusters))],bins=50,log=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(\"embedded_nodes\")\n",
    "# plt.scatter(coordinates[:,0],coordinates[:,1],s=.03,c=clusters,cmap='rainbow')\n",
    "# plt.savefig(\"node_clusters.png\",dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clst = coordinates[:,0] > 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print sum(clst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clst_indecies = np.arange(len(forest_nodes))[clst]\n",
    "# clst_nodes = []\n",
    "\n",
    "# for ind in clst_indecies:\n",
    "#     clst_nodes.append(forest_nodes[ind])\n",
    "\n",
    "# requirements = [x['requirements'] for x in clst_nodes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [len(x['samples']) for x in clst_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.hist([x[0] for y in x for x in requirements])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large_cluster_requirements = [forest_nodes[x]['requirements'] for x in clusters == 0]\n",
    "\n",
    "# print large_cluster_requirements[:300]\n",
    "# plt.figure()\n",
    "# plt.hist([x[0] for x in large_cluster_requirements if len(x) > 0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print [x['requirements'] for x in first_forest][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_absolute_gain_vectors(nodes):\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    gain_pairs = []\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_gain_pairs = []\n",
    "        try:\n",
    "            for feature,gain in zip(node['features'],node['absolute_gains']):\n",
    "                if feature not in features:\n",
    "                    features[feature] = len(features)\n",
    "                node_gain_pairs.append((features[feature],gain))\n",
    "            gain_pairs.append(node_gain_pairs)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    gain_matrix = np.zeros((len(nodes),len(features)))\n",
    "    \n",
    "    for n_ind,node_gain_pairs in enumerate(gain_pairs):\n",
    "        for f_ind,gain in node_gain_pairs:\n",
    "            gain_matrix[n_ind,f_ind] = gain\n",
    "    \n",
    "    features = [(x,features[x]) for x in features.keys()]\n",
    "    \n",
    "    features = sorted(features,key=lambda x: x[1])\n",
    "    \n",
    "    return gain_matrix, features\n",
    "\n",
    "def extract_local_gain_vectors(nodes):\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    gain_pairs = []\n",
    "    \n",
    "    for node in nodes:\n",
    "        node_gain_pairs = []\n",
    "        try:\n",
    "            for feature,gain in zip(node['features'],node['local_gains']):\n",
    "                if feature not in features:\n",
    "                    features[feature] = len(features)\n",
    "                node_gain_pairs.append((features[feature],gain))\n",
    "            gain_pairs.append(node_gain_pairs)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    gain_matrix = np.zeros((len(nodes),len(features)))\n",
    "    \n",
    "    for n_ind,node_gain_pairs in enumerate(gain_pairs):\n",
    "        for f_ind,gain in node_gain_pairs:\n",
    "            gain_matrix[n_ind,f_ind] = gain\n",
    "    \n",
    "    return gain_matrix\n",
    "\n",
    "\n",
    "def node_feature_pca(nodes):\n",
    "\n",
    "    node_encoding,feature_header = extract_absolute_gain_vectors(nodes)\n",
    "\n",
    "    embedding_model = PCA(n_components=2)\n",
    "\n",
    "    coordinates = embedding_model.fit_transform(node_encoding)\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "\n",
    "def node_feature_kpca(nodes):\n",
    "\n",
    "    node_encoding,feature_header = extract_absolute_gain_vectors(nodes)\n",
    "\n",
    "    embedding_model = KernelPCA(n_components=2,kernel='correlation')\n",
    "\n",
    "    coordinates = embedding_model.fit_transform(node_encoding)\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "def node_feature_tsne(nodes):\n",
    "\n",
    "    node_encoding,feature_header = extract_absolute_gain_vectors(nodes)\n",
    "\n",
    "#     embedding_model = TSNE(n_components=2,early_exaggeration=3,learning_rate=5,metric='correlation')\n",
    "\n",
    "    embedding_model = TSNE(n_components=2, metric='cosine')\n",
    "\n",
    "\n",
    "    coordinates = embedding_model.fit_transform(node_encoding)\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "def node_feature_mds(nodes):\n",
    "    \n",
    "    node_encoding,feature_header = extract_absolute_gain_vectors(nodes)\n",
    "    \n",
    "    embedding_model = MDS(n_components=2,dissimilarity='precomputed')\n",
    "        \n",
    "    distances = scipy.spatial.distance.squareform(pdist(node_encoding,metric='correlation'))\n",
    "    \n",
    "    coordinates = embedding_model.fit_transform(distances)\n",
    "    \n",
    "    return coordinates\n",
    "\n",
    "def node_feature_dbscan(nodes):\n",
    "\n",
    "    node_encoding,feature_header = extract_absolute_gain_vectors(nodes)\n",
    "\n",
    "    distances = scipy.spatial.distance.squareform(pdist(node_encoding,metric='cosine'))\n",
    "\n",
    "    clustering_model = DBSCAN(min_samples=10, eps = .35, metric='precomputed')\n",
    "    \n",
    "    clusters = clustering_model.fit_predict(distances)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Dbscan observed distances\")\n",
    "    plt.hist(distances.flatten(),bins=50)\n",
    "    plt.show()\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_forest_leaves = forest_to_leaves(first_forest)\n",
    "# second_forest_leaves = forest_to_leaves(second_layer)\n",
    "\n",
    "ff_feature_coordinates = node_feature_tsne(first_forest_leaves)\n",
    "\n",
    "ff_feature_clusters = node_feature_dbscan(first_forest_leaves)\n",
    "\n",
    "# fs_feature_coordinates = node_feature_mds(second_forest_leaves)\n",
    "\n",
    "# fs_feature_clusters = node_feature_dbscan(second_forest_leaves)\n",
    "\n",
    "plt.figure(\"first_forest_feature_pca\")\n",
    "plt.title(\"First Layer Forest Leaf Feature Gain\")\n",
    "plt.scatter(ff_feature_coordinates[:,0],ff_feature_coordinates[:,1],s=.03,c=ff_feature_clusters,cmap='rainbow')\n",
    "plt.savefig(prefix + \"ff_feature_tsne_dbscan_color.png\",dpi=500)\n",
    "\n",
    "# plt.figure(\"second_forest_feature_pca\")\n",
    "# plt.title(\"Second Layer Forest Node Feature Gain\")\n",
    "# plt.scatter(fs_feature_coordinates[:,0],fs_feature_coordinates[:,1],s=.03,c=fs_feature_clusters,cmap='rainbow')\n",
    "# plt.savefig(\"figures/fs_feature_tsne_dbscan_color.png\",dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print len(set(list(fs_feature_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist = pdist(extract_local_gain_vectors(forest_to_nodes(first_forest)),metric='cityblock')\n",
    "# print dist.shape\n",
    "# print dist[:10]\n",
    "# print np.sum(np.isnan(dist))\n",
    "# plt.figure(\"cos_dist_dist\")\n",
    "# plt.hist(dist,bins=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_forest_nodes = forest_to_nodes(first_forest)\n",
    "# # second_forest_nodes = forest_to_nodes(second_layer)\n",
    "\n",
    "# ff_feature_vectors = extract_local_gain_vectors(first_forest_nodes)\n",
    "\n",
    "# embedding_model = TSNE(n_components=2,metric='cosine',learning_rate=100,perplexity=50)\n",
    "# ff_local_feature_coordinates = embedding_model.fit_transform(ff_feature_vectors)\n",
    "\n",
    "# clustering_model = DBSCAN(eps=50,min_samples=2,metric='cityblock')\n",
    "# ff_local_feature_clusters = clustering_model.fit_predict(ff_feature_vectors)\n",
    "\n",
    "# plt.figure(\"first_forest_feature_pca\")\n",
    "# plt.title(\"First Layer Forest Node Feature Gain\")\n",
    "# plt.scatter(ff_local_feature_coordinates[:,0],ff_local_feature_coordinates[:,1],s=.03,c=ff_local_feature_clusters,cmap='rainbow')\n",
    "# plt.savefig(\"figures/ff_feature_tsne_dbscan_color.png\",dpi=500)\n",
    "\n",
    "# plt.figure(\"second_forest_feature_pca\")\n",
    "# plt.title(\"Second Layer Forest Node Feature Gain\")\n",
    "# plt.scatter(fs_feature_coordinates[:,0],fs_feature_coordinates[:,1],s=.03,c=fs_feature_clusters,cmap='rainbow')\n",
    "# plt.savefig(\"figures/fs_feature_tsne_dbscan_color.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_stats(nodes):\n",
    "    \n",
    "    split_features = [x['feature'] for x in nodes]\n",
    "\n",
    "    samples_per_node = [len(x['samples']) for x in nodes]\n",
    "\n",
    "    cluster_jaccard = scipy.spatial.distance.squareform(pdist(node_sample_encoding(nodes),metric='jaccard'))\n",
    "    \n",
    "    requirements = [x for y in nodes for x in zip(y['requirements'][::2],y['requirements'][1::2])]\n",
    "    \n",
    "    requirements_set = set([x[0] for x in requirements])\n",
    "    \n",
    "    requirement_frequencies = []\n",
    "    \n",
    "    for requirement in requirements_set:\n",
    "        \n",
    "        requirement_frequencies.append((requirement, sum([int(x[0] == requirement) for x in requirements])))\n",
    "    \n",
    "    requirement_frequencies = sorted(requirement_frequencies,key=lambda x: x[1])\n",
    "    \n",
    "    feature_gains, gains_header = extract_absolute_gain_vectors(nodes)\n",
    "\n",
    "    tree_ids = [x['tree_id'] for x in nodes]\n",
    "\n",
    "    mean_gains = np.mean(feature_gains, axis = 0)\n",
    "    \n",
    "    feature_gains_sort_indecies = np.argsort(mean_gains)\n",
    "    \n",
    "    \n",
    "    sorted_feature_gains = feature_gains.T[feature_gains_sort_indecies].T\n",
    "    sorted_features = np.array(gains_header)[feature_gains_sort_indecies]\n",
    "    \n",
    "    print \"Nodes: \" + str(len(nodes))\n",
    "    \n",
    "    print \"Top features\"\n",
    "    print sorted_features[-10:]\n",
    "    print np.mean(sorted_feature_gains,axis=0)[-10:]\n",
    "    \n",
    "    print \"Top Requirements:\"\n",
    "    print requirement_frequencies[-10:]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(tree_ids)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "for cluster in set(list(ff_sample_feature_cross_clusters)):\n",
    "    \n",
    "    indecies = np.arange(first_forest_leaves.shape[0])[ff_feature_clusters == cluster]\n",
    "    \n",
    "    cluster_nodes = []\n",
    "    \n",
    "    for index in indecies:\n",
    "        \n",
    "        cluster_nodes.append(first_forest_leaves[index])\n",
    "    \n",
    "    cluster_stats(cluster_nodes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
