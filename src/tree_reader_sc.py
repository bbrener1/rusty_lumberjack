import matplotlib as mpl
# COLOR = 'white'
# BACKGROUND = 'black'
# mpl.rcParams['text.color'] = COLOR
# mpl.rcParams['axes.labelcolor'] = COLOR
# mpl.rcParams['xtick.color'] = COLOR
# mpl.rcParams['ytick.color'] = COLOR
# mpl.rc('axes',fc=BACKGROUND)

mpl.rcParams['figure.dpi'] = 300

import matplotlib.pyplot as plt
import numpy as np
import re
import json
import sys
import os
import random
import glob
import pickle

from functools import reduce

import scipy.special
from scipy.stats import linregress
from scipy.spatial.distance import jaccard
from scipy.spatial.distance import pdist
from scipy.spatial.distance import squareform
from scipy.optimize import nnls

import sklearn
from sklearn.decomposition import PCA
from sklearn.decomposition import KernelPCA
from sklearn.manifold import TSNE
from sklearn.cluster import DBSCAN
from sklearn.cluster import AgglomerativeClustering
from sklearn.manifold import MDS
from sklearn.linear_model import Ridge,Lasso
from sklearn.decomposition import NMF
from umap import UMAP
from sklearn.metrics import jaccard_similarity_score
jaccard_index = jaccard_similarity_score


# from hdbscan import HDBSCAN

from scipy.cluster import hierarchy as hrc
from scipy.cluster.hierarchy import dendrogram,linkage

from multiprocessing import Pool

import copy

sdg_path_found = False

from pathlib import Path
sdg_path = Path(__file__).parent.parent.absolute()
sdg_path = str(sdg_path) + "/smooth_density_graph/"
# print(f"Attempting to locate Smooth Density Graph in {sdg_path}")
sys.path.append(sdg_path)

import smooth_density_graph as sdg

class Node:

    def __init__(self, node_json,tree,forest,parent=None,cache=False,lr=None,level=0):

        # Here we initialize the node from a json object generated by random forest

        self.cache = cache

        self.tree = tree
        self.forest = forest
        self.parent = parent
        self.lr = lr
        self.level = level
        self.split = node_json['split']
        self.prerequisites = node_json['prerequisites']
        try:
            self.samples = np.array([s['index'] for s in node_json['samples']])
        except:
            self.samples = np.array([t[0] for t in enumerate(node_json['samples']) if t[1] == 1])
        try:
            if len(node_json['braids']) > 0 and len(node_json['braids']) > level:
                self.braid = Braid(node_json['braids'][-1],self)
            else:
                self.braid = None
        except:
            if node_json['braid'] is not None:
                self.braid = Braid(node_json['braid'],self)
            else:
                self.braid = None
        self.weights = np.ones(len(self.forest.output_features))
        self.children = []
        self.child_clusters = ([],[])

        # Note the unpacking is recursive to maintain the tree structure
        # There are probably other ways to encode this like network connectivity
        # matrices but I think this leads to slightly more elegant structure.
        # We got python, we might as well use it

        if len(node_json['children']) > 0:
            self.children.append(Node(node_json['children'][0],self.tree,self.forest,parent=self,lr=0,level=level+1,cache=cache))
            self.children.append(Node(node_json['children'][1],self.tree,self.forest,parent=self,lr=1,level=level+1,cache=cache))

    ## Two nodes used for testing, not relevant for normal operation

    # def null():
    #
    #     null_dictionary = {}
    #     null_dictionary['feature'] = None
    #     null_dictionary['split'] = None
    #     null_dictionary['features'] = []
    #     null_dictionary['samples'] = []
    #     null_dictionary['children'] = []
    #     return Node(null_dictionary,None,None)
    #
    # def test_node(feature,split,features,samples,medians,dispersions):
    #     test_node = Node.null()
    #     test_node.feature = feature
    #     test_node.split = split
    #     test_node.features.extend(features)
    #     test_node.samples.extend(samples)
    #     return test_node


    def nodes(self):

        # Obtains all descendant nodes in deterministic order
        # Left to right

        nodes = []
        for child in self.children:
            nodes.extend(child.nodes())
        for child in self.children:
            nodes.append(child)
        return nodes

    def node_counts(self):

        # Obtains the count matrix of samples belonging to this node
        # Samples are in order they are in the node

        node_counts = np.zeros((len(self.samples),self.forest.output.shape[1]))
        for i,sample in enumerate(self.samples):
            node_counts[i] = self.forest.output[sample]
        return node_counts

    def medians(self):

        # Medians of all features in this node. Relies on node_counts

        if self.cache:
            if hasattr(self,'median_cache'):
                return self.median_cache
        matrix = self.node_counts()
        medians = np.median(matrix,axis=0)
        if self.cache:
            self.median_cache = medians
        return medians

    def feature_median(self,feature):

        # Median of a specific feature within this node.
        # Much faster than getting the entire matrix, obviously, unless medians are cached

        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        values = [self.forest.output[s,fi] for s in self.samples]
        return np.median(values)

    def means(self):

        # Means of all features within this node. Relies on node_counts

        if self.cache:
            if hasattr(self,'mean_cache'):
                return self.mean_cache
        matrix = self.node_counts()
        means = np.median(matrix,axis=0)
        if self.cache:
            self.mean_cache = means
        return means

    def feature_mean(self,feature):

        # As above, mean of an individual feature within this node

        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        values = [self.forest.output[s,fi] for s in self.samples]
        return np.mean(values)

    def dispersions(self):

        # Dispersions of this node. Hardcoded for SSME at the moment.
        # RECOMPUTED FROM SCRATCH. This SHOULD be mathematically equivalent to what
        # rust computes, but this is NOT guaranteed.
        # To do eventually: altered dispersion modes?

        if self.cache:
            if hasattr(self,'dispersion_cache'):
                return self.dispersion_cache
        matrix = self.forest.output[self.samples]
        dispersions = ssme(matrix,axis=0)
        if self.cache:
            self.dispersion_cache = dispersions
        return dispersions

    def absolute_gains(self):

        # Gains in dispersion relative to the root of the tree this node belongs to.
        # Only works if dispersions are of the summation type (eg sum of squared errors etc)

        if self.cache:
            if hasattr(self,'absolute_gain_cache'):
                return self.absolute_gain_cache
        own_dispersions = self.dispersions()
        root_dispersions = self.root().dispersions()
        gains = root_dispersions - own_dispersions
        if self.cache:
            self.absolute_gain_cache = gains
        return gains

    def local_gains(self):

        ## Gains in dispersion relative to the parent node.
        ## As above, only works for summation-type errors like SSME

        if self.cache:
            if hasattr(self,'local_gain_cache'):
                return self.local_gain_cache
        if self.parent is not None:
            parent_dispersions = self.parent.dispersions()
        else:
            parent_dispersions = self.dispersions()
        own_dispersions = self.dispersions()
        gains = parent_dispersions - own_dispersions
        if self.cache:
            self.local_gain_cache = gains
        return gains

    def additive_gains(self):

        ## What is the change in absolute values of medians of all features between parent node
        ## and this node?

        ## What the hell is this method? If you have a sample, and you have a number of nodes
        ## FROM THE SAME TREE that this sample belongs to, then summing the additive gains of
        ## all nodes produces a prediction for that sample.

        if self.cache:
            if hasattr(self,'additive_cache'):
                return self.additive_cache
        if self.parent is not None:
            parent_medians = self.parent.medians()
        else:
            parent_medians = np.zeros(len(self.forest.output_features))
        own_medians = self.medians()
        additive = own_medians - parent_medians
        if self.cache:
            self.additive_cache = additive
        return additive

    def feature_additive(self,feature):

        # Additive gains for a single feature. See additive_gains for explanation of additive gains

        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        own_value = self.feature_median(feature)
        if self.parent is not None:
            parent_value = self.parent.feature_median(feature)
        else:
            parent_value = 0.
        return own_value - parent_value

    def feature_additive_mean(self,feature):

        ## Additive gains using means instead of medians. Alternative prediction mode.
        ## MAY produce superior results for features of intermediate sparsity.

        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        own_value = self.feature_mean(feature)
        if self.parent is not None:
            parent_value = self.parent.feature_mean(feature)
        else:
            parent_value = 0.
        return own_value - parent_value

    def leaves(self):

        # Obtains all leaves belonging to this node

        leaves = []
        for child in self.children:
            leaves.extend(child.leaves())
        if len(leaves) < 1:
            for child in self.children:
                leaves.append(child)
        return leaves

    def stems(self):

        # Obtains all stems belonging to this node
        # (Stems are not roots and not leaves)

        stems = []
        for child in self.children:
            stems.extend(child.stems())
        for child in self.children:
            if len(child.children) > 0:
                stems.append(child)
        return stems

    def sister(self):

        # Obtains the sister node, if any

        if self.parent is None:
            return None
        else:
            for child in self.parent.children:
                if child is not self:
                    return child
            return None

    def descend(self,n):

        # All nodes no more than n levels down

        nodes = []
        if n > 0:
            for child in self.children:
                nodes.extend(child.descend(n-1))
            if len(nodes) < 1:
                nodes.append(self)
        else:
            nodes.append(self)
        return nodes

    def root(self):

        # Ascend to tree root

        if self.parent is not None:
            return self.parent.root()
        else:
            return self

    def ancestors(self):

        # All anscestors of this node

        ancestors = []
        if self.parent is not None:
            ancestors.append(self.parent)
            ancestors.extend(self.parent.ancestors())
        return ancestors

    def nodes_by_level(self):

        # Levelizes this tree

        # (Eg: a list of lists, list[x] corresponds to all descendent nodes from this one that are x levels down)

        levels = [[self]]
        for child in self.children:
            child_levels = child.nodes_by_level()
            for i,child_level in enumerate(child_levels):
                if len(levels) < i+2:
                    levels.append([])
                levels[i+1].extend(child_level)
        return levels

    def plotting_representation(self):

        # Used for plotting individual trees, returns a nested list of proportions
        # of samples present

        total_width = sum([len(x.samples) for x in self.children])
        child_proportions = []
        for child in self.children:
            child_proportions.append([float(len(child.samples)) / float(total_width),])
            child_proportions[-1].append(child.plotting_representation())
        # print(child_proportions)
        return child_proportions

    def sample_names(self):

        # Returns formal names (if any) for samples in this node

        return [self.forest.samples[i] for i in self.samples]

    def prerequisite_levels(self):

        # May be defunct if prerequisites are going by the wayside in favor of braids

        prerequisite_levels = []
        for child in self.children:
            prerequisite_levels.extend(child.prerequisite_levels())
        prerequisite_levels.append((self.prerequisites[-1],self.level))
        return prerequisite_levels

    def braid_levels(self):

        # Provides the braids that appear in the children of this node, paired to the level they occur at

        braid_levels = []
        for child in self.children:
            braid_levels.extend(child.braid_levels())
        braid_levels.append((self.braids[-1],self.level))
        return braid_levels

    def feature(self):

        # Best guess at the "split feature" of this node, if any

        if self.split is not None:
            return self.split.feature['name']
        elif self.braid is not None:
            return self.braid.features[0]
        else:
            return None

    def level(self,target):

        # Slices to a specific level of a given tree

        level_nodes = []
        if len(self.children) > 0:
            if self.level <= target:
                level_nodes.extend(self.children[0].level(target))
                level_nodes.extend(self.children[1].level(target))
        else:
            level_nodes.append(self)
        return level_nodes

    def depth(self,d=0):

        # How deep does this node go?

        for child in self.children:
            d = max(child.depth(d+1),d)
        return d

    def trim(depth):

        # NOT YET FULLY IMPLEMENTED

        all_children = self.nodes()
        keep_children = set(self.descend(depth))
        for child in all_children:
            if child not in keep_children:
                del(child)


    def sorted_node_counts(self):

        # Creates a sorted table of the node values, primarily useful for plotting.

        node_counts = self.node_counts()
        try:
            sort_feature_index = self.forest.truth_dictionary.feature_dictionary[self.feature()]
            sort_order = np.argsort(self.forest.counts[:,sort_feature_index][sample_mask])
        except:
            sort_order = np.arange(node_counts.shape[0])

        return node_counts[sort_order]



    def sample_leaves(self,sample):

        # Predictive method
        # Finds the leaves that a given sample belongs to in this (sub) tree

        if hasattr(self,"braid"):
            if self.braid is not None:
                braid_score = self.braid.score_sample(sample)
                # print("Braid prediction!")
                # print(braid_score)
                # print(self.braid.compound_split)
                # print("Trying comparison")
                # print(braid_score <= self.braid.compound_split)
                if braid_score <= self.braid.compound_split:
                    # print("Descending left")
                    return self.children[0].sample_leaves(sample)
                else:
                    # print("Descending right")
                    return self.children[1].sample_leaves(sample)
            else:
                return [self,]
        elif self.split is not None:
            if self.split['feature']['name'] in sample:
                if sample[self.split['feature']['name']] <= self.split['value']:
                    return self.children[0].sample_leaves(sample)
                else:
                    return self.children[1].sample_leaves(sample)
            else:
                return []
        else:
            return [self,]

    def sample_nodes(self,sample):

        # Predictive method

        # Finds all nodes that a sample belongs to in this (sub) tree

        nodes = [self,]

        if self.braid is not None:
            braid_score = self.braid.score_sample(sample)
            try:
                if braid_score <= self.braid.compound_split:
                    nodes.extend(self.children[0].sample_nodes(sample))
                else:
                    nodes.extend(self.children[1].sample_nodes(sample))
            except:
                raise
        elif self.split is not None:
            if self.split['feature']['name'] in sample:
                if sample[self.split['feature']['name']] <= self.split['value']:
                    nodes.extend(self.children[0].sample_descent(sample))
                else:
                    nodes.extend(self.children[1].sample_descent(sample))

        return nodes

    def tree_path_vector(self):

        # Binary left-right encoding of the path to this node. Primarily for internal use
        # May be defunct with prerequisite phaseout

        return np.array([0 if prerequisite[2] == '<' else 1 for prerequisite in self.prerequisites])

    def leaf_distances(self):

        # Experimental, attempts to produce within-tree distances of one leaf to another

        leaves = self.leaves()
        distances = np.zeros((len(leaves),len(leaves)))
        for i in range(len(leaves)):
            l1v = leaves[i].tree_path_vector()
            for j in range(i,len(leaves)):
                l2v = leaves[j].tree_path_vector()
                distance = leaves[i].level + leaves[j].level
                for l1,l2 in zip(l1v,l2v):
                    if l1 == l2:
                        distance -= 2
                    else:
                        break
                distances[i,j] = distance
                distances[j,i] = distance
        return distances

    def add_child_cluster(self,cluster,lr):

        # Keeps track of clusters that occur among child nodes.
        # Useful for calculating cluster-cluster tree distances
        # Experimental

        self.child_clusters[lr].append(cluster)
        if self.parent is not None:
            self.parent.add_child_cluster(cluster,self.lr)


    def set_split_cluster(self,cluster):

        # Sets the node split cluster

        self.split_cluster = cluster
        if self.parent is not None:
            self.parent.add_child_cluster(cluster,self.lr)
        for descendant in self.nodes():
            if not hasattr(descendant,'split_cluster'):
                descendant.split_cluster = cluster

    def add_child_cluster(self,cluster,lr):

        # Adds a cluster to the child cluster information of this node

        self.child_clusters[lr].append(cluster)
        if self.parent is not None:
            self.parent.add_child_cluster(cluster,self.lr)

    def l2_sum(self):

        # Descriptive but not currently in use

        counts = self.node_counts()
        medians = np.median(counts,axis=0)
        tile = np.tile(medians,(counts.shape[0],1))
        error = counts - tile
        return np.sum(np.power(error,2))

    def l1_sum(self):

        # Not currently in use (I think?)

        counts = self.node_counts()
        medians = np.median(counts,axis=0)
        tile = np.tile(medians,(counts.shape[0],1))
        error = counts - tile
        return np.sum(error)

    def l2_gain(self):

        # Not currently in use

        if len(self.children) > 0:
            return self.l2_sum() - self.children[0].l2_sum() - self.children[1].l2_sum()
        else:
            return 0

    def sample_cluster_means(self):

        # (roughly) provides the proportion of the samples in this node that belong to
        # each sample cluster. Allows us to test if node clusters and sample clusters have a
        # correspondence

        if self.cache:
            if hasattr(self,'sample_cluster_cache'):
                return self.sample_cluster_cache

        sample_clusters = self.forest.sample_cluster_encoding.T[self.samples].T
        sample_cluster_means = np.mean(sample_clusters,axis=1)

        if self.cache:
            self.sample_cluster_cache = sample_cluster_means

        return sample_cluster_means

    def reset_cache(self):

        # Resets the cache values for various things.
        # Save memory, avoid errors when you add/remove output features

        possible_caches = [
            "absolute_gain_cache",
            "local_gain_cache",
            "additive_cache",
            "median_cache",
            "dispersion_cache",
            "mean_cache",
        ]

        for cache in possible_caches:
            try:
                delattr(self,cache)
            except:
                continue

    # def lr_encoding_vectors(self):
    #     left = np.zeros(len(self.forest.samples),dtype=bool)
    #     right = np.zeros(len(self.forest.samples),dtype=bool)
    #     child_masks = [left,right]
    #     for i,child in enumerate(self.children):
    #         child_masks[i] = child.sample_mask()
    #     return child_masks


class Braid:

    def __init__(self,braid_json,node):
        try:
            self.node = node
            self.features = np.array([f['name'] for f in braid_json['features']])
            self.feature_splits = braid_json['feature_splits']
            self.split_flips = braid_json['split_flips']
            # self.samples = np.array([s['name'] for s in node_json['samples']])
            self.samples = self.node.samples
            self.compound_split = braid_json['compound_split']
            self.compound_values = braid_json['compound_values']
        except:
            print(braid_json)
            raise Exception

    def braid_matrix(mtx):

        from scipy.stats import rankdata

        ranked = mtx.copy()

        for i in range(mtx.shape[1]):
            ranked[:,i] = rankdata(ranked[:,i],method='min')

        return np.exp(np.mean(np.log(ranked),axis=1))

    def braid_scores(self):
        scores = np.zeros(len(self.node.forest.samples))
        sd = self.truth_dictionary.sample_dictionary
        for score,sample in zip(self.compound_values,self.samples):
            scores[sd[sample]] = score

        return scores

    def score_sample(self,sample):
        score = 0
        for split,flip in zip(self.feature_splits,self.split_flips):
            try:
                if flip:
                    if sample[split['feature']['name']] <= split['value']:
                        score += 1
                    else:
                        score -= 1
                else:
                    if sample[split['feature']['name']] <= split['value']:
                        score -= 1
                    else:
                        score += 1
            except:
                continue
        return score

class Tree:

    def __init__(self, tree_json, forest):
        self.root = Node(tree_json, self, forest,cache=forest.cache)
        self.forest = forest

    def nodes(self,root=True):
        nodes = []
        nodes.extend(self.root.nodes())
        if root:
            nodes.append(self.root)
        return nodes

    def leaves(self):
        leaves = self.root.leaves()
        if len(leaves) < 1:
            leaves.append(self.root)
        return leaves

    def stems(self):
        stems = self.root.stems()
        return stems

    def level(self,target):
        level_nodes = []
        for node in self.nodes():
            if node.level == target:
                level_nodes.append(node)
        return level_nodes

    def descend(self,level):
        return self.root.descend(level)

    def seek(self,directions):
        if len(directions) > 0:
            self.children[directions[0]].seek(directions[1:])
        else:
            return self

    def trim(self, depth):
        self.root.trim(depth)

    def feature_levels(self):
        return self.root.feature_levels()

    def plotting_representation(self,width=10,height=10):
        coordinates = []
        connectivities = []
        bars = []
        levels = self.root.nodes_by_level()
        jump = height / len(levels)
        for i,level in enumerate(levels):
            level_samples = sum([len(node.samples) for node in level])
            next_level_samples = 0
            if i < (len(levels)-1):
                next_level_samples = sum([len(node.samples) for node in levels[i+1]])
            consumed_width = 0
            next_consumed_width = 0
            for j,node in enumerate(level):
                sample_weight = float(len(node.samples)) / float(level_samples)
                half_width = (width * sample_weight)/2
                center = consumed_width + half_width
                consumed_width = consumed_width + (half_width * 2)
                coordinates.append((i*jump,center))
                if i < (len(levels)-1):
                    for child in node.children:
                        child_sample_weight = float(len(child.samples)) / float(next_level_samples)
                        child_half_width = (width * child_sample_weight)/2
                        child_center = next_consumed_width + child_half_width
                        next_consumed_width = next_consumed_width + (child_half_width * 2)
                        connectivities.append(([i*jump,(i+1)*jump],[center,child_center]))
        coordinates = np.array(coordinates)
        plt.figure()
        plt.scatter(coordinates[:,0],coordinates[:,1],s=1)
        for connection in connectivities:
            plt.plot(connection[0],connection[1])
        plt.show()

        # return coordinates,connectivities

    def recursive_plotting_repesentation(self,axes,height=None,height_step=None,representation=None,limits=None):
        if limits is None:
            limits = axes.get_xlim()
        current_position = limits[0]
        width = float(limits[1] - limits[0])
        center = (limits[1] + limits[0]) / 2
        if representation is None:
            representation = self.root.plotting_representation()
            print(representation)
        if height_step is None or height is None:
            depth = self.root.depth()
            height_limits = axes.get_ylim()
            height = height_limits[1]
            height_step = -1 * (height_limits[1] - height_limits[0]) / depth
        # print(representation)
        for i,current_representation in enumerate(representation):
            width_proportion = current_representation[0]
            children = current_representation[1]
            node_start = current_position
            node_width = width_proportion * width
            padding = node_width * .05
            node_width = node_width - padding
            node_center = (node_width/2) + current_position
            node_height = height + height_step
            node_end = (node_width) + current_position
            current_position = node_end + padding

            color = ['r','b'][(i%2)]

            axes.plot([center,node_center],[height,node_height],c=color)
            # axes.plot([node_center],[node_height])
            axes.plot([node_start,node_end],[node_height,node_height],c=color)

            self.recursive_plotting_repesentation(axes,height=node_height,height_step=height_step,representation=children,limits=(node_start,node_end))

    def plot(self):
        fig = plt.figure(figsize=(10,20))
        ax = fig.add_subplot(111)
        self.recursive_plotting_repesentation(ax)
        fig.show()

    def tree_movie_frame(self,location,level=0,sorted=True,previous_frame=None,split_lines=True):
        descent_nodes = self.descend(level)
        total_samples = sum([len(node.samples) for node in descent_nodes])
        heatmap = np.zeros((total_samples,len(self.forest.output_features)))
        node_splits = []
        running_samples = 0
        for node in descent_nodes:
            if sorted:
                node_counts = node.sorted_node_counts()
            else:
                node_counts = node.node_counts()
            node_samples = node_counts.shape[0]
            heatmap[running_samples:running_samples+node_samples] = node_counts
            running_samples += node_samples
            node_splits.append(running_samples)
        plt.figure(figsize=(10,10))
        if previous_frame is None:
            plt.imshow(heatmap,aspect='auto')
        else:
            plt.imshow(previous_frame,aspect='auto')
        if split_lines:
            for split in node_splits[:-1]:
                plt.plot([0,len(self.forest.output_features)-1],[split,split],color='w')
        plt.savefig(location)
        return heatmap

    def tree_movie(self,location):
        max_depth = max([leaf.level for leaf in self.leaves()])
        previous_frame = None
        for i in range(max_depth):
            self.tree_movie_frame(location+"."+str(i)+".a.png",level=i,sorted=False,previous_frame=previous_frame)
            previous_frame = self.tree_movie_frame(location+"."+str(i)+".b.png",level=i,sorted=True)
        self.tree_movie_frame(location+"."+str(i+1)+".b.png",level=i,sorted=True,split_lines=False)

    def summary(self, verbose=True):
        nodes = len(self.nodes)
        leaves = len(self.leaves)
        if verbose:
            print("Nodes: {}".format(nodes))
            print("Leaves: {}".format(leaves))

    def aborting_sample_descent(self,sample):
        return self.root.aborting_sample_descent(sample)

    def plot_leaf_counts(self):
        leaves = self.leaves()
        total_samples = sum([len(x.samples) for x in leaves])
        heatmap = np.zeros((total_samples,len(self.forest.output_features)))
        running_samples = 0
        for leaf in leaves:
            leaf_counts = leaf.node_counts()
            leaf_samples = leaf_counts.shape[0]
            heatmap[running_samples:running_samples+leaf_samples] = leaf_counts
            running_samples += leaf_samples

        ordering = dendrogram(linkage(heatmap.T),no_plot=True)['leaves']
        heatmap = heatmap.T[ordering].T
        plt.figure()
        im = plt.imshow(heatmap,aspect='auto')
        plt.colorbar()
        plt.show()

        return heatmap
    # def cluster_distances(self):
    #     for leaf in self.leaves():


class Forest:

    def __init__(self,trees,input,output,test=None,input_features=None,output_features=None,samples=None,split_labels=None,cache=False):
        if input_features is None:
            input_features = [str(i) for i in range(input.shape[1])]
        if output_features is None:
            output_features = [str(i) for i in range(output.shape[1])]
        if samples is None:
            samples = [str(i) for i in range(input.shape[0])]
        if test is not None:
            self.test = test

        self.cache = cache

        self.truth_dictionary = TruthDictionary(output,output_features,samples)

        self.input = input
        self.output = output
        self.samples = samples

        self.input_features = input_features
        self.output_features = output_features

        self.input_dim = input.shape
        self.output_dim = output.shape


        self.trees = trees

        for i,node in enumerate(self.nodes()):
            node.index = i

    def test_forest(roots,inputs,outputs,samples=None):
        test_forest = Forest([],inputs,outputs,samples)
        test_trees = [Tree.test_tree(root,test_forest) for root in roots]
        test_forest.trees = test_trees

########################################################################
########################################################################

            ## Node selection methods

      # Methods for picking specific nodes from the forest

########################################################################
########################################################################


    def nodes(self,root=True,depth=None):
        nodes = []
        for tree in self.trees:
            nodes.extend(tree.nodes(root=root))
        if depth is not None:
            nodes = [n for n in nodes if n.level <= depth]
        return nodes

    def leaves(self):
        leaves = []
        for tree in self.trees:
            leaves.extend(tree.leaves())
        return leaves

    def level(self,target):
        level = []
        for tree in self.trees:
            level.extend(tree.level(target))
        return level

    def stems(self,depth=None):
        stems = []
        for tree in self.trees:
            stems.extend(tree.stems())
        if depth is not None:
            stems = [s for s in stems if s.level <= depth]
        return stems

    def roots(self):
        return [tree.root for tree in self.trees]

    def node_sample_encoding(self,nodes):
        encoding = np.zeros((len(self.samples),len(nodes)),dtype=bool)
        for i,node in enumerate(nodes):
            for sample in node.samples:
                encoding[sample,i] = True
        return encoding

    def node_representation(self,nodes=None,mode='gain',metric=None,pca=0):

        if nodes is None:
            nodes = self.nodes()

        if mode == 'gain':
            print("Gain reduction")
            encoding = self.local_gain_matrix(nodes).T
        elif mode == 'additive':
            print("Additive reduction")
            encoding = self.additive_matrix(nodes).T
        elif mode == 'sample':
            print("Sample reduction")
            encoding = self.node_sample_encoding(nodes).T
        elif mode == 'sister':
            print("Sister reduction")
            encoding = self.node_sister_encoding(nodes).T
        elif mode == 'median' or mode == 'medians':
            print("Median reduction")
            encoding = self.node_matrix(nodes)
        elif mode == 'weights':
            print("Weight reduction")
            encoding = self.weight_matrix(nodes)
        else:
            raise Exception()

        if pca > 0:
            encoding = PCA(n_components=pca).fit_transform(encoding)

        if metric is not None:
            representation = squareform(pdist(encoding,metric=metric))
        else:
            representation = encoding

        return representation


    def node_sister_encoding(self,nodes):
        encoding = np.zeros((len(self.samples),len(nodes)),dtype=int)
        for i,node in enumerate(nodes):
            for sample in node.samples:
                encoding[sample,i] = 1
            if node.sister() is not None:
                for sample in node.sister().samples:
                    encoding[sample,i] = -1
        return encoding

    def trim(depth):

        for tree in self.trees:
            tree.trim(depth)

########################################################################
########################################################################

            ## Node/Matrix Methods

      # Methods for turning a set of nodes into an encoding matrix
      # For methods that turn nodes into float matrices see prediction

      # Encoding matrices are boolean matrices, usually node x property
      # Sample encoding matrix would be i,j, where i is ith node and j is whether
      # sample j appears in that node

########################################################################
########################################################################


    def absolute_gain_matrix(self,nodes):
        gains = np.zeros((len(self.output_features),len(nodes)))
        for i,node in enumerate(nodes):
            gains[i] = node.absolute_gains()
        return gains


    def local_gain_matrix(self,nodes):
        gains = np.zeros((len(self.output_features),len(nodes)))
        for i,node in enumerate(nodes):
            gains[:,i] = node.local_gains()
        return gains

    def additive_matrix(self,nodes):
        gains = np.zeros((len(self.output_features),len(nodes)))
        for i,node in enumerate(nodes):
            gains[:,i] = node.additive_gains()
        return gains

    def node_matrix(self,nodes):
        predictions = np.zeros((len(nodes),len(self.output_features)))
        for i,node in enumerate(nodes):
            predictions[i] = node.medians()
        return predictions

    def weight_matrix(self,nodes):
        weights = np.zeros((len(nodes),len(self.output_features)))
        for i,node in enumerate(nodes):
            weights[i] = node.weights
        return weights

########################################################################
########################################################################

            ## Loading/Creation Methods

      # This section deals with methods that load and unload the forest
      # from disk

########################################################################
########################################################################

    def backup(self,location):
        print("Saving forest")
        print(location)
        try:
            with open(location,mode='bw') as f:
                pickle.dump(self,f)
        except:
            print("Failed to save")

    def reconstitute(location):
        with open(location,mode='br') as f:
            return pickle.load(f)

    def set_cache(self,value):
        self.cache = value
        for node in self.nodes():
            node.cache = value

    def reset_cache(self):
        for node in self.nodes():
            node.reset_cache()

    def load(location, prefix="/run", ifh="/run.ifh",ofh='run.ofh',clusters='run.cluster',input="input.counts",output="output.counts",test="test.counts"):

        combined_tree_files = sorted(glob.glob(location + prefix + "*.compact"))

        input = np.loadtxt(location+input)
        output = np.loadtxt(location+output)
        ifh = np.loadtxt(location+ifh,dtype=str)
        ofh = np.loadtxt(location+ofh,dtype=str)

        try:
            test = np.loadtxt(location+test)
        except:
            print("Test data not detected")
            pass

        split_labels = None
        try:
            print(f"Looking for clusters:{location+clusters}")
            split_labels = np.loadtxt(location+clusters,dtype=int)
        except:
            pass

        first_forest = Forest([],input_features=ifh,output_features=ofh,input=input,output=output,test=test,split_labels=split_labels)

        trees = []
        for tree_file in combined_tree_files:
            print(f"Loading {tree_file}")
            trees.append(Tree(json.load(open(tree_file.strip())),first_forest))

        first_forest.prototype = Tree(json.load(open(location+prefix+".prototype")),first_forest)

        first_forest.trees = trees

        for i,node in enumerate(first_forest.nodes()):
            node.index = i

        sample_encoding = first_forest.node_sample_encoding(first_forest.leaves())

        if np.sum(np.sum(sample_encoding,axis=1) == 0) > 0:
            print("WARNING, UNREPRESENTED SAMPLES")

        return first_forest

    def add_output_feature(self,feature_values,feature_name=None):

        self.reset_cache()

        if hasattr(self,'added_features'):
            self.added_features += 1
        else:
            self.added_features = 1

        if feature_name is None:
            feature_name = str(len(self.output_features))

        feature_index = len(self.output_features)

        self.output_features = np.concatenate([self.output_features,np.array([feature_name,])])
        self.output = np.concatenate([self.output,np.array([feature_values,]).T],axis=1)
        self.truth_dictionary.feature_dictionary[feature_name] = feature_index

        for node in self.nodes():
            node.weights = np.append(node.weights,1.)

    def reset_output_featuers(self):

        if hasattr(self,'added_features'):
            if self.added_features > 0:
                removed_features = self.output_features[-self.added_features:]
                print(f"Removing:{removed_features}")
                self.output = self.output[:,:-self.added_features]
                self.output_features = self.output_features[:-self.added_features]
                for node in self.nodes():
                    node.weights = node.weights[:-self.added_features]
                for f in removed_features:
                    self.truth_dictionary.feature_dictionary.pop(f)
                self.added_features = 0

########################################################################
########################################################################

            ## PREDICTION METHODS

      # This section deals with methods that allow predictions on
      # samples

########################################################################
########################################################################

    def predict_sample_leaves(self,sample):
        sample_leaves = []
        for tree in self.trees:
            sample_leaves.extend(tree.root.sample_leaves(sample))
        return sample_leaves

    def predict_sample_nodes(self,sample):
        sample_nodes = []
        for tree in self.trees:
            sample_nodes.extend(tree.root.sample_nodes(sample))
        return sample_nodes


    def predict_node_sample_encoding(self,matrix):
        encoding = np.zeros((len(self.nodes()),matrix.shape[0]),dtype=bool)
        for i,sample in enumerate(matrix):
            leaves = self.predict_vector_leaves(sample)
            for leaf in leaves:
                encoding[leaf.index,i] = True
        return encoding

    def feature_weight_matrix(self,nodes):
        fd = self.truth_dictionary.feature_dictionary
        weights = np.zeros((len(nodes),len(fd)))
        for i,node in enumerate(nodes):
            weights[i] = node.weights
        return weights

    def nodes_median_predict_feature(self,nodes,feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_median(feature))
        return predictions

    def nodes_mean_predict_feature(self,nodes,feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_mean(feature))
        return predictions

    def nodes_weighted_median_predict_feature(self,nodes,feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_median(feature))
        fi = self.truth_dictionary.feature_dictionary[feature]
        weights = []
        for node in nodes:
            weights.append(node.weights[fi])
        return predictions,weights

    def nodes_additive_predict_feature(self,nodes,feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_additive(feature))
        fi = self.truth_dictionary.feature_dictionary[feature]
        weights = []
        for node in nodes:
            weights.append(node.weights[fi])
        return predictions,weights

    def nodes_mean_additive_predict_feature(self,nodes,feature):
        predictions = []
        for node in nodes:
            predictions.append(node.feature_additive_mean(feature))
        fi = self.truth_dictionary.feature_dictionary[feature]
        weights = []
        for node in nodes:
            weights.append(node.weights[fi])
        return predictions,weights

    def set_feature_weights(self,nodes,weights,feature):
        feature_index = self.truth_dictionary.feature_dictionary[feature]
        for node,weight in zip(nodes,weights):
            node.weights[feature_index] = weight


    def weigh_leaves(self,positive=True,feature_slice=None):

        if feature_slice is None:
            features = self.output_features
        else:
            features = self.output_features[feature_slice]

        median_representation = self.node_representation(self.leaves(), mode='median')

        for i,feature in enumerate(features):
            print(f"{i}. Calculating weights for {feature}")
            self.weigh_feature_median(feature,positive=positive,representation=median_representation)

        plt.figure()
        plt.hist(self.weight_matrix(self.leaves()).flatten(),bins=50,log=True)
        plt.show()

    def weigh_nodes(self,positive=True,feature_slice=None):

        if feature_slice is None:
            features = self.output_features
        else:
            features = self.output_features[feature_slice]

        additive_representation = self.node_representation(self.nodes(), mode='additive')

        for i,feature in enumerate(features):
            print(f"{i}. Calculating weights for {feature}")
            self.weigh_feature(feature,positive=positive,representation=additive_representation)

        plt.figure()
        plt.hist(self.weight_matrix(self.nodes()).flatten(),bins=50,log=True)
        plt.show()

    def weigh_feature_additive(self,feature,positive=True,representation=None):

        feature_index = self.truth_dictionary.feature_dictionary[feature]
        nodes = self.nodes()

        # nodes = self.leaves()
        # raw_predictions = np.array(self.node_representation(mode='median'))
        # raw_predictions = np.array([node.feature_mean(feature) for node in nodes])

        if representation is None:
            representation = self.node_representation(nodes,mode='additive')

        raw_predictions = representation[:,feature_index]
        # raw_predictions = self.node_representation(nodes,mode='additive')

        node_encoding = self.node_sample_encoding(nodes).astype(dtype=float)

        for i,prediction in enumerate(raw_predictions):
            node_encoding[:,i] *= prediction

        truth = self.output[:,feature_index]

        weights = Ridge(alpha=5).fit(node_encoding,truth).coef_
        # weights = NMF().fit()


        if positive:
            weights[weights < 0] = 0

        self.set_feature_weights(nodes,weights,feature)


    def weigh_feature_median(self,feature,positive=True,representation=None):

        feature_index = self.truth_dictionary.feature_dictionary[feature]
        nodes = self.leaves()

        # nodes = self.leaves()
        # raw_predictions = np.array(self.node_representation(mode='median'))
        # raw_predictions = np.array([node.feature_mean(feature) for node in nodes])

        if representation is None:
            representation = self.node_representation(nodes,mode='median')

        raw_predictions = representation[:,feature_index]
        # raw_predictions = self.node_representation(nodes,mode='additive')

        node_encoding = self.node_sample_encoding(nodes).astype(dtype=float)

        for i,prediction in enumerate(raw_predictions):
            node_encoding[:,i] *= prediction

        truth = self.output[:,feature_index]

        weights = Ridge(alpha=5).fit(node_encoding,truth).coef_
        # weights = NMF().fit()


        if positive:
            weights[weights < 0] = 0

        self.set_feature_weights(nodes,weights,feature)

    def predict_sample(self,sample):

        leaves = self.predict_sample_leaves(sample)
        consolidated_predictions = self.node_matrix(leaves)
        return np.mean(consolidated_predictions,axis=0)

    def predict_sample_cluster(self,sample):

        # nodes = self.predict_sample_nodes(sample)
        # print('cluster predict debug')
        # print(len(nodes))
        # cluster_predictions = np.zeros(len(self.sample_clusters))
        # for i,cluster in enumerate(self.sample_clusters):
        #     predictions,weights = self.nodes_mean_additive_predict_feature(nodes,f"sample_cluster_{int(cluster.id)}")
        #     # aggregate = np.sum(np.array(predictions) * np.array(weights)) / np.sum(weights)
        #     aggregate = np.sum(np.array(predictions) * np.array(weights))
        #     cluster_predictions[i] = aggregate
        # # print(cluster_predictions)
        # cluster = np.argmax(cluster_predictions)

        leaves = self.predict_sample_leaves(sample)
        cluster_predictions = np.zeros(len(self.sample_clusters))
        for i,cluster in enumerate(self.sample_clusters):
            predictions,weights = self.nodes_weighted_median_predict_feature(leaves,f"sample_cluster_{int(cluster.id)}")
            # aggregate = np.sum(np.array(predictions) * np.array(weights)) / np.sum(weights)
            aggregate = np.sum(np.array(predictions) * np.array(weights))
            cluster_predictions[i] = aggregate
        # print(cluster_predictions)
        cluster = np.argmax(cluster_predictions)

        return cluster

    def predict_sample_leaf_cluster(self,sample):
        leaves = self.predict_sample_leaves(sample)
        leaf_clusters = [l.leaf_cluster for l in leaves]
        return np.mode(leaf_clusters)[0][0]

    def weighted_predict_sample(self,sample):

        leaves = self.predict_sample_leaves(sample)
        return self.weighted_node_vector_prediction(leaves)

    def weighted_node_vector_prediction(self,nodes):
        raw_predictions = self.node_matrix(nodes)
        feature_weight_matrix = self.feature_weight_matrix(nodes)

        single_prediction = np.sum(raw_predictions * feature_weight_matrix,axis=0) / np.sum(feature_weight_matrix,axis=0)

        return single_prediction

    def predict_matrix(self,matrix,features=None,weighted=True):

        if features is None:
            features = self.input_features

        predictions = np.zeros((len(matrix),len(self.output_features)))

        for i,row in enumerate(matrix):
            sample = {feature:value for feature,value in zip(features,row)}
            if weighted:
                predictions[i] = self.weighted_predict_sample(sample)
            else:
                predictions[i] = self.predict_sample(sample)

        return predictions

    def predict_matrix_clusters(self,matrix,features=None):

        if features is None:
            features = self.input_features

        predictions = np.zeros(len(matrix),dtype=int)

        for i,row in enumerate(matrix):
            print(i)
            sample = {feature:value for feature,value in zip(features,row)}
            predictions[i] = self.predict_sample_cluster(sample)

        return predictions


    def predict_vector_leaves(self,vector,features=None):
        if features is None:
            features = self.input_features
        sample = {feature:value for feature,value in zip(features,vector)}
        return self.predict_sample_leaves(sample)

    def predict_vector_nodes(self,vector,features=None):
        if features is None:
            features = self.input_features
        sample = {feature:value for feature,value in zip(features,vector)}
        return self.predict_sample_nodes(sample)


########################################################################
########################################################################

            ## Clustering methods

########################################################################
########################################################################

    def split_labels(self,depth=3):
        nodes = self.nodes(depth=depth)
        return np.array([n.split_cluster for n in nodes])

    def set_sample_labels(self,sample_labels):

        self.sample_labels = np.array(sample_labels).astype(dtype=int)

        cluster_set = set(sample_labels)
        clusters = []
        for cluster in cluster_set:
            samples = np.arange(len(self.sample_labels))[self.sample_labels == cluster]
            clusters.append(SampleCluster(self,samples,int(cluster)))

        self.sample_clusters = clusters

        one_hot = np.array([sample_labels == x.id for x in clusters])

        for i,cluster in enumerate(one_hot):
            self.add_output_feature(cluster,feature_name=f"sample_cluster_{i}")

        self.sample_cluster_encoding = one_hot


    def cluster_samples_simple(self,override=False,pca=False,*args,**kwargs):

        if hasattr(self,'sample_labels'):
            self.reset_sample_clusters()

        counts = self.output

        if hasattr(self,'sample_clusters') and not override:
            print("Clustering has already been done")
            return self.sample_labels
        else:
            if pca is not False:
                self.set_sample_labels(sdg.fit_predict(PCA(n_components=pca).fit_transform(counts),*args,**kwargs))
            else:
                self.set_sample_labels(sdg.fit_predict(counts,*args,**kwargs))

        return self.sample_labels


    def cluster_samples_encoding(self,override=False,pca=False,*args,**kwargs):

        if hasattr(self,'sample_labels'):
            self.reset_sample_clusters()

        leaves = self.leaves()
        encoding = self.node_sample_encoding(leaves)

        if pca is not False:
            encoding = PCA(n_components=pca).fit_transform(encoding)

        if hasattr(self,'sample_clusters') and not override:
            print("Clustering has already been done")
        else:
            self.set_sample_labels(sdg.fit_predict(encoding,*args,**kwargs))

        return self.sample_labels

    def cluster_samples_coocurrence(self,override=False,*args,**kwargs):

        if hasattr(self,'sample_labels'):
            self.reset_sample_clusters()

        leaves = self.leaves()
        encoding = self.node_sample_encoding(leaves)
        coocurrence = coocurrence_matrix(encoding)

        if hasattr(self,'sample_clusters') and not override:
            print("Clustering has already been done")
            return self.sample_labels
        else:
            self.set_sample_labels(sdg.fit_predict(coocurrence,precomputed=True,*args,**kwargs))

        return self.sample_labels

    def cluster_samples_leaf_cluster(self,override=False,*args,**kwargs):

        if hasattr(self,'sample_labels'):
            self.reset_sample_clusters()

        leaves = self.leaves()
        encoding = self.node_sample_encoding(leaves)
        leaf_clusters = np.array([l.leaf_cluster for l in leaves])

        sample_labels = [np.mode(leaf_clusters[mask])[0][0] for mask in encoding.T]

        self.set_sample_labels(sample_labels)

        return self.sample_labels


    def set_leaf_labels(self,labels):

        leaves = self.leaves()
        self.leaf_labels = np.array(labels)

        cluster_set = set(self.leaf_labels)

        clusters = []

        for cluster in cluster_set:
            leaf_index = np.arange(len(self.leaf_labels))[self.leaf_labels == cluster]
            clusters.append(NodeCluster(self,[leaves[i] for i in leaf_index],cluster))

        self.leaf_clusters = clusters
        for leaf,label in zip(leaves,self.leaf_labels):
            leaf.leaf_cluster = label


    def cluster_leaves_samples(self,override=False,*args,**kwargs):

        leaves = self.leaves()
        encoding = self.node_sample_encoding(leaves).T

        if hasattr(self,'leaf_clusters') and not override:
            print("Clustering has already been done")
            return self.leaf_labels
        else:
            self.set_leaf_labels(sdg.fit_predict(encoding,*args,**kwargs))


        return self.leaf_labels

    def cluster_leaves_predictions(self,override=False,*args,**kwargs):

        leaves = self.leaves()
        predictions = self.node_representation(leaves,mode='median')

        if hasattr(self,'leaf_clusters') and not override:
            print("Clustering has already been done")
            return self.leaf_labels
        else:
            self.set_leaf_labels(sdg.fit_predict(predictions,*args,**kwargs))

        return self.leaf_labels

    def node_change_absolute(self,nodes1,nodes2):
        # First we obtain the medians for the nodes in question
        n1_medians = self.weighted_node_vector_prediction(nodes1)
        n2_medians = self.weighted_node_vector_prediction(nodes2)
        difference = n2_medians - n1_medians

        # Then sort by difference and return
        feature_order = np.argsort(difference)
        ordered_features = np.array(self.output_features)[feature_order]
        ordered_difference = difference[feature_order]

        return ordered_features,ordered_difference

    def node_change_log_fold(self,nodes1,nodes2):

        # First we obtain the medians for the nodes in question
        n1_medians = self.weighted_node_vector_prediction(nodes1)
        n2_medians = self.weighted_node_vector_prediction(nodes2)

        # We evaluate the ratio of median values
        log_fold_change = np.log2(n2_medians/n1_medians)

        # Because we are working with a division and a log, we have to filter for
        # results that don't have division by zero issues

        degenerate_mask = np.isfinite(log_fold_change)
        non_degenerate_features = np.array(self.output_features)[degenerate_mask]
        non_degenerate_changes = log_fold_change[degenerate_mask]

        # Finally sort and return
        feature_order = np.argsort(non_degenerate_changes)
        ordered_features = non_degenerate_features[feature_order]
        ordered_difference = non_degenerate_changes[feature_order]

        return ordered_features,ordered_difference

    def node_change_logistic(self,nodes1,nodes2):

        from sklearn.linear_model import LogisticRegression

        n1_counts = self.node_matrix(nodes1)
        n2_counts = self.node_matrix(nodes2)

        combined = np.concatenate([n1_counts,n2_counts],axis=0)

        labels = np.zeros(n1_counts.shape[0]+n1_counts.shape[0])

        labels[n2_counts.shape[0]:] = 1

        print(f"Logistic debug:{n1_counts.shape},{n2_counts.shape},{combined.shape},{labels.shape}")

        model = LogisticRegression().fit(combined,labels)

        feature_sort = np.argsort(model.coef_[0,:])

        ordered_features = np.array(self.output_features)[feature_sort]
        ordered_coefficients = model.coef_[0,:][feature_sort]

        return ordered_features,ordered_coefficients


    def cluster_leaf_total_predictions(self,override=False,*args,**kwargs):

        if hasattr(self,'leaf_clusters') and not override:
            print("Clustering has already been done")
            return self.leaf_labels
        else:
            leaves = self.leaves()
            meta_predictions = self.node_total_predictions(leaves)
            self.leaf_labels = np.array(sdg.fit_predict(meta_predictions,*args,**kwargs))

        cluster_set = set(self.leaf_labels)

        clusters = []

        for cluster in cluster_set:
            leaf_index = np.arange(len(self.leaf_labels))[self.leaf_labels == cluster]
            clusters.append(NodeCluster(self,[leaves[i] for i in leaf_index],cluster))

        self.leaf_clusters = clusters
        for leaf,label in zip(leaves,self.leaf_labels):
            leaf.leaf_cluster = label

        return self.leaf_labels

    def cluster_features(self,*args,**kwargs):
        gain_matrix = self.absolute_gain_matrix(self.leaves())
        return sdg.fit_predict(gain_matrix,*args,**kwargs)


    def sdg_cluster_representation(representation,**kwargs):
        return np.array(sdg.fit_predict(representation,**kwargs))

    def interpret_splits(self,override=False,mode='gain',metric='jaccard',distance='cosine',pca=False,depth=3,*args,**kwargs):

        from sklearn.manifold import MDS

        nodes = np.array(self.nodes(root=True,depth=depth))

        stem_mask = np.array([n.level != 0 for n in nodes])
        root_mask = np.logical_not(stem_mask)

        labels = np.zeros(len(nodes)).astype(dtype=int)

        representation = self.node_representation(nodes,mode=mode,metric=None,pca=pca)

        labels[stem_mask] = 1 + np.array(sdg.fit_predict(representation[stem_mask],metric=metric,*args,**kwargs))

        for node,label in zip(nodes,labels):
            node.set_split_cluster(label)
            # node.split_cluster = label


        cluster_set = set(labels)
        clusters = []
        for cluster in cluster_set:
            split_index = np.arange(len(labels))[labels == cluster]
            clusters.append(NodeCluster(self,[nodes[i] for i in split_index],cluster))

        split_order = np.argsort(labels)
        # split_order = dendrogram(linkage(reduction,metric='cos',method='average'),no_plot=True)['leaves']

        self.split_clusters = clusters

        return labels

    # def recursive_interpretation(self,nodes=None,mode='gain',metric='cosine',pca=False,distance='cosine',depth=3,level=0,**kwargs):
    #
    #     print("Recursively interpreting splits")
    #     print(f"Level:{level}")
    #
    #     if nodes is None:
    #         nodes = self.nodes()
    #
    #     children = []
    #     for node in nodes:
    #         children.extend(node.children)
    #
    #     child_representation = self.node_representation(children,mode=mode,metric=metric,pca=pca)
    #
    #     child_labels = self.sdg_cluster_representation(child_representation,**kwargs)
    #
    #     for child,label in zip(children,labels):
    #         child.set_split_cluster(label)
    #
    #     cluster_set = set(child_labels)
    #     clusters = []
    #     for cluster in cluster_set:
    #         split_index = np.arange(len(labels))[labels == cluster]
    #         clusters.append(NodeCluster(self,[nodes[i] for i in split_index],cluster))
    #
    #     if level <= depth:
    #
    #         for cluster in clusters:
    #             clusters.extend(recursive_interpretation(nodes = cluster.nodes(), mode=mode,metric=metric,pca=pca,distacne=distance,depth=depth,level=level+1,*args,**kwargs))
    #
    #     self.split_clusters = clusters
    #
    #     return clusters

    def create_root_cluster(self):

        roots = [t.root for t in self.trees]

        for node in roots:
            node.set_split_cluster(0)

        self.split_clusters = [NodeCluster(self,roots,0),]


    def plot_representation(self,representation,labels=None,metric='cos',pca=False):

        if metric is not None:
            # image = reduction[split_order].T[split_order].T
            agg_f = dendrogram(linkage(representation,metric=metric,method='average'),no_plot=True)['leaves']
            agg_s = dendrogram(linkage(representation.T,metric=metric,method='average'),no_plot=True)['leaves']
            image = representation[agg_f].T[agg_s].T
        else:
            # feature_order = dendrogram(linkage(reduction.T+1,metric='cosine',method='average'),no_plot=True)['leaves']
            # image = reduction[split_order].T[feature_order].T
            try:
                agg_f = dendrogram(linkage(representation.T,metric='cosine',method='average'),no_plot=True)['leaves']
            except:
                agg_f = dendrogram(linkage(representation.T,metric='cityblock',method='average'),no_plot=True)['leaves']
            try:
                agg_s = dendrogram(linkage(representation,metric='cosine',method='average'),no_plot=True)['leaves']
            except:
                agg_s = dendrogram(linkage(representation,metric='cityblock',method='average'),no_plot=True)['leaves']

            image = representation[agg_s].T[agg_f].T
            image = representation[agg_s].T[agg_f].T

        plt.figure(figsize=(10,10))
        plt.imshow(image,aspect='auto',cmap='bwr')
        plt.show()

        if labels is not None:

            split_order = np.argsort(labels)

            image = representation[split_order].T[agg_f].T
            plt.figure(figsize=(10,10))
            plt.imshow(image,aspect='auto',cmap='bwr')
            plt.show()

    def reset_sample_clusters(self):
        try:
            self.reset_output_featuers()
            del self.sample_clusters
            del self.sample_cluster_encoding
            del self.sample_labels
        except:
            print("No sample clusters")

    def reset_split_clusters(self):
        try:
            del self.split_clusters
            for node in self.nodes():
                node.child_clusters = ([],[])
                if hasattr(node,'split_cluster'):
                    del node.split_cluster
        except:
            print("No split clusters")

    def reset_leaf_clusters(self):
        try:
            del self.leaf_clusters
            del self.leaf_labels
            for node in self.nodes():
                if hasattr(node,'leaf_cluster'):
                    del node.leaf_cluster
        except:
            print("No leaf clusters")


    def reset_clusters(self):

        self.reset_sample_clusters()
        self.reset_split_clusters()
        self.reset_leaf_clusters()





########################################################################
########################################################################

            ## Plotting methods

########################################################################
########################################################################


    def plot_counts(self,no_plot=False):

        if not no_plot:
            cell_sort = dendrogram(linkage(encoding,metric='cos',method='average'),no_plot=True)['leaves']
            leaf_sort = dendrogram(linkage(encoding.T,metric='cos',method='average'),no_plot=True)['leaves']

            plt.figure(figsize=(10,10))
            plt.imshow(encoding[cell_sort].T[leaf_sort].T,cmap='binary')
            plt.show()

        return cell_sort,leaf_sort,self.ouput_counts

    def plot_cell_clusters(self,colorize=True,label=True):
        # if not hasattr(self,'leaf_clusters'):
        #     print("Warning, leaf clusters not detected")
        #     return None
        if not hasattr(self,'sample_clusters'):
            print("Warning, cell clusters not detected")
            return None

        coordinates = self.coordinates(no_plot=True)

        cluster_coordiantes = np.zeros((len(self.sample_clusters),2))

        for i,cluster in enumerate(self.sample_clusters):
            cluster_cell_mask = self.sample_labels == cluster.id
            mean_coordinates = np.mean(coordinates[cluster_cell_mask],axis=0)
            cluster_coordiantes[i] = mean_coordinates

        combined_coordinates = np.zeros((self.output.shape[0]+len(self.sample_clusters),2))

        combined_coordinates[0:self.output.shape[0]] = coordinates

        combined_coordinates[self.output.shape[0]:] = cluster_coordiantes

        highlight = np.ones(combined_coordinates.shape[0]) * 3
        highlight[len(self.sample_labels):] = [len(cluster.samples) for cluster in self.sample_clusters]
        # for i,cluster in enumerate(self.sample_clusters):
        #
        #     highlight[self.counts.shape[0] + i:] = len(cluster.samples/10)

        combined_labels = np.zeros(self.output.shape[0]+len(self.sample_clusters))
        if colorize:
            combined_labels[0:len(self.sample_labels)] = self.sample_labels
            combined_labels[len(self.sample_labels):] = [cluster.id for cluster in self.sample_clusters]

        cluster_names = [cluster.id for cluster in self.sample_clusters]
        cluster_coordiantes = combined_coordinates[len(self.sample_labels):]

        if label:
            f = plt.figure(figsize=(20,20))
            plt.title("Cell Coordinates")
            plt.scatter(combined_coordinates[:,0],combined_coordinates[:,1],s=highlight,c=combined_labels,cmap='rainbow')
            for cluster,coordinates in zip(cluster_names,cluster_coordiantes):
                plt.text(*coordinates,cluster,verticalalignment='center',horizontalalignment='center')
            plt.savefig("./tmp.delete.png",dpi=300)
        else:
            f = plt.figure(figsize=(20,20))
            plt.title("Cell Coordinates")
            plt.scatter(combined_coordinates[:len(self.samples),0],combined_coordinates[:len(self.samples),1],s=3,c=combined_labels[:len(self.samples)],cmap='rainbow')
            plt.savefig("./tmp.delete.png",dpi=300)
        return f

    def plot_split_clusters(self,colorize=True):
        if not hasattr(self,'split_clusters'):
            print("Warning, split clusters not detected")
            return None

        coordinates = self.coordinates(no_plot=True)

        cluster_coordinates = np.zeros((len(self.split_clusters),2))

        for i,cluster in enumerate(self.split_clusters):
            cluster_coordinates[i] = cluster.coordinates(coordinates=coordinates)

        combined_coordinates = np.zeros((self.output.shape[0]+len(self.split_clusters),2))

        combined_coordinates[0:self.output.shape[0]] = coordinates

        combined_coordinates[self.output.shape[0]:] = cluster_coordinates

        highlight = np.ones(combined_coordinates.shape[0])
        highlight[self.output.shape[0]:] = [len(cluster.nodes) for cluster in self.split_clusters]

        combined_labels = np.zeros(self.output.shape[0]+len(self.split_clusters))
        if colorize:
            combined_labels[self.output.shape[0]:] = [cluster.id for cluster in self.split_clusters]

        cluster_names = [cluster.id for cluster in self.split_clusters]
        cluster_coordiantes = combined_coordinates[-1 * len(self.split_clusters):]

        f = plt.figure(figsize=(5,5))
        plt.title("TSNE-Transformed Cell Coordinates")
        plt.scatter(combined_coordinates[:,0],combined_coordinates[:,1],s=highlight,c=combined_labels,cmap='rainbow')
        for cluster,coordinates in zip(cluster_names,cluster_coordiantes):
            plt.text(*coordinates,cluster,verticalalignment='center',horizontalalignment='center')
        plt.show()

        return f

    def sample_cluster_feature_matrix(self,features=None):
        if features is None:
            features = self.output_features
        coordinates = np.zeros((len(self.sample_clusters),len(features)))
        for i,sample_cluster in enumerate(self.sample_clusters):
            for j,feature in enumerate(features):
                # coordinates[i,j] = sample_cluster.feature_median(feature)
                coordinates[i,j] = sample_cluster.feature_mean(feature)
        return coordinates

    def split_cluster_feature_matrix(self,features=None):
        if features is None:
            features = self.output_features
        coordinates = np.zeros((len(self.split_clusters),len(features)))
        for i,split_cluster in enumerate(self.split_clusters):
            for j,feature in enumerate(features):
                coordinates[i,j] = split_cluster.feature_mean(feature)
        return coordinates



    def tsne(self,no_plot=False,pca=True,override=False,**kwargs):
        if not hasattr(self,'tsne_coordinates') or override:
            if pca:
                self.tsne_coordinates = TSNE().fit_transform(PCA(n_components=10).fit_transform(self.output))
            else:
                self.tsne_coordinates = TSNE().fit_transform(self.output)

        if not no_plot:
            plt.figure()
            plt.title("TSNE-Transformed Cell Coordinates")
            plt.scatter(self.tsne_coordinates[:,0],self.tsne_coordinates[:,1],s=.1,**kwargs)
            plt.show()

        return self.tsne_coordinates

    def tsne_encoding(self,no_plot=False,override=False,**kwargs):
        if not hasattr(self,'tsne_coordinates') or override:
            self.tsne_coordinates = TSNE().fit_transform(self.node_sample_encoding(self.leaves()))

        if not no_plot:
            plt.figure()
            plt.title("TSNE-Transformed Cell Coordinates")
            plt.scatter(self.tsne_coordinates[:,0],self.tsne_coordinates[:,1],s=.1,**kwargs)
            plt.show()

        return self.tsne_coordinates

    def pca(self,no_plot=False,override=False,**kwargs):
        if not hasattr(self,'pca_coordinates') or override:
            self.pca_coordinates = PCA(n_components=2).fit_transform(self.output)

        if not no_plot:
            plt.figure()
            plt.title("PCA-Transformed Cell Coordinates")
            plt.scatter(self.pca_coordinates[:,0],self.pca_coordinates[:,1],s=.1,**kwargs)
            plt.show()

        return self.pca_coordinates

    def umap(self,no_plot=False,override=False,**kwargs):
        if not hasattr(self,'umap_coordinates') or override:
            self.umap_coordinates = UMAP().fit_transform(self.output)

        if not no_plot:
            plt.figure()
            plt.title("UMAP-Transformed Cell Coordinates")
            plt.scatter(self.umap_coordinates[:,0],self.umap_coordinates[:,1],s=.1,**kwargs)
            plt.show()

        return self.umap_coordinates

    def umap_encoding(self,no_plot=False,override=False,**kwargs):
        if not hasattr(self,'umap_coordinates') or override:
            self.umap_coordinates = UMAP().fit_transform(self.node_sample_encoding(self.leaves()))

        if not no_plot:
            plt.figure()
            plt.title("UMAP-Transformed Cell Coordinates")
            plt.scatter(self.umap_coordinates[:,0],self.umap_coordinates[:,1],s=.1,**kwargs)
            plt.show()

        return self.umap_coordinates

    def coordinates(self,type=None,scaled=True,**kwargs):

        if type is None:
            if hasattr(self,'coordinate_type'):
                type = self.coordinate_type
            else:
                type = 'tsne'

        self.coordinate_type = type

        type_functions = {
            'tsne':self.tsne,
            'tsne_encoding': self.tsne_encoding,
            'pca': self.pca,
            'umap': self.umap,
            'umap_encoding': self.umap_encoding,
        }

        coordinates = type_functions[type](**kwargs)

        if scaled:
            coordinates = sklearn.preprocessing.scale(coordinates)

        return coordinates




    def plot_manifold(self,depth=3):

        f = self.plot_cell_clusters()

        def recursive_tree_plot(parent,children,figure,level=0):
            # print("Recursion debug")
            # print(f"p:{parent}")
            # print(f"c:{children}")
            pc = self.split_clusters[parent].coordinates()
            vectors = []
            for child,sub_children in children:
                if child == len(self.split_clusters):
                    continue
                cc = self.split_clusters[child].coordinates()
                # print("coordinates")
                # print(f"p{parent}:{pc}")
                # print(f"c{child}:{cc}")
                # v = pc - cc
                v = cc - pc
                # print(f"v:{v}")
                plt.figure(figure.number)
                # plt.arrow(pc[0],pc[1],v[0],v[1],width=(depth+1-level)*.3,length_includes_head=True)
                plt.arrow(pc[0],pc[1],v[0],v[1],length_includes_head=True)
                vectors.append((pc,v))
                figure,cv = recursive_tree_plot(child,sub_children,figure,level=level+1)
                vectors.extend(cv)
            # print("===============")
            return figure,vectors

        f,v = recursive_tree_plot(self.likely_tree[0],self.likely_tree[1],f)
        #
        # plt.savefig("./tmp.delete.png",dpi=300)
        #
        # return f,v

    def plot_braid_vectors(self):

        f = self.plot_cell_clusters(label=False)
        ax = f.add_axes([0,0,1,1])

        for cluster in self.split_clusters:
            ax = cluster.plot_braid_vectors(ax=ax,scatter=False,show=False)

        plt.savefig("./tmp.delete.png",dpi=300)

        return f


########################################################################
########################################################################

            ## Consensus tree methods

########################################################################
########################################################################

    def split_cluster_transition_matrix(self,depth=3):

        nodes = np.array(self.nodes(depth=depth))
        labels = self.split_labels(depth=depth)
        clusters = set(labels)
        transitions = np.zeros((len(clusters)+1,len(clusters)+1))

        for cluster in clusters:
            mask = labels == cluster
            cluster_nodes = nodes[mask]
            for node in cluster_nodes:
                node_state = node.split_cluster
                for child in node.children:
                    if hasattr(child,'split_cluster'):
                        child_state = child.split_cluster
                    else:
                        child_state = len(clusters)
                    transitions[node_state,child_state] += 1
                if len(node.children) == 0:
                    transitions[node_state,-1] += 1
                if node.parent is None:
                    transitions[-1,node_state] += 1

        self.split_cluster_transitions = transitions

        return transitions


    def relative_dependence_scores(self):

        relative_dependence_scores = np.zeros((len(self.split_clusters),len(self.split_clusters)))

        forest_nodes = self.nodes()

        for ci,cluster in enumerate(self.split_clusters):

            cluster_nodes = cluster.nodes

            cluster_children = [c for n in cluster_nodes for c in n.nodes()]
            child_indices = set([n.index for n in cluster_children])
            cluster_indices = set([n.index for n in cluster_nodes])
            exclude_set = child_indices.union(cluster_indices)

            external_nodes = [n for n in forest_nodes if n.index not in child_indices]
            external_indices = set([n.index for n in external_nodes])

            child_frequency = np.zeros(len(self.split_clusters))
            external_frequency = np.zeros(len(self.split_clusters))

            for cj,other_cluster in enumerate(self.split_clusters):
                for node in other_cluster.nodes:
                    ni = node.index
                    if ni in child_indices:
                        child_frequency[cj] += 1
                    if ni in external_indices:
                        external_frequency[cj] += 1

            child_nodes = np.sum(child_frequency)
            external_nodes = np.sum(external_frequency)

            total_frequency = child_frequency + external_frequency
            total_nodes = child_nodes + external_nodes

            # child_odds = (child_frequency+1)/(child_nodes+1)
            # external_odds = (external_frequency+1)/(external_nodes+1)
            # coverage_odds = (child_nodes + 1) /  (external_nodes + 1)
            # total_odds = (child_frequency + external_frequency + 1) / (len(total_nodes) + 1)

            # child_density = (child_frequency+1) / (child_nodes+1)
            # total_density = (total_frequency+1) / (total_nodes+1)

            relative_dependence = ((child_frequency +1) / (total_frequency + 1)) / ((child_nodes+1) / (total_nodes+1))
            # relative_dependence = (child_frequency + 1) / (total_frequency + 1) / (child_nodes + 1)
        #     relative_dependence = (child_odds / external_odds)
        #     relative_dependence = (child_odds / external_odds) / coverage_odds
        #     relative_dependence = child_density/total_density

            relative_dependence_scores[ci] = relative_dependence

        relative_dependence_scores = relative_dependence_scores - relative_dependence_scores.T

        return relative_dependence_scores

    def partial_dependence(self):
        total_nodes = self.nodes()
        path_matrix = np.zeros((len(self.split_clusters),len(total_nodes)))
        for node in total_nodes:
            path_matrix[node.split_cluster,node.index] = True
            if hasattr(node,'split_cluster'):
                for descendant in node.nodes():
                    path_matrix[node.split_cluster,descendant.index] = True
        path_covariance = np.cov(path_matrix)
        precision = np.linalg.pinv(path_covariance)
    #     return precision

        precision_normalization = np.sqrt(np.outer(np.diag(precision),np.diag(precision)))
        path_partials = precision / precision_normalization

        path_partials[np.isnan(path_partials)] = 0

        return path_partials

    def directional_matrix(self):
        downstream_frequency = np.zeros((len(self.split_clusters),len(self.split_clusters)),dtype=int)
        upstream_frequency = np.zeros((len(self.split_clusters),len(self.split_clusters)),dtype=int)

        for cluster in self.split_clusters:
            children = cluster.children()

            for child in children:
                if hasattr(child,'split_cluster'):
                    downstream_frequency[cluster.id,child.split_cluster] += 1

            ancestors = cluster.ancestors()

            for ancestor in ancestors:
                if hasattr(ancestor,'split_cluster'):
                    upstream_frequency[cluster.id,ancestor.split_cluster] += 1

        direction = np.sign(upstream_frequency-downstream_frequency)
        direction[direction < 0] = 0

        return direction

            # ancestors = cluster.ancestors()




        # mean_levels = np.array([c.mean_level() for c in self.split_clusters])
        # level_matrix = np.zeros((len(self.split_clusters),len(self.split_clusters)),dtype=bool)
        # for ci in range(len(self.split_clusters)):
        #     level_matrix[ci] = mean_levels < mean_levels[ci]
        #
        # return level_matrix.astype(dtype=float)

    ###############
    ##### Here we have several alternative methods for constructing the consensus tree.
    ###############

    ##### Most of them depend on these two helper methods that belong only in this scope

    #### The finite tree method takes a prototype, which is a list of lists.
    #### Each element in the list corresponds to which elements consider this element their parent

    def finite_tree(cluster,prototype,available):
        # print(cluster)
        children = []
        try:
            available.remove(cluster)
        except:
            pass
        for child in prototype[cluster]:
            if child in available:
                available.remove(child)
                children.append(child)
        return [cluster,[Forest.finite_tree(child,prototype,available) for child in children]]

    def reverse_tree(tree):
        root = tree[0]
        sub_trees = tree[1]
        child_entries = {}
        for sub_tree in sub_trees:
            for child,path in Forest.reverse_tree(sub_tree).items():
                path.append(root)
                child_entries[child] = path
        child_entries[root] = []
        return child_entries

    ##### End helpers



    def dependence_tree(self):

        ## This method constructs a tree based on partial correlations between the occurrences of split clusters in paths to forest leaves.

        dependence_scores = self.partial_dependence() * self.directional_matrix()

        self.dependence_scores = dependence_scores

        clusters = list(range(dependence_scores.shape[0]))

        proto_tree = [[] for cluster in clusters]

        for cluster in clusters:
            parent = np.argmin(dependence_scores[cluster])
            proto_tree[parent].append(cluster)

        print(f"Prototype:{proto_tree}")
        print(f"Dependence scores:{dependence_scores}")

        tree = []
        entry = 0

        tree = Forest.finite_tree(cluster=entry,prototype=proto_tree,available=clusters)
        rtree = Forest.reverse_tree(tree)

        self.likely_tree = tree
        self.reverse_likely_tree = rtree

        return tree


    def most_likely_tree(self,depth=3,transitions=None):

        if transitions is None:
            transitions = self.split_cluster_transition_matrix(depth=depth)

        transitions[np.identity(transitions.shape[0]).astype(dtype=bool)] = 0

        clusters = list(range(transitions.shape[0]))

        proto_tree = [[] for cluster in clusters]

        for cluster in clusters:
            parent = np.argmax(transitions[:,cluster])
            proto_tree[parent].append(cluster)

        print(f"Prototype:{proto_tree}")
        print(f"Transitions:{transitions}")

        tree = []
        entry = np.argmax(transitions[-1])

        tree = Forest.finite_tree(cluster=entry,prototype=proto_tree,available=clusters)
        rtree = Forest.reverse_tree(tree)

        self.likely_tree = tree
        self.reverse_likely_tree = rtree

        return tree

    def maximum_spanning_tree(self,depth=3,transitions=None):

        distances = self.split_cluster_transition_matrix(depth=depth)
        distances[:,-1] = 0

        mst = np.array(scipy.sparse.csgraph.minimum_spanning_tree(distances*-1).todense())*-1

        mst = np.maximum(mst,mst.T)

        clusters = set(range(len(self.split_clusters)))

        print("Max tree debug")
        print(distances)
        print(mst)

        def finite_tree(cluster,available):
            # print(cluster)
            # print(mst[cluster])
            children = []
            try:
                available.remove(cluster)
            except:
                pass
            for child in np.arange(mst.shape[0])[mst[cluster] > 0]:
                if child in available:
                    available.remove(child)
                    children.append(child)
            return [cluster,[finite_tree(child,available) for child in children]]


        tree = finite_tree(0,clusters)
        rtree = Forest.reverse_tree(tree)

        self.likely_tree = tree
        self.reverse_likely_tree = rtree

        return tree

    def sample_score_tree(self):

        sample_scores = np.zeros((len(self.split_clusters),len(self.samples)))
        sister_scores = np.zeros((len(self.split_clusters),len(self.samples)))

        for i,split_cluster in enumerate(self.split_clusters):
            sample_scores[i] = split_cluster.cell_counts()
            sample_normalization = np.max(sample_scores[i])
            sample_scores[i] *= (1./sample_normalization)
            sister_scores[i] = split_cluster.absolute_sister_scores()
            sister_normalization = np.max(sister_scores[i])
            sister_scores[i] *= (1./sister_normalization)

        print(sample_scores)
        print(sister_scores)

        distance_matrix = np.zeros((len(self.split_clusters),len(self.split_clusters)))

        for i in range(len(self.split_clusters)):
            print(i)
            plt.figure()
            plt.hist(np.abs(sister_scores[i]))
            plt.show()
            for j in range(len(self.split_clusters)):
                # print("#####################################")
                # print("#####################################")
                # print("#####################################")
                # print(j)
                # print(list(np.abs(sister_scores[i] - sample_scores[j])))
                # delta = np.sum(sister_scores[i] - sample_scores[j])
                # delta = np.sum(np.abs(sister_scores[i] - sample_scores[j]))
                delta = np.sum(np.sqrt(np.abs(sister_scores[i] - sample_scores[j])))
                # delta = np.sum(np.power(np.abs(sister_scores[i]) - sample_scores[j],2))
                # delta = np.sum(np.power(sister_scores[i],2) - np.power(sample_scores[j],2))
                # delta = np.dot(np.abs(sister_scores[i]),sample_scores[j])
                distance_matrix[i,j] = delta

        for i in range(len(self.split_clusters)):
            distance_matrix[i,i] = float('inf')

        print(distance_matrix)

        # cluster_parents = np.argmin(distance_matrix,axis=0)
        cluster_parents = np.array([np.argmin(x) for x in distance_matrix])
        cluster_parents[0] = -1

        print(cluster_parents)


        # return cluster_parents
        #
        def rec_tree(cluster,parents):
            output = [cluster,[]]
            children = np.arange(len(parents))[parents==cluster]
            for child in children:
                output[1].append(rec_tree(child,parents))
            return output


        tree =  rec_tree(0,cluster_parents)
        rtree = Forest.reverse_tree(tree)

        self.likely_tree = tree
        self.reverse_likely_tree = rtree

        return tree

    def plot_tree_summary(self,n=3,type="ud",custom=None,labels=None,features=None,primary=True,cmap='viridis',secondary=False,figsize=(30,30)):

        ## Helper methods:

        ## Find the leaves of a tree
        def leaves(tree):
            l = []
            for child in tree[1]:
                l.extend(leaves(child))
            if len(l) < 1:
                l.append(tree[0])
            return l

        ## Find out how many levels are in this tree (eg its maximum depth)
        def levels(tree,level=0):
            l = []
            for child in tree[1]:
                l.extend(levels(child,level=level+1))
            l.append(level)
            return l

        # First we compute the width/height of the individual cells
        width = 1/len(leaves(self.likely_tree))
        height = 1/(max(levels(self.likely_tree)) + 1)

        # Set up the figure
        fig = plt.figure(figsize=figsize)
        arrow_canvas = fig.add_axes([0,0,1,1])
        arrow_canvas.axis('off')

        # This function determines where to place everything, all coordinates are from 0 to 1, so are fractions of a canvas.
        # The coordinates are placed in a list we pass to the function, because I didn't want to think about how to avoid doing this
        def recursive_axis_coordinates(tree,child_coordinates,limits=[0,1]):
            [x,y] = limits
            child_width = 0
            # First we go lower in recursive layer and find how many children we need to account for from this leaf
            for child in tree[1]:
                cw = recursive_axis_coordinates(child,child_coordinates,[x+child_width,y-(height)])
                child_width += cw
            if child_width == 0:
                child_width = width
            # print("Recursive tree debug")
            # print(f"x:{x},y:{y}")
            # print(f"{tree[0]}")
            # print(f"cw:{child_width}")

            # We have to place the current leaf at the average position of all leaves below
            padding = (child_width - width) / 2
            coordinates = [x + padding + (width * .1),y - (height * .9),width*.8,height*.8]
            # print(f"coordinates:{coordinates}")

            child_coordinates.append([tree[0],coordinates])
            return child_width

            return child_coordinates

        ## Here we actually call the recursive function

        coordinates = []
        recursive_axis_coordinates(self.likely_tree,coordinates)
        coordinates = sorted(coordinates,key=lambda x: x[0])

        for i,[x,y,w,h] in coordinates:
            ax = fig.add_axes([x ,y , w, h])

            # Clean up the ticks, they are irrelevant
            ax.tick_params(bottom="off", left="off")
            ax.set_yticklabels([])
            ax.set_xticklabels([])

            # Place the panel generated by the cluster in the axis:
            # If it's not the terminal node or the origin node

            if type == "ud":
                if i < len(self.split_clusters) and i !=0:
                    self.split_clusters[i].up_down_panel(ax,n=n)
            if type == "id":
                if i < len(self.split_clusters) and i !=0:
                    text_rectangle(ax,f"{i}",[.4,.4,.2,.2],no_warp=True)
            if type == "custom":
                if i < len(self.split_clusters):
                    self.split_clusters[i].custom_panel(ax,custom,labels=labels)
            if type == "features":
                if i < len(self.split_clusters) and i !=0:
                    self.split_clusters[i].feature_panel(ax,features)
            if type == "additive_features":
                if i < len(self.split_clusters) and i !=0:
                    self.split_clusters[i].additive_panel(ax,features)
            if type == "score":
                if i < len(self.split_clusters) and i !=0:
                    self.split_clusters[i].score_panel(ax)
            if i == 0:
                text_rectangle(ax,"Origin",[.25,.5,.5,.5],no_warp=True)
            if i >= len(self.split_clusters):
                text_rectangle(ax,"Terminus",[.25,.5,.5,.5],no_warp=True)

        # If we created image panels during this process, we want to normalize all of them uniformly:

        from matplotlib.image import AxesImage

        if any([isinstance(cc,AxesImage) for c in fig.get_axes() for cc in c.get_children()]):

            # We want a uniform normalization between all the panels
            # First we find the vmax and vmin values:

            vmax = 0
            vmin = 0

            for ax in fig.get_axes():
                for child in ax.get_children():
                    if isinstance(child,AxesImage):
                        vmax = max(vmax,np.max(child.get_array()))
                        vmin = min(vmin,np.min(child.get_array()))

            if vmin == 0:
                vmin = -1
            if vmax == 0:
                vmax = 1

            print(f"vmin:{vmin},vmax:{vmax}")

            # Next we create a uniform normalization

            import matplotlib.colors as mcl

            # normalization = mcl.DivergingNorm(0,vmax=vmax,vmin=vmin)
            normalization = mcl.SymLogNorm(0.05,vmin=vmin,vmax=vmax)

            for ax in fig.get_axes():
                for child in ax.get_children():
                    if isinstance(child,AxesImage):
                        child.set_norm(normalization)
                        child.set_cmap(cmap)

            ## Finally we create a colorbar that corresponds to the unified view

            cb = fig.colorbar(mpl.cm.ScalarMappable(norm=normalization, cmap=cmap),ax=fig.get_axes())
            print(f"colorbar:{cb}")

        # fig.tight_layout()

        # Next we want to connect the nodes to their canonical children:

        def flatten_tree(tree):
            flat = []
            for child in tree[1]:
                flat.extend(flatten_tree(child))
            flat.append([tree[0],[c[0] for c in tree[1]]])
            return flat

        flat_tree = flatten_tree(self.likely_tree)

        if primary:

            # print(f"Coordinates:{coordinates}")
            # print(f"Flat tree:{flat_tree}")

            for i,children in flat_tree:
                x,y,w,h = coordinates[i][1]
                center_x = x + (w * .5)
                center_y = y + (h * .5)
                for ci in children:
                    cx,cy,cw,ch = coordinates[ci][1]
                    child_center_x = cx + (cw/2)
                    child_center_y = cy + (ch/2)

                    # We would like to set the arrow thickness to be proportional to the mean population of the child
                    if ci < len(self.split_clusters):
                        cp = self.split_clusters[ci].mean_population()
                    else:
                        cp = 1

                    arrow_canvas.plot([center_x,child_center_x],[center_y,child_center_y],linewidth=cp*.01,transform=arrow_canvas.transAxes)

        if secondary:
            # If we want to indicate secondary connections:
            for i in range(len(self.split_clusters)):
                for j in range(len(self.split_clusters)):
                    # if j not in flat_tree[i][1]:

                        # We scroll through every element in the split cluster transition
                        # matrix

                    if self.split_cluster_transitions[i,j] > 0:

                        # If the transitions are non-zero we obtain the coordinates

                        x,y,w,h = coordinates[i][1]
                        center_x = x + (width * .5)
                        center_y = y + (height * .5)
                        cx,cy,cw,ch = coordinates[j][1]
                        child_center_x = cx + (cw/2)
                        child_center_y = cy + (height/2)

                        # And plot a line with a weight equivalent to the number of transitions

                        # cp = self.split_cluster_transitions[i,j]
                        # total = np.sum(self.split_cluster_transitions[i])
                        # arrow_canvas.plot([center_x,child_center_x],[center_y,child_center_y],alpha=min(1,cp/total*2),linewidth=(cp**2)*.01,transform=arrow_canvas.transAxes)

                        ## Alternatively, plot a line with a weight equivalent to the partial correlation of split clusters:

                        cp = self.dependence_scores[i,j]
                        arrow_canvas.plot([center_x,child_center_x],[center_y,child_center_y],alpha=min(1,cp),linewidth=cp,transform=arrow_canvas.transAxes)

                        # total = np.sum(self.split_cluster_transitions[i])
                        # arrow_canvas.plot([center_x,child_center_x],[center_y,child_center_y],alpha=min(1,cp/total*2),linewidth=(cp**2)*.01,transform=arrow_canvas.transAxes)


            return fig

        #   print(f"N DEBUG TOP:{n}")

        #   recursive_axes(self.likely_tree,n=n)
        #   return fig


    def split_cluster_leaves(self):
        def tree_leaves(tree):
            leaves = []
            for child in tree[1]:
                leaves.extend(tree_leaves(child))
            if len(tree[1]) < 1:
                leaves.append(tree[0])
            return leaves

        tree = self.likely_tree
        leaf_clusters = [self.split_clusters[i] for i in tree_leaves(tree)]

        return leaf_clusters

    def cluster_samples_by_split_clusters(self,override=False,*args,**kwargs):

        if hasattr(self,'sample_clusters') and not override:
            print("Clustering has already been done")
            return self.sample_labels

        leaf_split_clusters = self.split_cluster_leaves()
        leaf_split_cluster_cell_scores = np.array([c.cell_counts() for c in leaf_split_clusters])
        sample_labels = np.array([np.argmax(leaf_split_cluster_cell_scores[:,i]) for i in range(len(self.samples))])

        self.set_sample_labels(sample_labels)

        print([c.id for c in self.split_clusters])

        return self.sample_labels

    def most_likely_sample_leaf_cluster(self,node_sample_encoding):
        cluster_scores = np.zeros((node_sample_encoding.shape[0],len(self.leaf_clusters)))
        for cluster in self.leaf_clusters:
            cluster_mask = np.array([n.leaf_cluster == cluster.id for n in self.leaves()])
            print(cluster.id)
            print(cluster_mask)
            print(np.sum(cluster_mask))
            for sample,sample_encoding in enumerate(node_sample_encoding):
                cluster_scores[sample,int(cluster.id)] = np.sum(np.logical_and(sample_encoding,cluster_mask).astype(dtype=int))
        plt.figure()
        plt.imshow(cluster_scores,aspect='auto',cmap='gray')
        plt.colorbar()
        plt.show()
        sample_clusters = np.argmax(cluster_scores,axis=1)
        return sample_clusters


class TruthDictionary:

    def __init__(self,counts,header,samples=None):

        self.counts = counts
        self.header = header
        self.feature_dictionary = {}

        self.sample_dictionary = {}
        for i,feature in enumerate(header):
            self.feature_dictionary[feature.strip('""').strip("''")] = i
        if samples is None:
            samples = map(lambda x: str(x),range(counts.shape[0]))
        for i,sample in enumerate(samples):
            self.sample_dictionary[sample.strip("''").strip('""')] = i

    def look(self,sample,feature):
#         print(feature)
        return(self.counts[self.sample_dictionary[sample],self.feature_dictionary[feature]])

class SampleCluster:

    def __init__(self,forest,samples,id):
        self.id = id
        self.samples = samples
        self.forest = forest

    def median_feature_values(self):
        return np.median(self.forest.output[self.samples],axis=0)

    def increased_features(self,n=50,plot=True):
        initial_medians = self.forest.weighted_node_vector_prediction([self.forest.prototype.root])
        current_medians = self.median_feature_values()

        difference = current_medians - initial_medians
        feature_order = np.argsort(difference)
        ordered_features = np.array(self.forest.features)[feature_order]
        ordered_difference = difference[feature_order]

        if plot:
            plt.figure(figsize=(10,8))
            plt.title("Upregulated Genes")
            plt.scatter(np.arange(n),ordered_difference[-n:])
            plt.xlim(0,n)
            plt.xlabel("Gene Symbol")
            plt.ylabel("Increase (LogTPM)")
            plt.xticks(np.arange(n),ordered_features[-n:],rotation=45,verticalalignment='top',horizontalalignment='right')
            plt.show()

        return ordered_features,ordered_difference


    def decreased_features(self,n=50,plot=True):
        initial_medians = self.forest.weighted_node_vector_prediction([self.forest.prototype.root])
        current_medians = self.median_feature_values()

        difference = current_medians - initial_medians
        feature_order = np.argsort(difference)
        ordered_features = np.array(self.forest.features)[feature_order]
        ordered_difference = difference[feature_order]

        if plot:
            plt.figure(figsize=(10,2))
            plt.title("Upregulated Genes")
            plt.scatter(np.arange(n),ordered_difference[:n])
            plt.xlim(0,n)
            plt.xlabel("Gene Symbol")
            plt.ylabel("Frequency")
            plt.xticks(np.arange(n),ordered_features[:n],rotation=45,verticalalignment='top',horizontalalignment='right')
            plt.show()

        return ordered_features,ordered_difference

    def leaf_encoding(self):
        leaves = self.forest.leaves()
        encoding = self.forest.node_sample_encoding(leaves)
        encoding = encoding[self.samples]
        return encoding

    def leaf_counts(self):
        encoding = self.leaf_encoding()
        return np.sum(encoding,axis=0)

    def leaf_cluster_frequency(self,plot=True):
        leaf_counts = self.leaf_counts()
        leaf_cluster_labels = self.forest.leaf_labels
        leaf_clusters = sorted(list(set(leaf_cluster_labels)))
        leaf_cluster_counts = []
        for leaf_cluster in leaf_clusters:
            cluster_mask = leaf_cluster_labels == leaf_cluster
            leaf_cluster_counts.append(np.sum(leaf_counts[cluster_mask]))
        if plot:
            plt.figure()
            plt.title(f"Distribution of Leaf Clusters in Cell Cluster {self.id}")
            plt.bar(np.arange(len(leaf_clusters)),leaf_cluster_counts,)
            plt.ylabel("Frequency")
            plt.xlabel("Leaf Cluster")
            plt.xticks(np.arange(len(leaf_clusters)),leaf_clusters)
            plt.show()

        return leaf_clusters,leaf_cluster_counts

    def feature_median(self,feature):
        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        vector = self.forest.output[self.samples][:,fi]
        return np.median(vector)

    def feature_mean(self,feature):
        fi = self.forest.truth_dictionary.feature_dictionary[feature]
        vector = self.forest.output[self.samples][:,fi]
        return np.mean(vector)


class NodeCluster:

    def __init__(self,forest,nodes,id):
        self.id = id
        self.nodes = nodes
        self.forest = forest

    def name(self):
        if hasattr(self,'stored_name'):
            return self.stored_name
        else:
            return str(self.id)

    def set_name(self,name):
        self.stored_name = name

    def encoding(self):
        return self.forest.node_sample_encoding(self.nodes)

    def children(self):

        return [c for n in self.nodes for c in n.nodes()]

    def parents(self):

        return [n.parent for n in self.nodes if n.parent is not None]

    def ancestors(self):

        return [a for n in self.nodes for a in n.ancestors()]

    def braids(self):
        return [node.braid for node in self.nodes if node.braid is not None]

    def parent_braids(self):
        return [node.parent.braid for node in self.nodes if node.parent is not None if node.parent.braid is not None]

    def parent_cluster(self):
        try:
            return self.forest.split_clusters[self.forest.reverse_likely_tree[self.id][0]]
        except:
            return self
    def weighted_feature_predictions(self):
        return self.forest.weighted_node_vector_prediction(self.nodes)

    def changed_absolute_root(self):
        root = [self.forest.prototype.root,]
        ordered_features,ordered_difference = self.forest.node_change_absolute(root,self.nodes)
        return ordered_features,ordered_difference

    def changed_absolute(self):
        parents = [n.parent for n in self.nodes if n.parent is not None]
        ordered_features,ordered_difference = self.forest.node_change_absolute(parents,self.nodes)
        return ordered_features,ordered_difference

    def changed_log_fold(self):
        parents = [n.parent for n in self.nodes if n.parent is not None]
        ordered_features,ordered_difference = self.forest.node_change_log_fold(parents,self.nodes)
        return ordered_features,ordered_difference

    def ranked_additive(self):
        additive = self.forest.node_representation(self.nodes,mode='additive')
        mean_additive = np.mean(additive,axis=0)
        sort = np.argsort(mean_additive)
        return self.forest.output_features[sort],mean_additive[sort]


    def logistic_sister(self,n=50,plot=True):
        sisters = [n.sister() for n in self.nodes]
        ordered_features,ordered_difference = self.forest.node_change_logistic(sisters,self.nodes)
        return ordered_features,ordered_difference

    def coordinates(self,coordinates=None):

        if coordinates is None:
            coordinates = self.forest.coordinates(no_plot=True)

        cell_scores = self.cell_scores()
        cell_scores = np.power(cell_scores,2)
        mean_coordinates = np.dot(cell_scores,coordinates) / np.sum(cell_scores)

        return mean_coordinates

    def feature_mean(self,feature):
        return np.mean(self.forest.nodes_mean_predict_feature(self.nodes,feature))

    # def feature_additive(self,feature):
    #     return np.mean(self.forest.nodes_additive_predict_feature(self.nodes,feature))
    def feature_additive(self,feature):
        return np.mean([n.feature_additive(feature) for n in self.nodes])

    def feature_mean_additive(self,feature):
        return np.mean(self.forest.nodes_mean_additive_predict_feature(self.nodes,feature))

    def feature_means(self,features):
        return np.array([self.feature_mean(feature) for feature in features])

    def feature_additives(self,features):
        return np.array([self.feature_additive(feature) for feature in features])

    def feature_mean_additives(self,features):
        return np.array([self.feature_mean_additive(feature) for feature in features])

    def mean_level(self):
        return np.mean([n.level for n in self.nodes])

    def mean_population(self):
        return np.mean([len(n.samples) for n in self.nodes])

    def cell_scores(self):
        cluster_encoding = self.encoding()
        return np.sum(cluster_encoding,axis=1) / (cluster_encoding.shape[1] + 1)

    def cell_counts(self):
        encoding = self.encoding()
        return np.sum(encoding,axis=1)

    def sister_scores(self):
        own = self.nodes
        sisters = [sister for n in own for sister in [n.sister(),] if sister is not None]
        own_encoding = self.forest.node_sample_encoding(own).astype(dtype=int)
        sister_encoding = self.forest.node_sample_encoding(sisters).astype(dtype=int)
        scores = (np.sum(own_encoding,axis=1) + (-1 * np.sum(sister_encoding,axis=1))) / own_encoding.shape[1]

        return scores

    def absolute_sister_scores(self):
        own = self.nodes
        sisters = [sister for n in own for sister in [n.sister(),] if sister is not None]
        own_encoding = self.forest.node_sample_encoding(own).astype(dtype=int)
        sister_encoding = self.forest.node_sample_encoding(sisters).astype(dtype=int)
        scores = (np.sum(own_encoding,axis=1) + np.sum(sister_encoding,axis=1)) / own_encoding.shape[1]

        return scores

    def mean_absolute_feature_gains(self):
        mean_gains = np.zeros(len(self.forest.features))
        node_gains = [node.absolute_gain_dictionary() for node in self.nodes]
        stacked_gains = stack_dictionaries(node_gains)
        print(list(stacked_gains.items())[:10])
        for i,feature in enumerate(self.forest.features):
            mean_gains[i] = np.mean(stacked_gains[feature])
        return mean_gains

    def ranked_feature_error_gain(self):
        nodes = self.nodes
        total_error_gain_matrix = self.forest.total_absolute_error_matrix(nodes)[0].T
        average_gains = np.mean(total_error_gain_matrix,axis=0)
        gain_rankings = np.argsort(average_gains)
        sorted_gains = average_gains[gain_rankings]
        sorted_features = self.forest.features[gain_rankings]
        return sorted_features,sorted_gains

    def ranked_mean_gains(self):
        mean_gains = self.mean_absolute_feature_gains()
        gain_order = np.argsort(mean_gains)
        sorted_features = np.array(self.forest.features)[gain_order]
        sorted_gains = mean_gains[gain_order]

        plt.figure(figsize=(10,2))
        plt.title("Features Gaining Information")
        plt.scatter(np.arange(50),sorted_gains[-50:])
        plt.xlim(0,50)
        plt.xlabel("Gene Symbol")
        plt.ylabel("Gain")
        plt.xticks(np.arange(50),sorted_features[-50:],rotation='vertical')
        plt.show()

        return sorted_features,sorted_gains

    def html_header(self):
        elements = [
            '<h1 style="text-align:center;">',
            "Split Cluster ",
            str(self.name()),
            "</h1>"
        ]
        return "".join(elements)

    def json_cluster_summary(self,n=20):

        import json

        attributes = {}

        # We copy over the html template for the summary:
        location = "../html/"

        import shutil
        shutil.copyfile('../cluster_summary_template_js.html',location+"cluster_summary_template_js.html")

        # Now we need to dump some summary information in that directory

        changed_features,change_fold = self.changed_log_fold()

        attributes['cluster_name'] = self.name()
        attributes['upregulated_html'] = generate_feature_value_html(reversed(changed_features[-n:]),reversed(change_fold[-n:]),cmap='bwr')
        attributes['downregulated_html'] = generate_feature_value_html(reversed(changed_features[:n]),reversed(change_fold[:n]),cmap='bwr')

        with open(location+"cluster_summary_template_js.html",'a') as html_file:
            json_string = f"""<div id="json_string"><!-- {json.dumps(attributes)} --></div>"""
            html_file.write(json_string)

        # And here we generate all the appropriate images

        forest_coordinates = self.forest.coordinates()
        sister_scores = self.sister_scores()
        plt.figure()
        plt.title("Distribution of Samples In This Cluster (Red) vs Its Sisters (Blue)")
        plt.scatter(forest_coordinates[:,0],forest_coordinates[:,1],c=sister_scores,cmap='bwr')
        plt.colorbar()
        plt.ylabel("tSNE Coordinates (AU)")
        plt.xlabel("tSNE Coordinates (AU)")
        plt.savefig(location+"sister_map.png")

        # Finally we ask the OS to open the html file.
        # os.system(f'open {location + "cluster_summary_template.html"}')
        from subprocess import run
        run(["open",location + "cluster_summary_template_js.html"])

        pass

    #
    # def html_cluster_summary(self,n=20):
    #
    #     # Create a temp directory for storing summary info
    #
    #     import tempfile as tmp
    #
    #     location = "../html/"
    #
    #     # We copy over the html template for the summary:
    #
    #     import shutil
    #     shutil.copyfile('../cluster_summary_template_js.html',location+"cluster_summary_template_js.html")
    #
    #     # Now we need to dump some summary information in that directory
    #
    #     changed_features,change_fold = self.changed_log_fold()
    #
    #     print(changed_features)
    #     print(change_fold)
    #
    #     with open(location+"cluster_name.html",'w') as cname:
    #         cname.write(self.html_header())
    #
    #     with open(location+"upregulated.html",'w') as upregulated_file:
    #         html_str = generate_feature_value_html(reversed(changed_features[-n:]),reversed(change_fold[-n:]),cmap='bwr')
    #         upregulated_file.write(html_str)
    #
    #     with open(location+"downregulated.html",'w') as downregulated_file:
    #         html_str = generate_feature_value_html(reversed(changed_features[:n]),reversed(change_fold[:n]),cmap='bwr')
    #         downregulated_file.write(html_str)
    #
    #     forest_coordinates = self.forest.coordinates()
    #     sister_scores = self.sister_scores()
    #     plt.figure()
    #     plt.scatter(forest_coordinates[:,0],forest_coordinates[:,1],c=sister_scores,cmap='bwr')
    #     plt.savefig(location+"sister_map.png")
    #
    #     # Finally we ask the OS to open the html file.
    #     # os.system(f'open {location + "cluster_summary_template.html"}')
    #     from subprocess import run
    #     run(["open",location + "cluster_summary_template_js.html"])
    #
    #     pass

    # def biological_cluster_summary(self):
    #     levels = [node.level for node in self.nodes]
    #
    #     fig = plt.figure(figsize=(20,10))
    #
    #     # fig.suptitle(f"Summary of Leaf Cluster {self.id}")
    #
    #     ax_levels = fig.add_axes([.875,.025,.1,.2])
    #     ax_levels.set_title("Leaf Levels")
    #     ax_levels.set_ylabel("Frequency")
    #     ax_levels.hist(levels)
    #
    #     leaf_size = [len(node.samples) for node in self.nodes]
    #
    #     ax_leaf_size = fig.add_axes([.875,.275,.1,.2])
    #     ax_leaf_size.set_title("Leaf Sizes")
    #     ax_leaf_size.hist(leaf_size)
    #     ax_leaf_size.set_ylabel("Frequency")
    #
    #     ordered_features,ordered_difference = self.changed_absolute_root()
    #
    #     range = max(np.abs(np.min(ordered_difference.flatten())),np.max(ordered_difference)) * 1.1
    #
    #     ax_downregulated = fig.add_axes([.025,.775,.2,.2])
    #     ax_downregulated.set_title("Downregulated Genes",fontsize=20)
    #     ax_downregulated.set_ylabel("Mean downregulation (Log TPM)",fontsize=10)
    #     ax_downregulated.bar(np.arange(10),ordered_difference[:10])
    #     ax_downregulated.set_ylim(-range,range)
    #     ax_downregulated.set_xticks(np.arange(10))
    #     ax_downregulated.set_xticklabels(ordered_features[:10],rotation=45,verticalalignment='top',horizontalalignment='right',fontsize=12)
    #
    #     ax_upregulated = fig.add_axes([.25,.775,.2,.2])
    #     ax_upregulated.set_title("Upregulated Genes",fontsize=20)
    #     ax_upregulated.set_ylabel("Mean upregulation (Log TPM)",labelpad=10,fontsize=10)
    #     ax_upregulated.yaxis.set_label_position('right')
    #     ax_upregulated.bar(np.arange(10),ordered_difference[-10:])
    #     ax_upregulated.set_ylim(-range,range)
    #     ax_upregulated.set_xticks(np.arange(10))
    #     ax_upregulated.set_xticklabels(ordered_features[-10:],rotation=45,verticalalignment='top',horizontalalignment='right',fontsize=12)
    #
    #     # ordered_prerequisites,prerequisite_counts = self.prerequisite_frequency(plot=False)
    #     #
    #     # ax_prerequisites = fig.add_axes([.025,.41,.45,.2])
    #     # ax_prerequisites.set_title("Prerequisites By Frequency",fontsize=20)
    #     # ax_prerequisites.bar(np.arange(10),prerequisite_counts[-10:])
    #     # ax_prerequisites.set_ylabel("Frequency",fontsize=15)
    #     # ax_prerequisites.set_xticks(np.arange(10))
    #     # ax_prerequisites.set_xticklabels(ordered_prerequisites[-10:],rotation=45,verticalalignment='top',horizontalalignment='right',fontsize=12)
    #
    #     cell_clusters,cell_cluster_frequency = self.cell_cluster_frequency(plot=False)
    #
    #     ax_cluster_frequency = fig.add_axes([.025,.025,.45,.2])
    #     ax_cluster_frequency.set_title("Leaf Cluster/Cell Cluster Relation",fontsize=20)
    #     ax_cluster_frequency.bar(np.arange(len(cell_clusters)),cell_cluster_frequency)
    #     ax_cluster_frequency.set_xticks(np.arange(len(cell_clusters)))
    #     ax_cluster_frequency.set_xticklabels(cell_clusters,fontsize=15)
    #     ax_cluster_frequency.set_xlabel("Cell Clusters",fontsize=15)
    #     ax_cluster_frequency.set_ylabel("Frequency",fontsize=15)
    #
    #     # prereqs = self.average_prereq_freq_level(plot=False)
    #     #
    #     # prereqs = [prereq for prereq in prereqs if prereq[0][:2] != "CG"]
    #     # prereqs = sorted(prereqs,key=lambda prereq: prereq[1][1])[::-1]
    #     #
    #     # prereq_features = [prereq[0] for prereq in prereqs]
    #     # prereq_levels = [prereq[1][0] for prereq in prereqs]
    #     # prereq_frequencies = [prereq[1][1] * 10 for prereq in prereqs]
    #     #
    #     # ax_path = fig.add_axes([.6,.025,.2,.95])
    #     # ax_path.set_title(f"The Path to Cluster {self.id}",fontsize=20)
    #     # ax_path.scatter(prereq_levels[:50],np.arange(49,-1,-1),s=prereq_frequencies[:50])
    #     # ax_path.set_xlabel("Average Level of Decision",fontsize=15)
    #     # # ax_path.set_xlim(max(prereq_levels[:20])*1.1,-0.01)
    #     # ax_path.set_yticks(np.arange(49,-1,-1))
    #     # ax_path.set_yticklabels(prereq_features[:50],fontsize=14)
    #     # # ax_path.grid(axis='y')
    #
    #     plt.show()


    def braid_scores(self):

        from scipy.stats import pearsonr
        from scipy.stats.mstats import gmean

        braids = self.parent_braids()

        fd = self.forest.truth_dictionary.feature_dictionary

        braid_matrix_dimension = [len(self.forest.samples),len(braids)]

        braid_matrix = np.zeros((braid_matrix_dimension[0],braid_matrix_dimension[1]))

        for i,braid in enumerate(braids):

            braid_matrix[:,i] = braid.braid_scores()
            if pearsonr(braid_matrix[:,i],braid_matrix[:,0])[0] < 0:
                braid_matrix[:,i] *= -1

        braided_scores = np.mean(braid_matrix)

        return braided_scores

    def braid_features(self):

        features = {}

        for braid in self.parent_braids():
            feature = braid.features[0]
            if feature not in features:
                features[feature] = 0
            features[feature] += 1

        return features

    def top_braid_features(self):

        features = {}

        for braid in self.braids():
            feature = braid.features[0]
            if feature not in features:
                features[feature] = 0
            features[feature] += 1

        braid_scores = self.braid_scores()

        for feature in features.keys():
            feature_index = self.forest.truth_dictionary.feature_dictionary[feature]
            feature_values = self.forest.output[:,feature_index]
            # features[feature] *= np.sign(scipy.stats.spearmanr(feature_values,braid_scores)[0])

        return features

    def braid_vectors(self,coordinates=None):

        braid_scores = self.braid_scores()
        braid_features = self.braid_features()

        sorted_features = sorted(list(braid_features.items()),key=lambda f: f[1][0])

        positive_sample_mask = braid_scores > 0
        negative_sample_mask = braid_scores < 0

        if coordinates is None:
            coordinates = self.forest.tsne(no_plot=True)

        positive_vector = np.dot(np.power(braid_scores[positive_sample_mask],2),coordinates[positive_sample_mask]) / np.sum(np.power(braid_scores[positive_sample_mask],2))
        negative_vector = np.dot(np.power(braid_scores[negative_sample_mask],2),coordinates[negative_sample_mask]) / np.sum(np.power(braid_scores[negative_sample_mask],2))

        positive_features = [f for f in sorted_features if f[1][1] > 0][-5:]
        negative_features = [f for f in sorted_features if f[1][1] < 0][-5:]

        return ((positive_vector,positive_features),(negative_vector,negative_features))

    def plot_braid_vectors(self,ax=None,coordinates=None,scatter=True,show=True):

        if ax is None:
            figure = plt.figure(figsize=(10,10))
            ax = figure.add_axes([0,0,1,1])

        if coordinates is None:
            coordinates = self.forest.tsne(no_plot=True)

        (positive_vector,positive_features),(negative_vector,negative_features) = self.braid_vectors()

        cc = self.coordinates(coordinates=coordinates)

        if scatter:
            braid_color = self.braid_scores()
            ax.scatter(coordinates[:,0],coordinates[:,1],c=braid_color,s=2,cmap='bwr')
        # plt.scatter(cc[0],cc[1],s=200)
        fraction = .3
        ax.arrow(cc[0],cc[1],(positive_vector[0]-cc[0]) * fraction ,(positive_vector[1]-cc[1]) * fraction ,width=1,color='red')
        ax.arrow(cc[0],cc[1],(negative_vector[0]-cc[0]) * fraction ,(negative_vector[1]-cc[1]) * fraction ,width=1,color='blue')
        if show:
            plt.show()
        return ax


    def cell_cluster_frequency(self,plot=True):
        cell_cluster_labels = self.forest.sample_labels
        cell_counts = self.cell_counts()
        cell_clusters = sorted(list(set(cell_cluster_labels)))
        cluster_counts = []
        for cluster in cell_clusters:
            cluster_mask = cell_cluster_labels == cluster
            cluster_counts.append(np.sum(cell_counts[cluster_mask]))

        if plot:
            plt.figure()
            plt.title("Frequency of cell clusters in leaf cluster")
            plt.bar(np.arange(len(cell_clusters)),cluster_counts,tick_labels=cell_clusters)
            plt.ylabel("Frequency")
            plt.show()

        return cell_clusters,cluster_counts



    def plot_cell_counts(self,**kwargs):
        counts = self.cell_counts()
        plt.figure(figsize=(15,10))
        plt.scatter(self.forest.coordinates(no_plot=True)[:,0],self.forest.coordinates(no_plot=True)[:,1],c=counts,**kwargs)
        plt.colorbar()
        plt.show()



    # def dependence_scores(self):
    #
    #     total_nodes = self.forest.nodes()
    #
    #     cluster_nodes = self.nodes
    #
    #     cluster_children = [c for n in cluster_nodes for c in n.nodes()]
    #     child_indices = set([n.index for n in cluster_children])
    #     cluster_indices = set([n.index for n in cluster_nodes])
    #     exclude_set = child_indices.union(cluster_indices)
    #
    #     external_nodes = [n for n in total_nodes if n.index not in child_indices]
    #     external_indices = set([n.index for n in external_nodes])
    #
    #     child_frequency = np.zeros(len(self.forest.split_clusters))
    #     external_frequency = np.zeros(len(self.forest.split_clusters))
    #
    #     for ci,cluster in enumerate(self.forest.split_clusters):
    #         for node in cluster.nodes:
    #             ni = node.index
    #             if ni in child_indices:
    #                 child_frequency[ci] += 1
    #             if ni in external_indices:
    #                 external_frequency[ci] += 1
    #
    #     # print(child_frequency)
    #     # print(external_frequency)
    #
    #     return (np.array(child_frequency) + 1) / (np.array(external_frequency) + 1)

    def prerequisites(self):
        prerequisite_dictionary = {}
        for node in self.nodes:
            for prerequisite,split,sign in node.prerequisites:
                if prerequisite not in prerequisite_dictionary:
                    prerequisite_dictionary[prerequisite] = []
                prerequisite_dictionary[prerequisite].append((split,sign))
        return prerequisite_dictionary

    def prerequisites_by_level(self):
        levels = []
        max_depth = max([node.level for node in self.nodes])
        for level in range(max_depth):
            levels.append([])
            for node in self.nodes:
                try:
                    levels[-1].append(node.prerequisites[level])
                except:
                    pass
        return levels


    def prerequisite_frequency(self,n=50,plot=True):
        prerequisites = list(self.prerequisites().items())
        prerequisites.sort(key=lambda x: len(x[1]))
        prerequisite_counts = [len(x[1]) for x in prerequisites]
        prerequisite_labels = [x[0] for x in prerequisites]

        if plot:
            plt.figure(figsize=(10,2))
            plt.title("Prerequisites By Frequency")
            plt.scatter(np.arange(n),prerequisite_counts[-n:])
            plt.xlim(0,n)
            plt.xlabel("Gene Symbol")
            plt.ylabel("Frequency")
            plt.xticks(np.arange(n),prerequisite_labels[-n:],rotation='vertical')
            plt.show()

        return prerequisite_labels,prerequisite_counts

    # def mean_feature_predictions(self):
    #     return self.forest.weighted_node_prediction(self.nodes)

    def plot_changed(self,n=50,plot=True):
        parents = [n.parent for n in self.nodes if n.parent is not None]

        # ordered_features,ordered_difference = self.forest.node_change_log_fold(parents,self.nodes)
        # ordered_features,ordered_difference = self.forest.node_change_absolute(parents,self.nodes)
        ordered_features,ordered_difference = self.forest.node_change_logistic(parents,self.nodes)

        if plot:
            plt.figure(figsize=(10,2))
            plt.title("Upregulated Genes")
            plt.scatter(np.arange(n),ordered_difference[-n:])
            plt.xlim(0,n)
            plt.xlabel("Gene Symbol")
            plt.ylabel("Change")
            plt.xticks(np.arange(n),ordered_features[-n:],rotation='vertical')
            plt.show()

            plt.figure(figsize=(10,2))
            plt.title("Downregulated Genes")
            plt.scatter(np.arange(n),ordered_difference[:n])
            plt.xlim(0,n)
            plt.xlabel("Gene Symbol")
            plt.ylabel("Change")
            plt.xticks(np.arange(n),ordered_features[:n],rotation='vertical')
            plt.show()

        return ordered_features,ordered_difference

    def score_panel(self,ax):
        coordinates = self.forest.coordinates(type='tsne',no_plot=True)
        scores = self.cell_scores()
        ax.scatter(coordinates[:,0],coordinates[:,1],c=scores)
        ax.set_xticks([])
        ax.set_yticks([])
        return ax


    def feature_panel(self,ax,features,**kwargs):
        panel_array = self.feature_means(features)
        ax.imshow(panel_array,aspect='auto',**kwargs)
        plt.yticks(np.arange(len(features)),features,rotation='horizontal')
        return ax

    def additive_panel(self,ax,features,**kwargs):
        panel_array = self.feature_additives(features)
        ax.imshow(panel_array,aspect='auto',**kwargs,cmap='bwr',)
        # plt.yticks(np.arange(len(features)),features,rotation='horizontal',horizontalalignment='left')
        return ax


    def custom_panel(self,ax,custom,labels=None,**kwargs):

        panel_array = np.zeros((1,custom.shape[1]))

        for i,cf in enumerate(custom.T):
            cells = self.cell_scores()
            feature_mean = (np.sum(cells * cf)) / np.sum(cells)
            panel_array[0,i] = feature_mean

        ax.imshow(panel_array,aspect='auto',**kwargs)
        if labels is not None:
            plt.xticks(np.arange(len(panel_array)),labels,rotation='vertical')
        return ax


    def up_down_panel(self,ax,n=3,mask_fraction=.05):

        text_rectangle(ax,f"Cluster {self.id}",[.04,.88,.52,.08],no_warp=True,linewidth=None)
        # ax.set_title(f"Cluster {self.id}")

        # ordered_features,ordered_difference = self.logistic_sister()
        # ordered_features,ordered_difference = self.changed_log_fold()
        ordered_features,ordered_difference = self.changed_absolute()

        # braid_feature_set = set(self.braid_features())
        # braid_mask = np.array([f in braid_feature_set for f in ordered_features])
        # ordered_features = ordered_features[braid_mask]
        # ordered_difference = ordered_difference[braid_mask]

        ordered_features = ordered_features[::-1]
        ordered_difference = ordered_difference[::-1]
        ordered_difference = [np.around(x,decimals=3) for x in ordered_difference]

        table = ax.table(cellText=np.array([[f[:6] for f in ordered_features[:n]] + [f[:6] for f in ordered_features[-n:]],list(ordered_difference[:n]) + list(ordered_difference[-n:])]).T,cellLoc="center",colLabels=["Symbol","Fold"],bbox=[0,0,.6
        ,.86],transform=ax.transAxes,edges="open")
        table.PAD=.0001
        table.set_fontsize(100)
        table.auto_set_font_size()
        for i in range(n*2):
            table[i+1,0].visible_edges = "R"
        # table[0,0].visible_edges = "B"
        # table[0,1].visible_edges = "B"
        table[0,0].set_text_props(weight='extra bold')
        table[0,1].set_text_props(weight='extra bold')
        table[n,0].visible_edges = "B"
        table[n,1].visible_edges = "BL"

        for i in range(n*2):
            if float(table[i+1,1].get_text().get_text()) < 0:
                table[i+1,1].set_text_props(color='r')
            if float(table[i+1,1].get_text().get_text()) > 0:
                table[i+1,1].set_text_props(color='g')

        scatter_insert = ax.inset_axes([.6,.7,.4,.3])
        coordinates = self.forest.coordinates(no_plot=True)
        scores = self.cell_scores()
        sub_mask = np.random.random(coordinates.shape[0]) < mask_fraction
        scatter_insert.scatter(coordinates[sub_mask][:,0],coordinates[sub_mask][:,1],c=scores[sub_mask])
        scatter_insert.set_xticks([])
        scatter_insert.set_yticks([])

        text_rectangle(ax,f"Mean",[.62,.6,.33,.06],no_warp=True,linewidth=None)
        text_rectangle(ax,f"Samples",[.62,.5,.33,.06],no_warp=True,linewidth=None)
        text_rectangle(ax,str(np.around(self.mean_population(),decimals=1)),[.62,.3,.33,.06],no_warp=True,linewidth=0)

        return ax

    def cluster_children(self,nodes=None,mode='gain',metric='cosine',pca=False,distance='cosine',**kwargs):

        print("Interpreting cluster children")
        print(f"Cluster:{self.id}")

        children = []
        for node in self.nodes:
            children.extend(node.children)

        print(f"Child nodes:{len(children)}")

        child_representation = self.forest.node_representation(children,mode=mode,metric=metric,pca=pca)

        child_labels = len(self.forest.split_clusters) + Forest.sdg_cluster_representation(child_representation,distance=distance,**kwargs)

        self.forest.plot_representation(child_representation,labels=child_labels,mode=mode,metric=metric,pca=pca)

        for child,label in zip(children,child_labels):
            child.set_split_cluster(label)

        cluster_set = set(child_labels)

        print(f"New clusters: {cluster_set}")

        clusters = []

        for cluster in cluster_set:
            split_indices = np.arange(len(children))[child_labels == cluster]
            clusters.append(NodeCluster(self.forest,[children[i] for i in split_indices],cluster))

        self.forest.split_clusters.extend(clusters)

        return clusters


################################
################################
################################
################################
#
#       Rest of module
#       Split out later
#
################################
################################
################################
################################

from sklearn.cluster import AgglomerativeClustering
# import community
# import networkx as nx

def numpy_mad(mtx):
    medians = []
    for column in mtx.T:
        medians.append(np.median(column[column!=0]))
    median_distances = np.abs(mtx - np.tile(np.array(medians), (mtx.shape[0],1)))
    mads = []
    for (i,column) in enumerate(median_distances.T):
        mads.append(np.median(column[mtx[:,i]!=0]))
    return np.array(mads)

def ssme(mtx,axis=None):
    medians = np.median(mtx,axis=0)
    median_distances = np.abs(mtx - np.tile(np.array(medians), (mtx.shape[0],1)))
    ssme = np.sum(np.power(median_distances,2),axis=axis)
    return ssme


def nonzero_var_column(mtx):
    nzv = np.zeros(mtx.shape[1])
    for i in range(mtx.shape[1]):
        nzv[i] = np.var(mtx[:,i][mtx[:,i] != 0])
    return nzv

def sample_node_encoding(nodes,samples):
    encoding = np.zeros((len(nodes),samples),dtype=bool)
    for i,node in enumerate(nodes):
        encoding[i] = node.sample_mask()
    sample_encoding = encoding.T
    unrepresented = np.sum(sample_encoding,axis=1) == 0
    if np.sum(unrepresented) > 0:
        sample_encoding[unrepresented] = 1;
    return sample_encoding


def feature_node_index(nodes,feature):
    encoding = np.zeros(len(nodes),dtype=bool)
    for i,node in enumerate(nodes):
        if feature in node.features:
            encoding[i] = True
    return encoding


def coocurrence_matrix(sample_encoding):

#     co_mtx = np.zeros((sample_encoding.shape[1],sample_encoding.shape[1]))
#     for node in sample_encoding:
#         node_tile = np.tile(node,(node.shape[0],1))
#         intersect = np.logical_and(node_tile,node_tile.T)
#         co_mtx += intersect.astype(dtype=int)
#     return co_mtx

    co_mtx = np.matmul(sample_encoding.astype(dtype=int),sample_encoding.T.astype(dtype=int)).astype(dtype=bool)

    print(co_mtx.shape)

    return np.array(co_mtx)

def coocurrence_distance(sample_encoding):

    co_mtx = coocurrence_matrix(sample_encoding)
    distance_mtx = 1.0/(co_mtx + 1)
    return distance_mtx

def node_tsne_sample(nodes,samples):

    encoding = node_sample_encoding(nodes,samples)
    embedding_model = TSNE(n_components=2,metric='correlation')
    coordinates = embedding_model.fit_transform(encoding)

    #     distances = coocurrence_matrix(encoding)

    return coordinates

def sample_tsne(nodes,samples):

    encoding = node_sample_encoding(nodes,samples)
    print("TSNE Encoding: {}".format(encoding.shape))
    pre_computed_distance = coocurrence_distance(encoding.T)
#     pre_computed_distance = scipy.spatial.distance.squareform(pdist(encoding.T,metric='jaccard'))
    print("TSNE Distance Matrix: {}".format(pre_computed_distance.shape))
#     pre_computed_distance[pre_computed_distance == 0] += .000001
    pre_computed_distance[np.isnan(pre_computed_distance)] = 10000000
    embedding_model = TSNE(n_components=2,metric='precomputed')
    coordinates = embedding_model.fit_transform(pre_computed_distance)

    #     distances = coocurrence_matrix(encoding)

    return coordinates
#
# def node_hdbscan_samples(nodes,samples):
#
#     node_encoding = node_sample_encoding(nodes,samples)
#
#     pre_computed_distance = pdist(node_encoding,metric='cityblock')
#
#     clustering_model = HDBSCAN(min_cluster_size=50, metric='precomputed')
#
# #     plt.figure()
# #     plt.title("Dbscan observed distances")
# #     plt.hist(pre_computed_distance,bins=50)
# #     plt.show()
#
#     clusters = clustering_model.fit_predict(scipy.spatial.distance.squareform(pre_computed_distance))
#
# #     clusters = clustering_model.fit_predict(node_encoding)

    return clusters

def node_gain_table(nodes,forest):
    node_gain_table = np.zeros((len(nodes),len(forest.features)))
    for i,node in enumerate(nodes):
        for j,feature in enumerate(node.features):
            feature_index = forest.truth_dictionary.feature_dictionary[feature]
            try:
                feature_gain = node.absolute_gains[j]
                node_gain_table[i,feature_index] = feature_gain
            except:
                print(node.absolute_gains)
    return node_gain_table
#
def hacked_louvain(nodes,samples):

    node_encoding = node_sample_encoding(nodes,len(samples))
    print("Louvain Encoding: {}".format(node_encoding.shape))
    pre_computed_distance = coocurrence_distance(node_encoding.T)
    print("Louvain Distances: {}".format(pre_computed_distance.shape))
    sample_graph = nx.from_numpy_matrix(pre_computed_distance)
    print("Louvain Cast To Graph")
    least_spanning_tree = nx.minimum_spanning_tree(sample_graph)
    print("Louvain Least Spanning Tree constructed")
    part_dict = community.best_partition(least_spanning_tree)
    print("Louvain Partition Done")
    clustering = np.zeros(len(part_dict))
    for i,sample in enumerate(samples):
        clustering[i] = part_dict[int(sample)]
    print("Louvain: {}".format(clustering.shape))
    return clustering

# def embedded_hdbscan(coordinates):
#
#     clustering_model = HDBSCAN(min_cluster_size=50)
#     clusters = clustering_model.fit_predict(coordinates)
#     return clusters
#
# def sample_hdbscan(nodes,samples):
#
#     node_encoding = node_sample_encoding(nodes,samples)
#     embedding_model = PCA(n_components=100)
#     pre_computed_embedded = embedding_model.fit_transform(node_encoding.T)
#     print("Sample HDBscan Encoding: {}".format(pre_computed_embedded.shape))
# #     pre_computed_distance = coocurrence_distance(node_encoding)
#     pre_computed_distance = scipy.spatial.distance.squareform(pdist(pre_computed_embedded,metric='correlation'))
#     print("Sample HDBscan Distance Matrix: {}".format(pre_computed_distance.shape))
# #     pre_computed_distance[pre_computed_distance == 0] += .000001
#     pre_computed_distance[np.isnan(pre_computed_distance)] = 10000000
#     clustering_model = HDBSCAN(min_samples=3,metric='precomputed')
#     clusters = clustering_model.fit_predict(pre_computed_distance)
#
#     return clusters

def cluster_labels_to_connectivity(labels):
    samples = labels.shape[0]
    clusters = list(set(labels))
    cluster_masks = []
    connectivity = np.zeros((samples,samples),dtype=bool)
    for cluster in clusters:
        cluster_masks.append(labels == cluster)
    for cluster_mask in cluster_masks:
        vertical_mask = np.zeros((samples,samples),dtype=bool)
        horizontal_mask = np.zeros((samples,samples),dtype=bool)
        vertical_mask[:,cluster_mask] = True
        horizontal_mask[cluster_mask] = True
        square_mask = np.logical_and(vertical_mask,horizontal_mask)
        connectivity[square_mask] = True
    return connectivity

def sample_agglomerative(nodes,samples,n_clusters):

    node_encoding = node_sample_encoding(nodes,samples)

    pre_computed_distance = pdist(node_encoding.T,metric='cosine')

    clustering_model = AgglomerativeClustering(n_clusters=n_clusters,affinity='precomputed')

    clusters = clustering_model.fit_predict(scipy.spatial.distance.squareform(pre_computed_distance))

#     clusters = clustering_model.fit_predict(node_encoding)

    return clusters

def stack_dictionaries(dictionaries):
    stacked = {}
    for dictionary in dictionaries:
        for key,value in dictionary.items():
            if key not in stacked:
                stacked[key] = []
            stacked[key].append(value)
    return stacked

def partition_mutual_information(p1,p2):
    p1 = p1.astype(dtype=float)
    p2 = p2.astype(dtype=float)
    population = p1.shape[1]
    intersections = np.dot(p1,p2.T)
    partition_size_products = np.outer(np.sum(p1,axis=1),np.sum(p2,axis=1))
    log_term = np.log(intersections) - np.log(partition_size_products) + np.log(population)
    log_term[np.logical_not(np.isfinite(log_term))] = 0
    mutual_information_matrix = (intersections / population) * log_term
    return mutual_information_matrix

def consolidate_entries(keys,dictionaries):
    consolidated = empty_list_dictionary(keys)
    for dictionary in dictionaries:
        for key,value in iter(dictionary):
            if key not in consolidated:
                consolidated[key] = []
            consolidated[entry].append(value)
    return consolidated

def empty_list_dictionary(keys):
    return {key:[] for key in keys}

def count_list_elements(elements):
    dict = {}
    for element in elements:
        if element not in dict:
            dict[element] = 0
        dict[element] += 1
    return dict

def generate_feature_value_html(features,values,normalization=None,cmap=None):

    if not isinstance(cmap,mpl.colors.Colormap):
        try:
            matplotlib.cm.get_cmap(cmap)
        except:
            from matplotlib.cm import get_cmap
            cmap = get_cmap('viridis')
    if normalization is None:
        from matplotlib.colors import SymLogNorm,DivergingNorm
        normalization = DivergingNorm(0)
        # normalization = SymLogNorm(linthresh=.05)

    html_elements = [
        '<table width="100%">',
        "<style>","th,td {padding:5px;border-bottom:1px solid #ddd;}","</style>",
        "<tr>",
        "<th>","Features","</th>",
        "<th>","Values","</th>",
        "</tr>",
    ]
    for feature,value in zip(features,values):
        value_color_tag = ""
        if normalization is not None:
            normed_value = normalization(value)
            r,g,b,a = cmap(normed_value)
            r,g,b,a = r*100,g*100,b*100,a*100
            value_color_tag = f'style="background-color:rgba({r}%,{g}%,{b}%,50%);"'
        feature_elements = f"""
            <tr>
                <td>{feature}</td>
                <td {value_color_tag}>{value}</td>
            </td>
        """
        html_elements.append(feature_elements)

    html_elements.append("</table>")
    return "".join(html_elements)

def text_rectangle(ax,text,rect,no_warp=True,color=None,edgecolor='b',linewidth=3,**kwargs):

    if color is None:
        try:
            color = mpl.rcParams['text.color']
        except:
            color = 'b'

    from matplotlib.text import TextPath
    import matplotlib.patches as mpatches
    import matplotlib.transforms as trns

    [x,y,w,h,] = rect

    text_path = TextPath((0,0),s=text)
    patch = mpatches.PathPatch(text_path,**kwargs)

    _,_,t_w,t_h = patch.get_extents().bounds

    sx = w/t_w
    sy = h/t_h

    if no_warp:

        [_,_,a_w,a_h] = ax.get_position().bounds
        ax_aspect = a_w/a_h
        patch_aspect = t_w/t_h
        rect_aspect = w/h

        # print("Aspect debug:")
        # print(f"patch aspect:{patch_aspect}")
        # print(f"rect_aspect:{rect_aspect}")
        # print(f"ax_aspect:{ax_aspect}")
        # print(f"ax_dimension:{ax.get_position()}")

        if ax_aspect > patch_aspect:
            sx = sx * (patch_aspect/(rect_aspect*ax_aspect))
        else:
            sy = sy * ((rect_aspect*ax_aspect)/patch_aspect)


    text_path = trns.Affine2D().scale(sx=sx,sy=sy).translate(x,y).transform_path(text_path)

    ax_transform = ax.transAxes
    patch = mpatches.PathPatch(text_path,transform=ax_transform,color=color,edgecolor=edgecolor,linewidth=linewidth,**kwargs)

    ax.add_artist(patch)


if __name__ != "__main__":
    import matplotlib as mpl
    mpl.rcParams['figure.dpi'] = 300
