### Infinite Hidden Markov Model for clustering together tree nodes generated by Random Forests

import numpy as np
import random
import pickle
from functools import reduce
from scipy.misc import comb as nCk
from tree_reader import Node as TreeReaderNode

from itertools import repeat

from scipy.special import logit,expit
from scipy.special import gamma as gamma_f

import multiprocessing as mp

import matplotlib.pyplot as plt



## This model contains several interdependent variables:

##  - Hidden state sequence for nodes
##  - Transition probablities between hidden states
##  - Probability of any given sample emitting 0 or 1 when in a given state
##  - Hyperparameters governing the generative mechanism of the states

## These variables are sampled sequentially

class IHMM():
    def __init__(self,forest,alpha=1,beta=1,gamma=1,beta_e=None,start_states=20,inf_check=False,p=None,hierarchal=True):

        # if __name__ == '__main__':
        self.pool = mp.Pool(p)

        self.inf_check = inf_check
        self.hierarchal = hierarchal

        self.forest = forest

        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma

        self.hidden_states = start_states

        self.nodes = forest.nodes()
        self.live_nodes = forest.stems() + forest.roots()
        self.live_mask = np.zeros(len(self.nodes),dtype=bool)

        for node in self.live_nodes:
            self.live_mask[node.index] = True

        self.total_features = len(forest.output_features)
        self.feature_dictionary = forest.truth_dictionary.feature_dictionary
        self.total_nodes = len(self.nodes)

        self.node_states = np.zeros(self.total_nodes,dtype=int)

        self.child_index_l = np.zeros(self.total_nodes,dtype=int)
        self.child_index_r = np.zeros(self.total_nodes,dtype=int)
        self.child_state_l = np.zeros(self.total_nodes,dtype=int)
        self.child_state_r = np.zeros(self.total_nodes,dtype=int)

        for node in self.live_nodes:
            self.child_index_l[node.index] = node.children[0].index
            self.child_index_r[node.index] = node.children[1].index

        self.node_sample_encoding = forest.node_sample_encoding(self.nodes)
        self.node_samples = np.sum(self.node_sample_encoding,axis=0)

        self.node_features = np.zeros((self.total_nodes,self.total_features))
        self.node_feature_mask = np.zeros((self.total_nodes,self.total_features),dtype=bool)

        for node in self.live_nodes:
            node_features = node.features
            local_gains = np.log2(np.abs(node.local_gains) + 1) * np.sign(node.local_gains)
            # local_gains = node.local_gains
            # local_gains = node.medians
            for feature,gain in zip(node_features,local_gains):
                fi = self.feature_dictionary[feature]
                self.node_features[node.index,fi] = gain
                self.node_feature_mask[node.index,fi] = True

        for node in self.live_nodes:
            self.node_states[node.index] = random.randint(1,self.hidden_states) + 1

        self.oracle_indicator_l = np.random.rand(self.total_nodes) < .5
        self.oracle_indicator_l[np.logical_not(self.live_mask)] = False

        self.oracle_indicator_r = np.random.rand(self.total_nodes) < .5
        self.oracle_indicator_r[np.logical_not(self.live_mask)] = False

        if beta_e is None:
            beta_e = np.sum(self.live_mask)
        self.beta_e = beta_e

        self.initialize()
    ## Here we begin the methods for updating the state of the model over many "sweeps"

    ## Things that stay constant:
    ## Node feature gains/Features present in nodes
    ## Number of samples
    ## Number of nodes
    ## State of nodes that aren't live or marked as null state
    ##    - These nodes don't have a gain of information (probably because they're leaves), so they can't be modeled
    ## Node relations (eg child indices)
    ##    -  Topology of the forest remains constant, we're only trying to learn it

    ## Things that update:
    ## Node states for live nodes
    ## Number of states
    ## Emission model for each state
    ## Number of transitions between states
    ## State of children
    ## Hyperparameters

    ## Dependence flow:

    ## Node state gives gives=>
    ##  - Number of hidden states
    ##  - State masks

    ## - Number of states + masks + oracle transition indicator =>
    ##  - Transitions
    ##  - Oracle transitions
    ##  - State gain expectation and covariance

    ## - Transitions + number of states =>
    ##  - Transition log odds

    ## - Oracle indicator + Transitions =>
    ##  - MAP Hyperparameters

    ###### HERE WE BEGIN TO RECOMPUTE THE NODE STATE ####

    ## - Transition log odds + state emission model (Wishart) =>
    ##  - Log odds of direct state transitions per node
    ##  - Log odds of oracle per node

    ####### HERE WE DO THE FIRST SAMPLING STEP ##########

    ## - Oracle indicator + oracle log odds =>
    ##  - Log odds given oracle
    ##  - Log odds of secondary oracle

    ################ SECOND SAMPLING STEP ###############

    ##  - New states discovered
    ##  - Loop back to recomputing transitions & hypers

    def initialize(self):
        self.compute_data_properties()
        self.initialize_states()
        #
        self.node_states = self.clean_state_indeces(self.node_states)
        self.state_masks = self.recompute_state_masks(self.node_states)
        self.reestablish_states(self.state_masks)


    def establish_parameters(self):

        ## This method establishes descriptive parameters given some sequence of hidden states over the node sequences.

        print(self.hidden_states)
        print(self.node_states)
        print(list(np.sum(self.state_masks,axis=1)))

        ### Here we re-assign hidden states to cleaned up indecies, in order to eliminate hidden states that no longer exist

        self.node_states = self.clean_state_indeces(self.node_states)

        ## Here we update the states assigned to the children of each node, this is setup to count the transition frequencies

        self.update_node_relations()

        ## State masks are a more conveninet way of storing which nodes belong to which states. They are numpy boolean mask arrays

        self.state_masks = self.recompute_state_masks(self.node_states)

        self.reestablish_states(self.state_masks)

        self.transition_counts = self.recompute_transition_counts(self.live_mask,self.oracle_indicator_l,self.oracle_indicator_r,self.node_states,self.child_state_l,self.child_state_r)

        ## Here we count which transitions occurred through an oracle in order to recalculate the oracle transition frequencies

        self.oracle_transition_matrix = self.recompute_oracle_transition_matrix(self.oracle_indicator_l,self.oracle_indicator_r,self.node_states,self.child_state_l,self.child_state_r)
        self.oracle_transition_counts = self.recompute_oracle_transition_counts(self.oracle_transition_matrix)

        ## And finally here, on the basis of how often the oracle was used and the number of observed states, we sample the hyperparameters

        self.sample_hypers()




    def update_states(self):

        virtual_states = len(self.components)

        ### The above methods establish a description of the current state.

        ### The methods below establish the log odds of each given state for each given node, and whether an oracle was used to reach it

        ## First we establish the log odds of a given state based on the divergence observed

        self.state_log_odds_given_features = self.compute_state_log_odds_given_features(self.node_features,self.node_feature_mask,self.live_mask)

        # self.state_log_odds_given_divergence /= self.hidden_states

        ## This matrix contains only odds of existing states based on divergence, eg it is hidden_states x nodes in dimension, plus an extra row for a novel state

        if self.inf_check:
            print("Feature odds")
            print(list(self.state_log_odds_given_features))
            if np.isnan(self.state_log_odds_given_features).any():
                raise Exception("NaN state log odds given features")
            if np.isinf(self.state_log_odds_given_features).any():
                raise Exception("Inf state log odds given features")

        #### IMPORTANT ####

        ## This next section works with raw odds because there is a need to add together the odds of a state given oracle and given no oracle_mask

        #### DO NOT MIX ODDS AND LOG ODDS, BAD ####

        ## Next we establish the odds of a given state or the oracle given its children

        self.direct_state_odds_given_child_l = self.compute_state_odds_given_child_direct_transition(self.beta,self.transition_counts,self.node_states,self.child_state_l)
        self.direct_state_odds_given_child_r = self.compute_state_odds_given_child_direct_transition(self.beta,self.transition_counts,self.node_states,self.child_state_r)

        ## We extract the odds of visiting the oracle for each given node and tile them

        self.oracle_odds_given_child_l = np.tile(self.direct_state_odds_given_child_l[0],(virtual_states-1,1))
        self.oracle_odds_given_child_r = np.tile(self.direct_state_odds_given_child_r[0],(virtual_states-1,1))

        if self.inf_check:
            print(list(self.direct_state_odds_given_child_l))
            print(list(self.direct_state_odds_given_child_r))
            if np.isnan(self.oracle_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("NaN oracle odds")
            if np.isnan(self.oracle_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("NaN oracle odds")

            if np.isinf(self.oracle_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("Inf oracle odds")
            if np.isinf(self.oracle_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("Inf oracle odds")


            if (np.sum(self.direct_state_odds_given_child_l,axis=0) == 0).any():
                raise Exception("Total zero transition odds")
            if (np.sum(self.direct_state_odds_given_child_r,axis=0) == 0).any():
                raise Exception("Total zero transition odds")



        # print("ORACLE_ODDS_DEBUG")
        # print(self.oracle_odds_given_child_l.shape)
        # print(self.oracle_odds_given_child_r.shape)

        ### THIS IS IMPORTANT ###
        ## We NOW set the oracle rows to zero, so that it will be filled in with the odds of a novel state momentarily
        self.direct_state_odds_given_child_l[0] = 0
        self.direct_state_odds_given_child_r[0] = 0

        if self.inf_check:
            if np.isnan(self.direct_state_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("NaN direct state odds")
            if np.isnan(self.direct_state_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("NaN direct state odds")
            if np.isinf(self.direct_state_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("Inf direct state odds")
            if np.isinf(self.direct_state_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("Inf direct state odds")

        ## Next we need to establish the odds of a given state given that an oracle was visited
        ## These are tiled across every node because they are uniform

        self.oracle_odds = self.compute_state_odds_given_oracle(self.gamma,self.total_nodes,self.oracle_transition_counts)

        if self.inf_check:
            print(list(self.oracle_odds))
            if np.isnan(self.oracle_odds[:,self.live_mask]).any():
                raise Exception("NaN oracle odds")
            if np.isinf(self.oracle_odds[:,self.live_mask]).any():
                raise Exception("Inf oracle odds")


        ## Next we get the odds of obtaining each state by the oracle route through multiplication:

        self.state_odds_given_oracle_l = self.oracle_odds_given_child_l * self.oracle_odds
        self.state_odds_given_oracle_r = self.oracle_odds_given_child_r * self.oracle_odds

        if self.inf_check:
            if np.isnan(self.state_odds_given_oracle_l[:,self.live_mask]).any():
                mask = np.logical_and(np.sum(np.isnan(self.state_odds_given_oracle_l),axis=0) > 0,self.live_mask)
                print("Total Odds")
                print(list(self.state_odds_given_oracle_l[:,mask]))
                print("Direct Odds")
                print(list(self.direct_state_odds_given_child_l[:,mask]))
                print(list(self.direct_state_odds_given_child_r[:,mask]))
                print("Odds of Oracle")
                print(list(self.oracle_odds_given_child_l[:,mask]))
                print(list(self.oracle_odds_given_child_r[:,mask]))
                print("Oracle odds")
                print(list(self.oracle_odds[:,mask]))
                raise Exception("NaN state odds given oracle")
            if np.isinf(self.state_odds_given_oracle_l[:,self.live_mask]).any():
                raise Exception("Inf state odds given oracle")
            if np.isnan(self.state_odds_given_oracle_r[:,self.live_mask]).any():
                raise Exception("NaN state odds given oracle")
            if np.isinf(self.state_odds_given_oracle_r[:,self.live_mask]).any():
                raise Exception("Inf state odds given oracle")



        # print("ORACLE_ODDS_DEBUG2")
        # print(self.state_odds_given_oracle_l.shape)
        # print(self.state_odds_given_oracle_r.shape)

        ## Finally we combine the odds and transform them into log form
        ## NOTE the final row is now the odds of a novel state, because it is the odds of visiting the oracle multiplied by the odds of obtaining a new state from the oracle

        self.state_odds_given_child_l = self.direct_state_odds_given_child_l + self.state_odds_given_oracle_l
        self.state_odds_given_child_r = self.direct_state_odds_given_child_r + self.state_odds_given_oracle_r

        if self.inf_check:
            if np.isnan(self.state_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("NaN child state odds")
            if np.isnan(self.state_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("NaN child state odds")

            if (self.state_odds_given_child_l < 0).any():
                raise Exception("Negative odds!")
            if (self.state_odds_given_child_r < 0).any():
                raise Exception("Negative odds!")

        ## And here we transform the plain odds into log odds:

        self.state_log_odds_given_child_l = np.log2(self.state_odds_given_child_l)
        self.state_log_odds_given_child_r = np.log2(self.state_odds_given_child_r)

        if self.inf_check:
            if np.isnan(self.state_log_odds_given_child_l[1:,self.live_mask]).any():
                raise Exception("NaN child state log odds")
            if np.isnan(self.state_log_odds_given_child_r[1:,self.live_mask]).any():
                raise Exception("NaN child state log odds")
            if np.isinf(self.state_log_odds_given_child_l[1:,self.live_mask]).any():
                raise Exception("Inf child state log odds")
            if np.isinf(self.state_log_odds_given_child_r[1:,self.live_mask]).any():
                raise Exception("Inf child state log odds")


        #THIS IS A TEMPORARY HACK FOR AN UNSTRUCTURED DP MIXTURE MODEL:
        # self.state_log_odds_given_child_l = np.log2(self.oracle_odds)
        # self.state_log_odds_given_child_r = np.log2(self.oracle_odds)
        #TEMP HACK ENDS HERE

        ## Finally we want to combine all log odds and sample the resulting distribution

        self.state_log_odds = self.state_log_odds_given_features + self.state_log_odds_given_child_l + self.state_log_odds_given_child_r

        new_node_states = self.node_states.copy()
        new_state_indicator = np.zeros(self.total_nodes,dtype=bool)

        new_node_states[self.live_mask],new_state_indicator[self.live_mask] = self.sample_states(self.live_mask,self.state_log_odds)

        oracle_probability_l,oracle_probability_r = self.oracle_indicator_probabilities(new_node_states,self.live_mask,self.direct_state_odds_given_child_l,self.direct_state_odds_given_child_r,self.state_odds_given_oracle_l,self.state_odds_given_oracle_r)

        new_oracle_indicator_l,new_oracle_indicator_r = self.sample_oracle_indicator(oracle_probability_l,oracle_probability_r)

        if self.inf_check:
            if np.isnan(self.state_log_odds[1:,self.live_mask]).any():
                raise Exception("NaN additive log state odds")
            if np.isinf(self.state_log_odds[1:,self.live_mask]).any():
                raise Exception("Inf additive log state odds")

        new_oracle_indicator_l[new_state_indicator] = True
        new_oracle_indicator_r[new_state_indicator] = True

        total_new_states = np.sum(new_state_indicator)

        # print("New states created:")
        # print(total_new_states)
        # new_node_states[new_state_indicator] = np.arange(self.hidden_states, self.hidden_states + total_new_states)

        print("New state created")
        print(total_new_states)


        return new_node_states,new_oracle_indicator_l,new_oracle_indicator_r

    def sweep(self):

        self.establish_parameters()

        new_node_states,new_oracle_indicator_l,new_oracle_indicator_r = self.update_states()

        self.node_states = new_node_states
        self.oracle_indicator_l = new_oracle_indicator_l
        self.oracle_indicator_r = new_oracle_indicator_r


    def subsample_sweep(self,fraction=.1):

        self.establish_parameters()

        global_live_mask = self.live_mask.copy()

        self.live_mask[self.live_mask] = np.random.random(np.sum(self.live_mask)) < fraction

        new_node_states,new_oracle_indicator_l,new_oracle_indicator_r = self.update_states()

        self.node_states[self.live_mask] = new_node_states[self.live_mask]
        self.oracle_indicator_l[self.live_mask] = new_oracle_indicator_l[self.live_mask]
        self.oracle_indicator_r[self.live_mask] = new_oracle_indicator_r[self.live_mask]

        self.live_mask = global_live_mask


    def max_likelihood_sweep(self):

        ## This sweep assigns nodes to their most likely state, not a state sampled at random.

        self.sweep()
        new_node_states = np.zeros(self.node_states.shape[0])
        new_node_states[self.live_mask] = np.argmax(self.state_log_odds[:,self.live_mask],axis=0)
        self.node_states = new_node_states
        self.establish_parameters()


    def sample_hypers(self):


        print("Sampling Hypers")
        # print(f"Beta:{self.beta}")
        # print(f"Gamma:{self.gamma}")

        self.beta = self.recompute_beta()
        self.gamma = self.recompute_gamma()
        self.beta_e = self.recompute_beta_e()

        print(f"Beta:{self.beta}")
        print(f"Gamma:{self.gamma}")


    def recompute_beta(self):

        ## Here we compute the maximum a-posteriori probability for the beta parameter
        ## Due to falling factorials in the formula, we will want to use a stirling approximation,
        ## For this it is convenient to pre-compute a series of logs of integers

        ## The likelihood of beta is given by: beta ^ (k-1) / (sum of all non-oracle transitions)

        k = self.hidden_states

        transition_matrix = self.transition_counts + self.oracle_transition_matrix

        sums = np.sum(transition_matrix,axis=1)

        # max_sum = int(np.sum(sums) * 2)
        max_sum = int(np.max(sums) * 2)

        log_sequence = np.log2(np.arange(1,max_sum*2))

        likelihood_sequence = np.zeros(max_sum)[1:]

        for i,transitions in enumerate(transition_matrix):
            transitions = self.transition_counts[i]
            # total = np.sum(transitions)
            total = int(sums[i])
            non_zero = int(np.sum(transitions > 0))
            l2ls = lambda b: (non_zero * np.log2(b)) - np.sum(log_sequence[b:total+b])
            likelihood_sequence += np.array([l2ls(b) for b in range(1,max_sum)])

        likelihood_sequence -= log_sequence[:len(likelihood_sequence)]

        beta = np.argmax(likelihood_sequence) + 1

        return beta

    def recompute_gamma(self,max_gamma=None):

        k = self.hidden_states - 1

        oracle_total = int(np.sum(self.oracle_transition_counts))

        if max_gamma is None:

            max_gamma = oracle_total + 1

        log_sequence = np.log2(np.arange(1,max_gamma*2))

        # l2l = lambda g: ((k * np.log2(g)) - (np.sum(log_sequence[g+1:oracle_total+g+1]) + (g * 1.44)))
        l2l = lambda g: ((k * np.log2(g)) - (np.sum(log_sequence[g+1:oracle_total+g+1]) + np.log2(g)))

        likelihood_sequence = np.array([l2l(g) for g in range(1,max_gamma)])

        gamma = np.argmax(likelihood_sequence)+1

        # print("GAMMA DEBUG")
        # print(len(likelihood_sequence))
        # print(gamma)
        # print(max_gamma)
        # print(oracle_total)
        # print(likelihood_sequence[:50])
        # print("Alleged maximum likelihood")
        # print(likelihood_sequence[gamma-1])

        if (gamma + 1 == max_gamma) and max_gamma < self.total_nodes * 100:
            gamma = self.recompute_gamma(max_gamma = max_gamma * 5)

        return gamma

    def recompute_beta_e(self,beta_max=None):

        k = self.hidden_states
        features = self.total_features
        if beta_max is None:
            beta_max = max(k,features)

        trace_product = self.state_precisions()[:k,np.identity(features,dtype=bool)] * np.tile(self.precision_prior()[np.identity(features,dtype=bool)],(k,1))

        trace_sum = np.sum(trace_product + np.ones((k,features)))

        log_trace_sum = np.sum(np.log(trace_product + np.ones((k,features))))

        log_cum_sum = np.cumsum(np.log(np.arange(1,beta_max)))

        def likelihood(beta): return (-k * log_cum_sum[int(beta/2)]) + (-1/(2 * beta)) + (((k*beta - 3)/2) * (beta / 2)) + ((beta / 2) * (log_trace_sum - trace_sum))

        sequence = np.array([likelihood(b) for b in range(1,beta_max)])

        beta_e = np.argmin(sequence) + 1
        # other_beta = np.argmax(sequence) + 1

        print("BETA_E DEBUG")
        print("beta_e")
        print(beta_e)
        # print("Trace sum")
        # print(trace_sum)
        # print("Log trace sum")
        # print(log_trace_sum)
        # print("Other")
        # print(other_beta)
        # print(sequence[:10])

        if beta_e == len(sequence):
            if beta_max < 999999:
                beta_e = self.recompute_beta_e(beta_max = beta_max * 10)

        return beta_e

        # return self.total_features
        # return (self.total_features * 2) - 1


    def update_node_relations(self):

        self.child_state_l[self.live_mask] = self.node_states[self.child_index_l][self.live_mask]
        self.child_state_r[self.live_mask] = self.node_states[self.child_index_r][self.live_mask]

    def clean_state_indeces(self,node_states):

        print("State Index Cleanup")
        new_states = sorted(list(set(node_states)))
        if new_states[:1] != [0,]:
            new_states = [0,] + new_states
        if new_states[1:2] != [1,]:
            new_states = [0,1] + new_states[1:]
        new_indices = {old_index:i for i,old_index in enumerate(new_states)}
        print(new_indices)

        new_state_sequence = np.zeros(node_states.shape,dtype=int)

        for i,old_state in enumerate(node_states):
            new_state_sequence[i] = new_indices[old_state]

        self.hidden_states = len(new_states[2:])

        return new_state_sequence

    def recompute_state_masks(self,node_states):

        print("Computing State Masks")

        print("HIDDEN_STATES")
        print(self.hidden_states)

        new_states = self.hidden_states

        new_state_masks = np.zeros((new_states,node_states.shape[0]),dtype=bool)

        for state in range(new_states):
            new_state_masks[state] = node_states == state+2

        # print(new_state_masks.shape)

        if np.sum(self.node_states[self.live_mask] == 0) > 0:
            raise Exception()

        return new_state_masks

    def initialize_states(self):

        components = []

        components.append(Component(np.zeros(self.total_nodes,dtype=bool),self.node_summary()))
        components.append(Component(self.live_mask,self.node_summary()))

        for component in components:
            component.initialize()

        self.components = components

    def reestablish_states(self,state_masks):

        summary = self.node_summary()

        print("Reestablish debug")
        print("hidden_states")
        print(self.hidden_states)

        mean_prior,mean_prior_precision,prior_power = self.mean_priors()
        covariance_prior,precision_prior,prior_power = self.scale_priors()

        # print(self.hidden_states)
        # print(mean_prior)
        # print(covariance_prior)
        # print(precision_prior)

        components = []

        components.append(Component(np.zeros(self.total_nodes,dtype=bool),summary))
        components.append(Component(self.live_mask,summary))

        for mask in state_masks:
            component = Component(mask,summary)
            components.append(component)

        # for component in components:
        #     component.estimate_paramters(mean_prior,mean_prior_precision,covariance_prior,precision_prior,prior_power)

        component_async_handles = []
        component_results = []
        for component in components:
            component_async_handles.append(self.pool.apply_async(Component.estimate_async,((component,mean_prior,mean_prior_precision,covariance_prior,precision_prior,prior_power),)))
        for i,cr in enumerate(component_async_handles):
            print(f"Computing component {i}")
            component_results.append(cr.get())

        self.components = component_results

    def node_summary(self):
        node_summary = {}
        node_summary['beta_e'] = self.beta_e
        node_summary['features'] = self.total_features
        node_summary['live_mask'] = self.live_mask
        node_summary['node_samples'] = self.node_samples
        node_summary['node_features'] = self.node_features
        node_summary['node_feature_mask'] = self.node_feature_mask
        return node_summary

    def node_masked_summary(self,mask):
        node_summary = {}
        node_summary['beta_e'] = self.beta_e
        node_summary['features'] = self.total_features
        node_summary['mask'] = mask
        node_summary['node_features'] = self.node_features[mask]
        node_summary['node_samples'] = self.node_samples[mask]
        node_summary['node_feature_mask'] = self.node_feature_mask[mask]
        return node_summary

    def feature_masked_summary(self,mask):
        node_summary = {}
        node_summary['beta_e'] = self.beta_e
        node_summary['features'] = self.total_features - np.sum(mask)
        node_summary['mask'] = mask
        node_summary['node_samples'] = self.node_samples
        node_summary['node_features'] = self.node_features[:,mask]
        node_summary['node_feature_mask'] = self.node_feature_mask[:,mask]
        return node_summary

    def double_masked_summary(self,node_mask,feature_mask):
        node_summary = {}
        node_summary['beta_e'] = self.beta_e
        node_summary['features'] = self.total_features - np.sum(feature_mask)
        node_summary['node_mask'] = node_mask
        node_summary['feature_mask'] = feature_mask
        node_summary['node_features'] = self.node_features[node_mask].T[feature_mask].T
        node_summary['node_samples'] = self.node_samples[mask]
        node_summary['node_feature_mask'] = self.node_feature_mask[node_mask].T[feature_mask].T
        return node_summary


    def estimate_node_means(node_mask,node_summary):

        feature_sums = np.sum(node_summary['node_features'][node_mask],axis=0)
        state_feature_node_totals = np.ones(node_summary['features']) + np.sum(node_summary['node_feature_mask'][node_mask],axis=0)

        return (feature_sums / state_feature_node_totals , np.sum(node_mask))

    def estimate_node_covariance_precision(node_mask,node_summary,prior=None,feature_means=None):

        nodes = np.sum(node_mask)
        features = node_summary['features']
        node_samples = node_summary['node_samples']
        node_features = node_summary['node_features']
        node_feature_mask = node_summary['node_feature_mask']
        mean_samples = np.mean(node_samples[node_mask])
        if np.isnan(mean_samples):
            mean_samples = 1

        if prior is None:
            prior = (np.identity(features),nodes)

        prior_matrix,prior_power = prior

        if feature_means is None:
            feature_means = IHMM.estimate_node_means(node_mask,node_summary)[0]

        centered_features = np.zeros((nodes,features))
        centered_features[node_feature_mask[node_mask]] = node_features[node_mask][node_feature_mask[node_mask]]
        centered_features[node_feature_mask[node_mask]] -= np.tile(feature_means,(nodes,1))[node_feature_mask[node_mask]]

        # raw_outer_sum = np.sum(np.array([np.outer(x,x) for x in centered_features]),axis=0)

        ### Do this instead of the above, huge memory savings, don't have to hold a node x feature x feature matrix in mem

        raw_outer_sum = np.zeros((features,features))
        for nf in centered_features:
            raw_outer_sum += np.outer(nf,nf)

        # scaling_value = (np.sum(np.array([np.outer(x,x) for x in node_feature_mask[node_mask]]),axis=0) + 1) / (nodes + 1)

        scaling_value = np.zeros((features,features))
        for fm in node_feature_mask[node_mask]:
            scaling_value += np.outer(nf,nf)
        scaling_value = (scaling_value + 1) / (nodes+1)

        # print("scaling_value_debug")
        # print(scaling_value)


        scaled_outer_sum = (raw_outer_sum / scaling_value)

        unscaled_covariance_estimate = scaled_outer_sum + (prior_matrix * (prior_power + 1))

        unscaled_precision_estimate = np.linalg.inv(unscaled_covariance_estimate)

        covariance_estimate = unscaled_covariance_estimate / (2 + nodes + features)

        precision_estimate = (2 + nodes + features) * unscaled_precision_estimate

        # precision_estimate = np.linalg.inv(covariance_estimate)

        # print("Scale Debug")
        # print(raw_outer_sum)
        # print(scaling_value)
        # print(scaled_outer_sum)
        # print(mean_samples)
        # print(unscaled_covariance_estimate)
        # print(unscaled_precision_estimate)

        return (covariance_estimate,precision_estimate)

    def compute_data_properties(self):

        self.data_means = IHMM.estimate_node_means(self.live_mask,self.node_summary())[0]
        self.data_covariance,self.data_precision = IHMM.estimate_node_covariance_precision(self.live_mask,self.node_summary())

    def mean_priors(self):

        mean_prior = (np.sum(np.array([c.means for c in self.components[2:]]),axis=0) + self.data_means) / (self.hidden_states + 1)
        mean_prior_precision = (np.sum(np.array([np.outer(c.means,c.means) for c in self.components[2:]]),axis=0) + (self.hidden_states * np.identity(self.total_features))) / (self.hidden_states + self.total_features + 2)

        return (mean_prior,mean_prior_precision,self.beta_e)

    def covariance_prior(self):
        covariance_prior = (np.sum(np.array([c.covariance for c in self.components[2:]]),axis=0) + self.data_covariance) / (self.hidden_states + 1)
        return covariance_prior

    def precision_prior(self):
        precision_prior = (np.sum(np.array([c.precision for c in self.components[2:]]),axis=0) + self.data_precision) / (self.hidden_states + 1)
        return precision_prior


    def scale_priors(self):

        covariance_prior = self.covariance_prior()
        precision_prior = self.precision_prior()

        return (covariance_prior,precision_prior,self.beta_e)

    def state_means(self):
        states = len(self.components[1:])
        features = self.total_features
        m = np.zeros((states,features))
        for i,state in enumerate(self.components[1:]):
            cv[i] = state.means
        return m

    def state_covariances(self):
        states = len(self.components[1:])
        features = self.total_features
        cv = np.zeros((states,features,features))
        for i,state in enumerate(self.components[1:]):
            cv[i] = state.covariance
        return cv

    def state_precisions(self):
        states = len(self.components[1:])
        features = self.total_features
        pr = np.zeros((states,features,features))
        for i,state in enumerate(self.components[1:]):
            pr[i] = state.precision
        return pr

    def recompute_transition_counts(self,live_mask,oracle_indicator_l,oracle_indicator_r,node_states,child_state_l,child_state_r):

        print("Recomputing Transition Counts")

        states = len(self.components)

        new_transition_counts = np.zeros((states,states))

        transition_mask_l = np.logical_and(live_mask,np.logical_not(oracle_indicator_l))
        transition_mask_r = np.logical_and(live_mask,np.logical_not(oracle_indicator_r))

        # transition_mask = live_mask

        for ps,csl in zip(node_states[transition_mask_l],child_state_l[transition_mask_l]):
            # print(csl)
            # print(ps)
            new_transition_counts[csl,ps] += 1

        for ps,csr in zip(node_states[transition_mask_r],child_state_r[transition_mask_r]):
            # print(csr)
            # print(ps)
            new_transition_counts[csr,ps] += 1

        # print(new_transition_counts)

        return new_transition_counts

    def recompute_oracle_transition_matrix(self,oracle_indicator_l,oracle_indicator_r,node_states,child_state_l,child_state_r):

        print("Recomputing oracle transition count")

        states = len(self.components)

        new_oracle_transitions = np.zeros((states,states))

        for ps,csl in zip(node_states[oracle_indicator_l],child_state_l[oracle_indicator_l]):
            new_oracle_transitions[csl,ps] += 1

        for ps,csr in zip(node_states[oracle_indicator_r],child_state_r[oracle_indicator_r]):
            new_oracle_transitions[csr,ps] += 1

        new_oracle_transitions[0] = 0

        # print(new_oracle_transitions)

        return new_oracle_transitions


    def recompute_oracle_transition_counts(self,oracle_transition_matrix):

        print("Recomputing oracle transition count")

        new_oracle_transitions = np.ones(oracle_transition_matrix.shape[0])

        new_oracle_transitions += np.sum(oracle_transition_matrix,axis=0)

        new_oracle_transitions[0] = 0

        # print(new_oracle_transitions)

        return new_oracle_transitions

    def compute_state_log_odds_given_features(self,node_features,node_feature_mask,live_mask):

        print("Computing state log odds | features")

        ## Here we have a wrapper function that allows us to parallelize the node state odds computation
        ## An inner function allows us to compute multiple states at the same time in a process pool

        feature_log_odds = np.zeros((len(self.components[1:]),node_features.shape[0]))

        # print("Node features")
        # print(node_features[live_mask].shape)
        # print(node_feature_mask[live_mask].shape)

        for i,component in enumerate(self.components[1:]):
            print(f"Component {i} log likelihood")
            feature_log_odds[i][live_mask] = np.array(self.pool.map(component.node_log_likelihood_async,zip(node_features[live_mask],node_feature_mask[live_mask])))

        # print("Node ratio debug")
        # print(feature_log_odds.shape)
        # print(feature_log_odds[:,live_mask].shape)

        # if np.isnan(node_output).any():
        #     raise Exception("Computed nan divergence log odds")

        return feature_log_odds

    def node_state_log_odds_given_features(task):

        node_index,node_features,node_feature_mask = task

        total_node_features = np.sum(node_feature_mask)

        k = state_feature_means.shape[0]

        centered_features = np.tile(node_features[node_feature_mask],(k,1)) - state_feature_means[:,node_feature_mask]

        masked_precisions = state_feature_precisions[:,node_feature_mask].T[node_feature_mask].T

        state_log_likelihood = np.zeros(k)


    def compute_state_odds_given_child_direct_transition(self,beta,transition_counts,node_states,node_child_states):

        ## Here we have to start considering the potential fact that an oracle may be sampled
        ## Since this is the first transition, for simplicity we will simply compute the odds of a direct transition and an oracle transition of any kind

        padded_transition_counts = transition_counts[:,1:]
        padded_transition_counts[:,0] += beta

        # print("Transition odds debug")
        # print(padded_transition_counts)

        total_transitions = np.sum(padded_transition_counts,axis=1)

        ## Here we  compute the odds of any other transition besides a given transition. It's important to normalize the transitions by an additional beta.
        ## This is important to prevent potential infinite odds being present when the ONLY transitionto a state is via the oracle.

        counter_odds = np.tile(total_transitions,(padded_transition_counts.shape[1],1)).T + beta
        counter_odds -= padded_transition_counts

        ## Finally we again have to account for the idea that when a node is being evaluated, we have to remove its influence from the transition counts
        ## To do this we will calculate the odds of any given state given a child, and then reduce by one the odds of the state that the node currently occupies

        ## This is important to allow an IHMM to converge from many states to few. When a large number of states are present, the oracle parameter is inferred
        ## to be small, which gives an outsize influence to the transitions that rare states anticipate based on their parent nodes.
        ## Given this, an IHMM can get trapped in a situation where many states containing only one node exist, forcing beta to remain small,
        ## but not allowing the nodes belonging to singleton states to explore other states.

        node_transition_odds = padded_transition_counts[node_child_states]

        print("transition_debug")
        print(node_transition_odds[2000])
        print(node_transition_odds.shape)

        # print("direct transition debug")
        # print(node_transition_odds.shape)
        # print(list(node_transition_odds))

        # node_transition_odds[np.arange(node_transition_odds.shape[0]),node_states] -= 1

        # node_transition_odds[node_transition_odds < 0] = 0
        # print(list(node_transition_odds))

        node_transition_counter_odds = counter_odds[node_child_states]

        # print(node_transition_counter_odds.shape)

        # transition_odds = padded_transition_counts.astype(dtype=float) / counter_odds.astype(dtype=float)

        # print(transition_odds)

        node_state_odds = node_transition_odds.astype(dtype=float) / node_transition_counter_odds.astype(dtype=float)

        print("transition debug")
        print(transition_counts.shape)
        print(node_state_odds.shape)

        return node_state_odds.T

    def log_sampling(sequence):
        sort = np.argsort(sequence)[::-1]
        sorted_log = sequence[sort]
        sorted_log -= sorted_log[0]
        raw_odds = np.exp2(sorted_log)

        draw = random.random()*np.sum(raw_odds)

        for i,w in zip(sort,raw_odds):
            draw -= w
            if draw <= 0 or (not np.isfinite(draw)):
                return i

        return len(raw_odds) - 1

    def sample_states(self,live_mask,state_log_odds):

        print("Sampling states")

        live_nodes = np.sum(live_mask)

        draw_index = self.pool.map(IHMM.log_sampling,state_log_odds[1:,live_mask].T)
        draw_index = [i+2 for i in draw_index]

        new_state_indicator = [di == 1 for di in draw_index]

        return np.array(draw_index), np.array(new_state_indicator)

    def compute_state_odds_given_oracle(self,gamma,nodes,oracle_transition_counts):

        print("Computing oracle odds")

        print(oracle_transition_counts)

        odds = np.ones(oracle_transition_counts[1:].shape[0])
        odds += oracle_transition_counts[1:]
        odds[0] += gamma


        counter_odds = np.sum(odds) * np.ones(odds.shape)
        counter_odds -= odds


        fractional_odds = odds/counter_odds

        ## Here we tile the odds across all nodes, because they are uniform
        fractional_odds = np.tile(fractional_odds,(nodes,1)).T

        # print("Oracle Odds Debug")
        # print(odds)
        # print(counter_odds)
        # print(fractional_odds)

        return fractional_odds

    def oracle_indicator_probabilities(self,node_state,live_mask,direct_state_odds_l,direct_state_odds_r,oracle_state_odds_l,oracle_state_odds_r):

        # print("Oracle sampler debug")

        states = direct_state_odds_l.shape[0]
        nodes = node_state.shape[0]
        live_nodes = np.sum(live_mask)
        total_samples = live_mask.shape[0]

        # print(states)
        # print(nodes)
        # print(live_nodes)
        # print(total_samples)

        # print(direct_state_odds_l.shape)
        # print(direct_state_odds_r.shape)


        # print(self.hidden_states)

        state_mask = np.equal(np.tile(np.arange(states),(live_nodes,1)).T,np.tile(node_state[live_mask]-1,(states,1)))

        # print(np.sum(np.sum(state_mask,axis=0) < 1))
        # print(np.sum(np.sum(state_mask,axis=1) < 1))
        #
        # print(node_state[np.sum(state_mask,axis=0) < 1])

        if self.inf_check:
            if np.sum(state_mask,axis=0).shape[0] != live_nodes:
                print((np.sum(state_mask,axis=0) == 0).shape)
                print(live_mask.shape)
                raise Exception("Switch axies dummy")
            if (np.sum(state_mask,axis=0) == 0).any():
                print((np.sum(state_mask,axis=0) == 0).shape)
                print(node_state[live_mask][(np.sum(state_mask,axis=0) == 0)])
                raise Exception("State mask didn't work")

            if (oracle_state_odds_l[:,live_mask] == 0).any():
                raise Exception("Zero oracle state odds")
            if (oracle_state_odds_r[:,live_mask] == 0).any():
                raise Exception("Zero oracle state odds")

        #
        # print("problem_node_state")
        # print(np.tile(np.arange(direct_state_odds_l.shape[0]),(direct_state_odds_l.shape[1],1))[np.sum(state_mask,axis=0) == 0])
        # print(node_state[np.sum(state_mask,axis=0) == 0])

        direct_state_odds_l = direct_state_odds_l[:,live_mask][state_mask]
        direct_state_odds_r = direct_state_odds_r[:,live_mask][state_mask]

        # print(direct_state_odds_l.shape)
        # print(direct_state_odds_r.shape)
        # print(state_mask.shape)
        # print(state_mask.shape)
        # print(oracle_state_odds_l.shape)
        # print(oracle_state_odds_r.shape)

        oracle_state_odds_l = oracle_state_odds_l[:,live_mask][state_mask]
        oracle_state_odds_r = oracle_state_odds_r[:,live_mask][state_mask]

        oracle_probability_l = np.zeros(nodes)
        oracle_probability_r = np.zeros(nodes)

        oracle_probability_l[live_mask] = (oracle_state_odds_l / (oracle_state_odds_l + direct_state_odds_l))
        oracle_probability_r[live_mask] = (oracle_state_odds_r / (oracle_state_odds_r + direct_state_odds_r))

        if self.inf_check:
            if np.isnan(oracle_probability_l).any():
                raise Exception("Nan Oracle probability")
            if np.isnan(oracle_probability_r).any():
                raise Exception("Nan Oracle probability")
            if np.isinf(oracle_probability_l).any():
                raise Exception("Inf Oracle probability")
            if np.isinf(oracle_probability_r).any():
                raise Exception("Inf Oracle probability")

        # print("Oracle Indicator Debug")
        # print(list(oracle_probability_l))
        # print(list(oracle_probability_r))

        # if np.sum(np.isnan(oracle_probability_l[self.live_mask] + np.isnan(oracle_probability_r[self.live_mask]))) > 0:
        #     raise Exception("NaN Oracle Probability")

        return oracle_probability_l,oracle_probability_r

    def sample_oracle_indicator(live_mask,oracle_probability_l,oracle_probability_r):

        oracle_indicator_l = np.random.rand(oracle_probability_l.shape[0]) < oracle_probability_l
        oracle_indicator_r = np.random.rand(oracle_probability_r.shape[0]) < oracle_probability_r

        return oracle_indicator_l,oracle_indicator_r

    def generate_new_states(self,states,new_state_mask):

        print("Labeling new states")
        print(f"Label:{states+1}")
        # total_new_states = np.sum(new_state_mask)
        # return np.arange(states,states+total_new_states)

        return states + 1


    def state_node_odds(self,state):

        state_mask = self.state_masks[state]

        return self.state_log_odds[state,state_mask]

    def pad_root_transitions(self,raw_counts):
        for node in self.nodes:
            if node.parent is None:
                raw_counts[self.node_states[node.index],0] += 1
        return raw_counts

    def most_likely_parent_to_child(self):

        raw_counts = self.raw_transition_counts()

        raw_counts = self.pad_root_transitions(raw_counts)

        most_likely_transition_matrix = np.zeros((self.hidden_states,self.hidden_states))

        for hidden_state,transitions in enumerate(raw_counts[1:]):
            print(hidden_state)
            print(transitions)
            most_likely_parent = np.argmax(transitions)
            print(most_likely_parent)
            most_likely_transition_matrix[hidden_state+1,most_likely_parent] += 1

        return most_likely_transition_matrix.T

    def raw_transition_counts(self):

        print("Recomputing Transition Counts")

        self.establish_parameters()

        states = self.hidden_states

        transition_mask = self.live_mask

        new_transition_counts = np.zeros((states+1,states+1))

        # transition_mask = live_mask

        for ps,csl in zip(self.node_states[transition_mask],self.child_state_l[transition_mask]):
            # print(csl)
            # print(ps)
            new_transition_counts[csl,ps] += 1

        for ps,csr in zip(self.node_states[transition_mask],self.child_state_r[transition_mask]):
            # print(csr)
            # print(ps)
            new_transition_counts[csr,ps] += 1

        # print(new_transition_counts)

        return new_transition_counts

    def backup(self,location):
        self.pool = None
        with open(location,mode='bw') as f:
            pickle.dump(self,f)

    def reconstitute(location):
        with open(location,mode='br') as f:
            braider = pickle.load(f)
            braider.pool = mp.Pool()
            return braider

    def hidden_state_to_nodes(self,hidden_state):
        node_indices = np.arange(self.total_nodes)[self.state_masks[hidden_state]]
        return [self.nodes[ni] for ni in node_indices]

    def state_lr_samples(self,hidden_state):
        finite = self.lr_finite(hidden_state)
        left = np.arange(self.total_samples)[finite < .5]
        right = np.arange(self.total_samples)[finite >= .5]
        return left,right

    def state_lr_gradient(self,hidden_state):
        return self.state_sample_log_odds[hidden_state]

class Component():

    def __init__(self,node_mask,node_summary):

        self.node_summary = node_summary
        self.mask = node_mask
        self.nodes = np.sum(self.mask)

    def estimate_async(task):
        component,prior_means,prior_mean_precision,prior_covariance,prior_precision,prior_power = task
        component.estimate_paramters(prior_means,prior_mean_precision,prior_covariance,prior_precision,prior_power)
        return component

    def initialize(self):

        prior_means = np.zeros(self.node_summary['features'])
        prior_mean_precision = np.identity(self.node_summary['features'])
        prior_covariance = np.identity(self.node_summary['features'])
        prior_precision = np.identity(self.node_summary['features'])
        prior_power = 1

        self.estimate_paramters(prior_means,prior_mean_precision,prior_covariance,prior_precision,prior_power)

    def estimate_paramters(self,prior_means,prior_mean_precision,prior_covariance,prior_precision,prior_power):

        estimated_means,mean_estimate_power = IHMM.estimate_node_means(self.mask,self.node_summary)

        estimated_covariance,estimated_precision = IHMM.estimate_node_covariance_precision(self.mask,self.node_summary,feature_means=estimated_means)

        posterior_precision = ((estimated_precision * self.nodes) + (prior_precision * prior_power)) / (self.nodes + prior_power)

        posterior_covariance = np.linalg.inv(posterior_precision)

        # print("Parameter debug")
        # print(estimated_precision)
        # print(posterior_precision)
        # print(estimated_covariance)
        # print(posterior_covariance)

        posterior_mean_numerator = self.nodes * np.dot(estimated_means,posterior_precision) + np.dot(prior_means,prior_mean_precision)
        posterior_mean_inverse_denominator = np.linalg.inv((self.nodes * posterior_precision) + prior_mean_precision)

        posterior_means = np.dot(posterior_mean_numerator,posterior_mean_inverse_denominator)

        covariance_log_determinant = np.linalg.slogdet(posterior_covariance)[0] * np.log2(np.e)

        self.means = posterior_means
        self.precision = posterior_precision
        self.covariance = posterior_covariance
        self.covariance_log_determinant = covariance_log_determinant

    def node_log_likelihood_async(self,task):

        features,node_feature_mask = task
        features = features[node_feature_mask]

        # print("loglikedebug")
        # print(features.shape)
        # print(node_feature_mask.shape)

        centered = features - self.means[node_feature_mask]
        masked_precision = self.precision[node_feature_mask].T[node_feature_mask].T
        masked_covariance = self.covariance[node_feature_mask].T[node_feature_mask].T

        masked_determinant = np.linalg.slogdet(masked_covariance)[0]

        precision_fit = np.dot(np.dot(centered,masked_precision),centered)
        log_likelihood = masked_determinant + precision_fit + np.log2(2*np.pi)

        return log_likelihood


if __name__ == '__main__':
    main()




##
