### Infinite Hidden Markov Model for clustering together tree nodes generated by Random Forests

import numpy as np
import random
from functools import reduce
from scipy.misc import comb as nCk
from tree_reader import Node as TreeReaderNode

from itertools import repeat

from scipy.special import logit,expit
from scipy.special import gamma as gamma_f

import multiprocessing as mp

import matplotlib.pyplot as plt



## This model contains several interdependent variables:

##  - Hidden state sequence for nodes
##  - Transition probablities between hidden states
##  - Probability of any given sample emitting 0 or 1 when in a given state
##  - Hyperparameters governing the generative mechanism of the states

## These variables are sampled sequentially

class IHMM():
    def __init__(self,forest,alpha=1,beta=1,gamma=1,alpha_e=.5,beta_e=.5,start_states=20,inf_check=False,p=None,hierarchal=True):

        self.pool = mp.Pool(p)

        self.inf_check = inf_check
        self.hierarchal = hierarchal

        self.forest = forest

        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma

        self.alpha_e = alpha_e
        self.beta_e = beta_e

        self.hidden_states = start_states

        self.nodes = forest.nodes()
        self.live_nodes = forest.roots() + forest.stems()
        self.live_mask = np.zeros(len(self.nodes),dtype=bool)

        for node in self.live_nodes:
            self.live_mask[node.index] = True

        self.total_features = len(forest.output_features)
        self.feature_dictionary = forest.truth_dictionary.feature_dictionary
        self.total_nodes = len(self.nodes)

        self.node_states = np.zeros(self.total_nodes,dtype=int)

        self.child_index_l = np.zeros(self.total_nodes,dtype=int)
        self.child_index_r = np.zeros(self.total_nodes,dtype=int)
        self.child_state_l = np.zeros(self.total_nodes,dtype=int)
        self.child_state_r = np.zeros(self.total_nodes,dtype=int)

        for node in self.live_nodes:
            self.child_index_l[node.index] = node.children[0].index
            self.child_index_r[node.index] = node.children[1].index

        self.node_samples = forest.node_sample_encoding(self.nodes)

        self.node_features = np.zeros((self.total_nodes,self.total_features))
        self.node_feature_mask = np.zeros((self.total_nodes,self.total_features),dtype=bool)

        for node in self.live_nodes:
            node_features = node.features
            local_gains = node.medians
            for feature,gain in zip(node_features,local_gains):
                fi = self.feature_dictionary[feature]
                self.node_features[node.index,fi] = gain
                self.node_feature_mask[node.index,fi] = True

        for node in self.live_nodes:
            self.node_states[node.index] = random.randint(1,self.hidden_states-1)

        self.oracle_indicator_l = np.random.rand(self.total_nodes) < .5
        # self.oracle_indicator_l = np.random.rand(self.total_nodes) < 1/self.hidden_states
        # self.oracle_indicator_l = np.ones(self.total_nodes,dtype=bool)
        self.oracle_indicator_l[np.logical_not(self.live_mask)] = False

        self.oracle_indicator_r = np.random.rand(self.total_nodes) < .5
        # self.oracle_indicator_r = np.random.rand(self.total_nodes) < 1/self.hidden_states
        # self.oracle_indicator_r = np.ones(self.total_nodes,dtype=bool)
        self.oracle_indicator_r[np.logical_not(self.live_mask)] = False

        self.state_masks = np.zeros((self.hidden_states,self.total_nodes),dtype=bool)

        self.state_means = np.zeros((self.hidden_states,self.total_features))

        self.state_covariances = np.zeros((self.hidden_states,self.total_features,self.total_features))
        self.state_covariances[:,np.identity(self.total_features,dtype=bool)] = 1

        self.state_precisions = np.zeros((self.hidden_states,self.total_features,self.total_features))
        self.state_precisions[:,np.identity(self.total_features,dtype=bool)] = 1

        self.state_log_dets = np.zeros(self.total_features)

        self.compute_data_properties()

    ## Here we begin the methods for updating the state of the model over many "sweeps"

    ## Things that stay constant:
    ## Node feature gains/Features present in nodes
    ## Number of samples
    ## Number of nodes
    ## State of nodes that aren't live or marked as null state
    ##    - These nodes don't have a gain of information (probably because they're leaves), so they can't be modeled
    ## Node relations (eg child indices)
    ##    -  Topology of the forest remains constant, we're only trying to learn it

    ## Things that update:
    ## Node states for live nodes
    ## Number of states
    ## Emission model for each state
    ## Number of transitions between states
    ## State of children
    ## Hyperparameters

    ## Dependence flow:

    ## Node state gives gives=>
    ##  - Number of hidden states
    ##  - State masks

    ## - Number of states + masks + oracle transition indicator =>
    ##  - Transitions
    ##  - Oracle transitions
    ##  - State gain expectation and covariance

    ## - Transitions + number of states =>
    ##  - Transition log odds

    ## - Oracle indicator + Transitions =>
    ##  - MAP Hyperparameters

    ###### HERE WE BEGIN TO RECOMPUTE THE NODE STATE ####

    ## - Transition log odds + state emission model (Wishart) =>
    ##  - Log odds of direct state transitions per node
    ##  - Log odds of oracle per node

    ####### HERE WE DO THE FIRST SAMPLING STEP ##########

    ## - Oracle indicator + oracle log odds =>
    ##  - Log odds given oracle
    ##  - Log odds of secondary oracle

    ################ SECOND SAMPLING STEP ###############

    ##  - New states discovered
    ##  - Loop back to recomputing transitions & hypers

    def establish_parameters(self):

        ## This method establishes descriptive parameters given some sequence of hidden states over the node sequences.

        print(self.hidden_states)
        print(self.node_states)
        print(list(np.sum(self.state_masks,axis=1)))

        ### Here we re-assign hidden states to cleaned up indecies, in order to eliminate hidden states that no longer exist

        self.node_states = self.clean_state_indeces(self.node_states)

        ## Here we update the states assigned to the children of each node, this is setup to count the transition frequencies

        self.update_node_relations()

        ## State masks are a more conveninet way of storing which nodes belong to which states. They are numpy boolean mask arrays

        self.state_masks = self.recompute_state_masks(self.node_states)

        ## Here we compute the state log odds of emission for each sample for each state given the node assignments above

        self.recompute_component_mean_prior()
        self.recompute_component_precision_prior()
        self.recompute_state_features()

        ## We would like to pad this matrix with the log odds of the oracle state. For it, we will use the log odds of all samples without partitioning

        self.state_means = np.concatenate((self.state_means,np.array([self.background_mean_priors])),axis=0)
        self.state_precisions = np.concatenate((self.state_precisions,np.array([self.background_precision_prior])),axis=0)
        self.state_covariances = np.concatenate((self.state_covariances,np.array([self.background_covariance_prior])),axis=0)
        self.state_log_dets = np.concatenate((self.state_log_dets,np.array([self.background_log_det])))

        ## And here we count the transition counts

        self.transition_counts = self.recompute_transition_counts(self.live_mask,self.oracle_indicator_l,self.oracle_indicator_r,self.hidden_states,self.node_states,self.child_state_l,self.child_state_r)

        ## Here we count which transitions occurred through an oracle in order to recalculate the oracle transition frequencies

        self.oracle_transition_matrix = self.recompute_oracle_transition_matrix(self.hidden_states,self.oracle_indicator_l,self.oracle_indicator_r,self.node_states,self.child_state_l,self.child_state_r)
        self.oracle_transition_counts = self.recompute_oracle_transition_counts(self.oracle_transition_matrix)

        ## And finally here, on the basis of how often the oracle was used and the number of observed states, we sample the hyperparameters

        self.sample_hypers()


    def update_states(self):

        if not self.hierarchal:
            self.transition_counts = np.ones(self.transition_counts.shape) * ((np.sum(oracle_transition_counts) / self.hidden_states) + 1)

        self.establish_parameters()

        if not self.hierarchal:
            self.transition_counts = np.ones(self.transition_counts.shape) * ((np.sum(oracle_transition_counts) / self.hidden_states) + 1)


        ### The above methods establish a description of the current state.

        ### The methods below establish the log odds of each given state for each given node, and whether an oracle was used to reach it

        ## First we establish the log odds of a given state based on the divergence observed

        self.state_log_odds_given_features = self.compute_state_log_odds_given_features(self.node_features,self.node_feature_mask,self.live_mask,self.state_means,self.state_precisions,self.state_log_dets)

        # self.state_log_odds_given_divergence /= self.hidden_states

        ## This matrix contains only odds of existing states based on divergence, eg it is hidden_states x nodes in dimension, plus an extra row for a novel state

        if self.inf_check:
            print("Feature odds")
            print(list(self.state_log_odds_given_features))
            if np.isnan(self.state_log_odds_given_features).any():
                raise Exception("NaN state log odds given features")
            if np.isinf(self.state_log_odds_given_features).any():
                raise Exception("Inf state log odds given features")

        #### IMPORTANT ####

        ## This next section works with raw odds because there is a need to add together the odds of a state given oracle and given no oracle_mask

        #### DO NOT MIX ODDS AND LOG ODDS, BAD ####

        ## Next we establish the odds of a given state or the oracle given its children

        self.direct_state_odds_given_child_l = self.compute_state_odds_given_child_direct_transition(self.beta,self.transition_counts,self.node_states,self.child_state_l)
        self.direct_state_odds_given_child_r = self.compute_state_odds_given_child_direct_transition(self.beta,self.transition_counts,self.node_states,self.child_state_r)

        ## We extract the odds of visiting the oracle for each given node and tile them

        self.oracle_odds_given_child_l = np.tile(self.direct_state_odds_given_child_l[-1],(self.hidden_states + 1,1))
        self.oracle_odds_given_child_r = np.tile(self.direct_state_odds_given_child_r[-1],(self.hidden_states + 1,1))

        if self.inf_check:
            print(list(self.direct_state_odds_given_child_l))
            print(list(self.direct_state_odds_given_child_r))
            if np.isnan(self.oracle_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("NaN oracle odds")
            if np.isnan(self.oracle_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("NaN oracle odds")

            if np.isinf(self.oracle_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("Inf oracle odds")
            if np.isinf(self.oracle_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("Inf oracle odds")


            if (np.sum(self.direct_state_odds_given_child_l,axis=0) == 0).any():
                raise Exception("Total zero transition odds")
            if (np.sum(self.direct_state_odds_given_child_r,axis=0) == 0).any():
                raise Exception("Total zero transition odds")



        # print("ORACLE_ODDS_DEBUG")
        # print(self.oracle_odds_given_child_l.shape)
        # print(self.oracle_odds_given_child_r.shape)

        ### THIS IS IMPORTANT ###
        ## We NOW set the bottom rows to zero, so that it will be filled in with the odds of a novel state momentarily
        self.direct_state_odds_given_child_l[-1] = 0
        self.direct_state_odds_given_child_r[-1] = 0

        if self.inf_check:
            if np.isnan(self.direct_state_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("NaN direct state odds")
            if np.isnan(self.direct_state_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("NaN direct state odds")
            if np.isinf(self.direct_state_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("Inf direct state odds")
            if np.isinf(self.direct_state_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("Inf direct state odds")

        ## Next we need to establish the odds of a given state given that an oracle was visited
        ## These are tiled across every node because they are uniform

        self.oracle_odds = self.compute_state_odds_given_oracle(self.gamma,self.total_nodes,self.oracle_transition_counts)

        if self.inf_check:
            print(list(self.oracle_odds))
            if np.isnan(self.oracle_odds[:,self.live_mask]).any():
                raise Exception("NaN oracle odds")
            if np.isinf(self.oracle_odds[:,self.live_mask]).any():
                raise Exception("Inf oracle odds")


        ## Next we get the odds of obtaining each state by the oracle route through multiplication:

        self.state_odds_given_oracle_l = self.oracle_odds_given_child_l * self.oracle_odds
        self.state_odds_given_oracle_r = self.oracle_odds_given_child_r * self.oracle_odds

        if self.inf_check:
            if np.isnan(self.state_odds_given_oracle_l[:,self.live_mask]).any():
                mask = np.logical_and(np.sum(np.isnan(self.state_odds_given_oracle_l),axis=0) > 0,self.live_mask)
                print("Total Odds")
                print(list(self.state_odds_given_oracle_l[:,mask]))
                print("Direct Odds")
                print(list(self.direct_state_odds_given_child_l[:,mask]))
                print(list(self.direct_state_odds_given_child_r[:,mask]))
                print("Odds of Oracle")
                print(list(self.oracle_odds_given_child_l[:,mask]))
                print(list(self.oracle_odds_given_child_r[:,mask]))
                print("Oracle odds")
                print(list(self.oracle_odds[:,mask]))
                raise Exception("NaN state odds given oracle")
            if np.isinf(self.state_odds_given_oracle_l[:,self.live_mask]).any():
                raise Exception("Inf state odds given oracle")
            if np.isnan(self.state_odds_given_oracle_r[:,self.live_mask]).any():
                raise Exception("NaN state odds given oracle")
            if np.isinf(self.state_odds_given_oracle_r[:,self.live_mask]).any():
                raise Exception("Inf state odds given oracle")



        # print("ORACLE_ODDS_DEBUG2")
        # print(self.state_odds_given_oracle_l.shape)
        # print(self.state_odds_given_oracle_r.shape)

        ## Finally we combine the odds and transform them into log form
        ## NOTE the final row is now the odds of a novel state, because it is the odds of visiting the oracle multiplied by the odds of obtaining a new state from the oracle

        self.state_odds_given_child_l = self.direct_state_odds_given_child_l + self.state_odds_given_oracle_l
        self.state_odds_given_child_r = self.direct_state_odds_given_child_r + self.state_odds_given_oracle_r

        if self.inf_check:
            if np.isnan(self.state_odds_given_child_l[:,self.live_mask]).any():
                raise Exception("NaN child state odds")
            if np.isnan(self.state_odds_given_child_r[:,self.live_mask]).any():
                raise Exception("NaN child state odds")

            if (self.state_odds_given_child_l < 0).any():
                raise Exception("Negative odds!")
            if (self.state_odds_given_child_r < 0).any():
                raise Exception("Negative odds!")

        ## And here we transform the plain odds into log odds:

        self.state_log_odds_given_child_l = np.log2(self.state_odds_given_child_l)
        self.state_log_odds_given_child_r = np.log2(self.state_odds_given_child_r)

        if self.inf_check:
            if np.isnan(self.state_log_odds_given_child_l[1:,self.live_mask]).any():
                raise Exception("NaN child state log odds")
            if np.isnan(self.state_log_odds_given_child_r[1:,self.live_mask]).any():
                raise Exception("NaN child state log odds")
            if np.isinf(self.state_log_odds_given_child_l[1:,self.live_mask]).any():
                raise Exception("Inf child state log odds")
            if np.isinf(self.state_log_odds_given_child_r[1:,self.live_mask]).any():
                raise Exception("Inf child state log odds")


        #THIS IS A TEMPORARY HACK FOR AN UNSTRUCTURED DP MIXTURE MODEL:
        # self.state_log_odds_given_child_l = np.log2(self.oracle_odds)
        # self.state_log_odds_given_child_r = np.log2(self.oracle_odds)
        #TEMP HACK ENDS HERE

        ## Finally we want to combine all log odds and sample the resulting distribution

        self.state_log_odds = self.state_log_odds_given_features + self.state_log_odds_given_child_l + self.state_log_odds_given_child_r

        new_node_states = self.node_states.copy()
        new_state_indicator = np.zeros(self.total_nodes,dtype=bool)

        new_node_states[self.live_mask],new_state_indicator[self.live_mask] = self.sample_states(self.live_mask,self.state_log_odds)

        oracle_probability_l,oracle_probability_r = self.oracle_indicator_probabilities(new_node_states,self.live_mask,self.direct_state_odds_given_child_l,self.direct_state_odds_given_child_r,self.state_odds_given_oracle_l,self.state_odds_given_oracle_r)

        new_oracle_indicator_l,new_oracle_indicator_r = self.sample_oracle_indicator(oracle_probability_l,oracle_probability_r)

        if self.inf_check:
            if np.isnan(self.state_log_odds[1:,self.live_mask]).any():
                raise Exception("NaN additive log state odds")
            if np.isinf(self.state_log_odds[1:,self.live_mask]).any():
                raise Exception("Inf additive log state odds")

        new_oracle_indicator_l[new_state_indicator] = True
        new_oracle_indicator_r[new_state_indicator] = True

        total_new_states = np.sum(new_state_indicator)

        # print("New states created:")
        # print(total_new_states)
        # new_node_states[new_state_indicator] = np.arange(self.hidden_states, self.hidden_states + total_new_states)

        print("New state created")
        print(total_new_states)


        return new_node_states,new_oracle_indicator_l,new_oracle_indicator_r

    def sweep(self):

        self.establish_parameters()

        new_node_states,new_oracle_indicator_l,new_oracle_indicator_r = self.update_states()

        self.node_states = new_node_states
        self.oracle_indicator_l = new_oracle_indicator_l
        self.oracle_indicator_r = new_oracle_indicator_r


    def subsample_sweep(self,fraction=.1):

        self.establish_parameters()

        global_live_mask = self.live_mask.copy()

        self.live_mask[self.live_mask] = np.random.random(np.sum(self.live_mask)) < fraction

        new_node_states,new_oracle_indicator_l,new_oracle_indicator_r = self.update_states()

        self.node_states[self.live_mask] = new_node_states[self.live_mask]
        self.oracle_indicator_l[self.live_mask] = new_oracle_indicator_l[self.live_mask]
        self.oracle_indicator_r[self.live_mask] = new_oracle_indicator_r[self.live_mask]

        self.live_mask = global_live_mask


    def max_likelihood_sweep(self):

        ## This sweep assigns nodes to their most likely state, not a state sampled at random.

        self.sweep()
        new_node_states = np.zeros(self.node_states.shape[0])
        new_node_states[self.live_mask] = np.argmax(self.state_log_odds[:,self.live_mask],axis=0)
        self.node_states = new_node_states
        self.establish_parameters()


    def sample_hypers(self):


        print("Sampling Hypers")
        # print(f"Beta:{self.beta}")
        # print(f"Gamma:{self.gamma}")

        self.beta = self.recompute_beta()
        self.gamma = self.recompute_gamma()
        self.beta_e = self.recompute_beta_e()

        print(f"Beta:{self.beta}")
        print(f"Gamma:{self.gamma}")


    def recompute_beta(self):

        ## Here we compute the maximum a-posteriori probability for the beta parameter
        ## Due to falling factorials in the formula, we will want to use a stirling approximation,
        ## For this it is convenient to pre-compute a series of logs of integers

        ## The likelihood of beta is given by: beta ^ (k-1) / (sum of all non-oracle transitions)

        k = self.hidden_states

        transition_matrix = self.transition_counts + self.oracle_transition_matrix

        sums = np.sum(transition_matrix,axis=1)

        # max_sum = int(np.sum(sums) * 2)
        max_sum = int(np.max(sums) * 2)

        log_sequence = np.log2(np.arange(1,max_sum*2))

        likelihood_sequence = np.zeros(max_sum)[1:]

        for i,transitions in enumerate(transition_matrix):
            transitions = self.transition_counts[i]
            # total = np.sum(transitions)
            total = int(sums[i])
            non_zero = int(np.sum(transitions > 0))
            l2ls = lambda b: (non_zero * np.log2(b)) - np.sum(log_sequence[b:total+b])
            likelihood_sequence += np.array([l2ls(b) for b in range(1,max_sum)])

        likelihood_sequence -= log_sequence[:len(likelihood_sequence)]

        beta = np.argmax(likelihood_sequence) + 1

        return beta

    def recompute_gamma(self,max_gamma=None):

        k = self.hidden_states - 1

        oracle_total = int(np.sum(self.oracle_transition_counts))

        if max_gamma is None:

            max_gamma = oracle_total + 1

        log_sequence = np.log2(np.arange(1,max_gamma*2))

        # l2l = lambda g: ((k * np.log2(g)) - (np.sum(log_sequence[g+1:oracle_total+g+1]) + (g * 1.44)))
        l2l = lambda g: ((k * np.log2(g)) - (np.sum(log_sequence[g+1:oracle_total+g+1]) + np.log2(g)))

        likelihood_sequence = np.array([l2l(g) for g in range(1,max_gamma)])

        gamma = np.argmax(likelihood_sequence)+1

        # print("GAMMA DEBUG")
        # print(len(likelihood_sequence))
        # print(gamma)
        # print(max_gamma)
        # print(oracle_total)
        # print(likelihood_sequence[:50])
        # print("Alleged maximum likelihood")
        # print(likelihood_sequence[gamma-1])

        if (gamma + 1 == max_gamma) and max_gamma < self.total_nodes * 100:
            gamma = self.recompute_gamma(max_gamma = max_gamma * 5)

        return gamma

    def recompute_beta_e(self,beta_max=None):

        k = self.hidden_states
        features = self.total_features
        if beta_max is None:
            beta_max = max(k,features)

        trace_product = self.state_precisions[:k,np.identity(features,dtype=bool)] * np.tile(self.background_precision_prior[np.identity(features,dtype=bool)],(k,1))

        trace_sum = np.sum(trace_product + np.ones((k,features)))

        log_trace_sum = np.sum(np.log(trace_product + np.ones((k,features))))

        log_cum_sum = np.cumsum(np.log(np.arange(1,beta_max)))

        def likelihood(beta): return (-k * log_cum_sum[int(beta/2)]) + (-1/(2 * beta)) + (((k*beta - 3)/2) * (beta / 2)) + ((beta / 2) * (log_trace_sum - trace_sum))

        sequence = np.array([likelihood(b) for b in range(1,beta_max)])

        beta_e = np.argmin(sequence) + 1
        other_beta = np.argmax(sequence) + 1

        print("BETA_E DEBUG")
        print("beta_e")
        print(beta_e)
        print("Trace sum")
        print(trace_sum)
        print("Log trace sum")
        print(log_trace_sum)
        print("Other")
        print(other_beta)
        print(sequence[:10])

        return beta_e

        # return self.total_features
        # return (self.total_features * 2) - 1

    def compute_data_properties(self):

        print("Computing Data Properties")

        nodes = self.total_nodes
        features = self.total_features
        live_mask = self.live_mask
        node_feature_mask = self.node_feature_mask
        node_features = self.node_features
        hidden_states = self.hidden_states


        ## We need to compute the mean gain for each feature using only the values from nodes that actually contain that feautre:

        feature_sums = np.zeros(features) + np.sum(node_features[live_mask],axis=0)
        state_feature_node_totals = np.ones(features) + np.sum(node_feature_mask[live_mask],axis=0)

        state_feature_means = feature_sums / state_feature_node_totals

        ## Note, this is a minimum-mean-square-error estimate of the mean: https://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1277&context=facpub

        ## Now we will need a centered feature gain matrix

        centered_state_features = np.zeros((np.sum(live_mask),features))
        centered_state_features[node_feature_mask[live_mask]] = node_features[live_mask][node_feature_mask[live_mask]]
        centered_state_features[node_feature_mask[live_mask]] -= np.tile(state_feature_means,(np.sum(live_mask),1))[node_feature_mask[live_mask]]

        print(centered_state_features.shape)

        raw_cross_product = np.dot(centered_state_features.T,centered_state_features)

        scaling_value = np.dot(node_feature_mask[live_mask].T.astype(dtype=float),node_feature_mask[live_mask].astype(dtype=float))

        # raw_cross_product += np.identity(features)
        #
        # scaling_value += np.identity(features)

        covariance_estimate = raw_cross_product / (scaling_value + 1)

        covariance_estimate += np.identity(features)

        if np.any(np.linalg.matrix_rank(covariance_estimate) < self.total_features):
            print("WARNING:")
            print("Feature covariance matrix shape:")
            print(covariance_estimate.shape)
            print("Rank:")
            print(np.linalg.matrix_rank(covariance_estimate))
            raise Exception("Rank-deficient covariance matrix, you are probably using a forest that is too small. Please generate more trees")

        # if self.inf_check:
        #     print("Unrepresented features?")
        #     if np.any(np.sum(node_feature_mask,axis=0) == 0):
        #         raise Exception("Unrepresented features, covariance can't be computed")

        if np.any(np.diagonal(covariance_estimate) == 0):
            print("WARNING")
            print("Zero self-covariance, probably an unrepresented feature")
            print("Setting self-covariance (eg variance) to 1, and all other covariances to zero")
            mask = np.diagonal(covariance_estimate) == 0
            print("Affected features:")
            print(np.arange(self.total_features)[mask])
            covariance_estimate[mask] = 0
            covariance_estimate[:,mask] = 0
            covariance_estimate[mask,mask] = 1

        if self.inf_check:
            if np.any(np.linalg.eigvals(covariance_estimate) < 0):
                print("Cov estimate:")
                print(covariance_estimate.shape)
                print("Rank:")
                print(np.linalg.matrix_rank(covariance_estimate))
                raise Exception("Negative eigenvalues in covariance")


        precision_estimate = np.linalg.inv(covariance_estimate)

        ## Let's test whether or not the estimate is a positive definite matrix:

        if self.inf_check:
            if not np.all(np.linalg.eigvals(precision_estimate) >= 0):
                raise Exception("Negative eigenvalues in precision")

        self.data_means = state_feature_means
        self.data_covariance = covariance_estimate
        self.data_precision = precision_estimate

    def recompute_component_mean_prior(self):

        nodes = self.total_nodes
        features = self.total_features
        live_mask = self.live_mask
        node_feature_mask = self.node_feature_mask
        node_features = self.node_features
        hidden_states = self.hidden_states

        background_mean_covariance_priors = (np.sum(self.state_covariances,axis=0) + self.data_covariance) / (hidden_states + 1)

        background_mean_precision_priors = np.linalg.inv(background_mean_covariance_priors) * (hidden_states + 1)

        background_mean_prior_numerator = np.dot(self.data_means, self.data_precision)

        background_mean_prior_numerator += np.dot(np.sum(self.state_means,axis=0),background_mean_precision_priors)

        background_mean_inverse_denomenator = np.linalg.inv(self.data_precision + (hidden_states * background_mean_precision_priors))

        background_mean_priors = np.dot(background_mean_prior_numerator,background_mean_inverse_denomenator)

        self.background_mean_priors = background_mean_priors
        self.background_mean_covariance_priors = background_mean_covariance_priors
        self.background_mean_precision_priors = background_mean_precision_priors

        self.precomputed_mean_dot = np.dot(background_mean_priors,background_mean_precision_priors)

        # Now we can find a posterior distribution for the state mean and covariance:

    def recompute_component_precision_prior(self):

        features = self.total_features

        sum_precision = (np.identity(features) + self.data_precision + (self.beta_e * np.sum(self.state_precisions[1:-1],axis=0)))

        mean_precision = sum_precision / ((self.hidden_states * self.beta_e) + 1)

        # prior_covariance = np.linalg.inv(mean_precision) * ((self.hidden_states * self.beta_e) + 1)
        prior_covariance = np.linalg.inv(mean_precision)

        prior_determinant = np.log2(np.linalg.slogdet(prior_covariance)[1]) / np.log2(np.e)

        self.background_covariance_prior = prior_covariance
        self.background_precision_prior = mean_precision
        # self.background_precision_prior = mean_precision / ((self.hidden_states * self.beta_e) + 1)
        self.background_log_det = prior_determinant

    def update_node_relations(self):

        self.child_state_l[self.live_mask] = self.node_states[self.child_index_l][self.live_mask]
        self.child_state_r[self.live_mask] = self.node_states[self.child_index_r][self.live_mask]

    def clean_state_indeces(self,node_states):

        print("State Index Cleanup")
        new_states = sorted(list(set(node_states)))
        print(new_states)
        new_indices = {old_index:i for i,old_index in enumerate(new_states)}
        print(new_indices)

        new_state_sequence = np.zeros(node_states.shape,dtype=int)

        for i,old_state in enumerate(node_states):
            new_state_sequence[i] = new_indices[old_state]

        self.hidden_states = len(new_states)

        # print(self.hidden_states)

        new_state_means = np.zeros((self.hidden_states,self.total_features))

        for old_index,state in enumerate(self.state_means):
            if old_index in new_indices:
                new_state_means[new_indices[old_index]] = state

        self.state_means = new_state_means

        # print(new_state_sequence)

        return new_state_sequence

    def recompute_state_masks(self,node_states):

        print("Computing State Masks")

        new_states = self.hidden_states

        new_state_masks = np.zeros((new_states,node_states.shape[0]),dtype=bool)

        for state in range(new_states):
            new_state_masks[state] = node_states == state

        # print(new_state_masks.shape)

        if np.sum(self.node_states[self.live_mask] == 0) > 0:
            raise Exception()

        return new_state_masks

    # def recompute_state_feature_gains(self,states,features,nodes,state_masks,node_feature_mask,gains):
    def recompute_state_features(self):

        print("Recomputing State Means and Covariance")

        states = self.hidden_states
        nodes = self.total_nodes
        features = self.total_features
        state_masks = self.state_masks
        node_feature_mask = self.node_feature_mask
        node_features = self.node_features

        prior_means = self.background_mean_priors
        prior_mean_precisions = self.background_mean_precision_priors
        prior_mean_covariances = self.background_mean_covariance_priors

        prior_covariance = self.background_covariance_prior
        prior_precision = self.background_precision_prior

        precomputed_background_dot_product = self.precomputed_mean_dot

        new_means = np.zeros((states,features))

        new_covariances = np.zeros((states,features,features))
        new_precisions = np.zeros((states,features,features))

        new_log_dets = np.zeros(states)

        for state,state_node_mask in list(enumerate(state_masks))[1:]:

            print(f"State:{state}")
            print(f"Total nodes:{np.sum(state_node_mask)}")

            state_occupancy = np.sum(state_node_mask)

            ## First we want:
            ##  -only nodes in state
            ##  -feature gains for each node
            ##      -Not every node has gains for every feature, so we will need to be efficient about computing their means

            covariance_estimate = np.zeros((features,features))

            state_feature_node_mask = np.zeros((features,nodes),dtype=bool)

            ## We need to compute the mean gain for each feature using only the values from nodes that actually contain that feautre:

            feature_sums = np.sum(node_features[state_node_mask],axis=0)
            state_feature_node_totals = np.ones(features) + np.sum(node_feature_mask[state_node_mask],axis=0)

            state_feature_means = feature_sums / state_feature_node_totals

            print(feature_sums[:10])
            print(state_feature_node_totals[:10])
            print(state_feature_means[:10])

            ## Note, this is a minimum-mean-square-error estimate of the mean: https://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1277&context=facpub

            ## Now we will need a centered feature gain matrix

            centered_state_features = np.zeros((state_occupancy,features))
            centered_state_features[node_feature_mask[state_node_mask]] = node_features[state_node_mask][node_feature_mask[state_node_mask]]
            centered_state_features[node_feature_mask[state_node_mask]] -= np.tile(state_feature_means,(state_occupancy,1))[node_feature_mask[state_node_mask]]

            raw_cross_product = np.dot(centered_state_features.T,centered_state_features)

            # scaling_value = np.dot(node_feature_mask[state_node_mask].T,node_feature_mask[state_node_mask])
            # covariance_estimate = raw_cross_product / (scaling_value + 1)
            # covariance_estimate = raw_cross_product / (np.sum(state_node_mask) + 1)
            # covariance_estimate = raw_cross_product / np.sum(state_node_mask)

            ## Let's test whether or not the estimate is a positive definite matrix:



            # posterior_covariance = prior_covariance + raw_cross_product + (((state_occupancy * self.beta_e)/ (state_occupancy + self.beta_e)) * np.sum(np.power(prior_means - state_feature_means,2)))

            posterior_covariance = (((prior_covariance * self.beta_e) + raw_cross_product) / (self.beta_e + state_occupancy)) + np.identity(features)

            covariance_log_determinant = np.log2(np.linalg.slogdet(posterior_covariance)[1])/np.log2(np.e)


            # print("DETERMINANT DEBUG")
            # print(raw_cross_product)
            # print(self.beta_e)
            # print(state_occupancy)
            # print(np.any(np.isinf(posterior_covariance)))
            # print(covariance_log_determinant)

            posterior_precision = np.linalg.inv(posterior_covariance)

            # occupied_mean_dot_product = np.dot((state_feature_means * state_occupancy),posterior_precision)
            #
            # posterior_mean_numerator = occupied_mean_dot_product + precomputed_background_dot_product
            #
            # posterior_mean_inverse_denominator = np.linalg.inv(occupied_mean_dot_product + prior_mean_precisions)

            # posterior_means = np.dot(posterior_mean_numerator, posterior_mean_inverse_denominator)

            # new_means[state] = posterior_means

            new_means[state] = ((state_feature_means * state_occupancy) + (prior_means * self.beta_e)) / (self.beta_e + state_occupancy)

            new_precisions[state] = posterior_precision

            new_covariances[state] = posterior_covariance

            new_log_dets[state] = covariance_log_determinant

            if np.isinf(covariance_log_determinant):
                raise Exception("Infinite log determinant, singular covariance?")

            # if np.any(np.linalg.eigvals(covariance_estimate) < 0):
            #     raise Exception("Covariance estimate not positive-definite")

            # Now we can find a posterior distribution for the state mean and covariance:

        self.state_means = new_means
        self.state_precisions = new_precisions
        self.state_covariances = new_covariances
        self.state_log_dets = new_log_dets

            # print(len(feature_means))
            # print(self.total_features)

    def recompute_transition_counts(self,live_mask,oracle_indicator_l,oracle_indicator_r,states,node_states,child_state_l,child_state_r):

        print("Recomputing Transition Counts")

        new_transition_counts = np.zeros((states,states))

        transition_mask_l = np.logical_and(live_mask,np.logical_not(oracle_indicator_l))
        transition_mask_r = np.logical_and(live_mask,np.logical_not(oracle_indicator_r))

        # transition_mask = live_mask

        for ps,csl in zip(node_states[transition_mask_l],child_state_l[transition_mask_l]):
            # print(csl)
            # print(ps)
            new_transition_counts[csl,ps] += 1

        for ps,csr in zip(node_states[transition_mask_r],child_state_r[transition_mask_r]):
            # print(csr)
            # print(ps)
            new_transition_counts[csr,ps] += 1

        # print(new_transition_counts)

        return new_transition_counts

    def recompute_oracle_transition_matrix(self,states,oracle_indicator_l,oracle_indicator_r,node_states,child_state_l,child_state_r):

        print("Recomputing oracle transition count")

        new_oracle_transitions = np.zeros((states,states))

        for ps,csl in zip(node_states[oracle_indicator_l],child_state_l[oracle_indicator_l]):
            new_oracle_transitions[csl,ps] += 1

        for ps,csr in zip(node_states[oracle_indicator_r],child_state_r[oracle_indicator_r]):
            new_oracle_transitions[csr,ps] += 1

        new_oracle_transitions[0] = 0

        # print(new_oracle_transitions)

        return new_oracle_transitions


    def recompute_oracle_transition_counts(self,oracle_transition_matrix):

        print("Recomputing oracle transition count")

        new_oracle_transitions = np.ones(oracle_transition_matrix.shape[0])

        new_oracle_transitions += np.sum(oracle_transition_matrix,axis=0)

        new_oracle_transitions[0] = 0

        # print(new_oracle_transitions)

        return new_oracle_transitions

    def compute_state_log_odds_given_features(self,node_features,node_feature_mask,live_mask,state_feature_means,state_precisions,state_log_dets):

        print("Computing state log odds | features")

        ## Here we have a wrapper function that allows us to parallelize the node state odds computation
        ## An inner function allows us to compute multiple states at the same time in a process pool

        feature_log_odds = np.zeros((node_features.shape[0],state_log_dets.shape[0]))

        print("Node features")
        print(node_features[live_mask].shape)
        print(node_feature_mask[live_mask].shape)

        node_output = self.pool.map(IHMM.node_state_log_odds_given_features,zip(range(np.sum(live_mask)),node_features[live_mask],node_feature_mask[live_mask],repeat(state_feature_means),repeat(state_precisions),repeat(state_log_dets)))

        print("Node ratio debug")
        print(np.array(node_output).shape)
        print(feature_log_odds.shape)
        print(feature_log_odds[live_mask].shape)

        feature_log_odds[live_mask] = np.array(node_output)

        # if np.isnan(node_output).any():
        #     raise Exception("Computed nan divergence log odds")

        return feature_log_odds.T

    def node_state_log_odds_given_features(task):

        node_index,node_features,node_feature_mask,state_feature_means,state_feature_precisions,state_log_dets = task

        total_node_features = np.sum(node_feature_mask)

        k = state_feature_means.shape[0]

        centered_features = np.tile(node_features[node_feature_mask],(k,1)) - state_feature_means[:,node_feature_mask]

        masked_precisions = state_feature_precisions[:,node_feature_mask].T[node_feature_mask].T

        state_log_likelihood = np.zeros(k)

        print("Node:")
        print(node_index)
        print(centered_features.shape)
        # print(centered_features)
        print(masked_precisions.shape)
        # print(masked_precisions)
        print(state_log_dets.shape)
        # print(state_determinants)

        for state,state_centered_features,state_precision_mtx,state_log_determinant in zip(range(k),centered_features,masked_precisions,state_log_dets):

            # print(state_centered_features.shape)
            # print(state_precision_mtx.shape)
            # print(np.dot(np.dot(state_centered_features,state_precision_mtx),state_centered_features).shape)
            # print(np.dot(np.dot(state_centered_features,state_precision_mtx),state_centered_features) + state_log_determinant + (k*np.log2(np.pi * 2)))

            state_log_likelihood[state] = np.dot(np.dot(state_centered_features,state_precision_mtx),state_centered_features) + state_log_determinant + (k*np.log2(np.pi * 2))

        print(state_log_likelihood)

        return state_log_likelihood

    def compute_state_odds_given_child_direct_transition(self,beta,transition_counts,node_states,node_child_states):

        ## Here we have to start considering the potential fact that an oracle may be sampled
        ## Since this is the first transition, for simplicity we will simply compute the odds of a direct transition and an oracle transition of any kind

        padded_transition_counts = np.zeros((transition_counts.shape[0],transition_counts.shape[1]+1))
        padded_transition_counts[:,:-1] += transition_counts
        padded_transition_counts[:,-1] += beta

        # print("Transition odds debug")
        # print(padded_transition_counts)

        total_transitions = np.sum(padded_transition_counts,axis=1)

        ## Here we  compute the odds of any other transition besides a given transition. It's important to normalize the transitions by an additional beta.
        ## This is important to prevent potential infinite odds being present when the ONLY transitionto a state is via the oracle.

        counter_odds = np.tile(total_transitions,(padded_transition_counts.shape[1],1)).T + beta
        counter_odds -= padded_transition_counts

        ## Finally we again have to account for the idea that when a node is being evaluated, we have to remove its influence from the transition counts
        ## To do this we will calculate the odds of any given state given a child, and then reduce by one the odds of the state that the node currently occupies

        ## This is important to allow an IHMM to converge from many states to few. When a large number of states are present, the oracle parameter is inferred
        ## to be small, which gives an outsize influence to the transitions that rare states anticipate based on their parent nodes.
        ## Given this, an IHMM can get trapped in a situation where many states containing only one node exist, forcing beta to remain small,
        ## but not allowing the nodes belonging to singleton states to explore other states.

        node_transition_odds = padded_transition_counts[node_child_states]

        # print("direct transition debug")
        # print(node_transition_odds.shape)
        # print(list(node_transition_odds))

        node_transition_odds[np.arange(node_transition_odds.shape[0]),node_states] -= 1

        node_transition_odds[node_transition_odds < 0] = 0
        # print(list(node_transition_odds))

        node_transition_counter_odds = counter_odds[node_child_states]

        # print(node_transition_counter_odds.shape)

        # transition_odds = padded_transition_counts.astype(dtype=float) / counter_odds.astype(dtype=float)

        # print(transition_odds)

        node_state_odds = node_transition_odds.astype(dtype=float) / node_transition_counter_odds.astype(dtype=float)

        node_state_odds[:,0] = 0

        return node_state_odds.T

    def log_sampling(sequence):
        sort = np.argsort(sequence)[::-1]
        sorted_log = sequence[sort]
        sorted_log -= sorted_log[0]
        raw_odds = np.exp2(sorted_log)

        draw = random.random()*np.sum(raw_odds)

        for i,w in zip(sort,raw_odds):
            draw -= w
            if draw <= 0 or (not np.isfinite(draw)):
                return i

        return len(raw_odds) - 1

    def sample_states(self,live_mask,state_log_odds):

        print("Sampling states")

        live_nodes = np.sum(live_mask)

        draw_index = self.pool.map(IHMM.log_sampling,state_log_odds[1:,live_mask].T)
        draw_index = [i+1 for i in draw_index]

        new_state_indicator = [di >= state_log_odds.shape[0] - 1 for di in draw_index]

        return np.array(draw_index), np.array(new_state_indicator)

    def compute_state_odds_given_oracle(self,gamma,nodes,oracle_transition_counts):

        print("Computing oracle odds")

        print(oracle_transition_counts)

        odds = np.ones(oracle_transition_counts.shape[0]+1)
        odds[:-1] += oracle_transition_counts
        odds[-1] += gamma


        counter_odds = np.sum(odds) * np.ones(odds.shape)
        counter_odds -= odds


        fractional_odds = odds/counter_odds

        ## Here we tile the odds across all nodes, because they are uniform
        fractional_odds = np.tile(fractional_odds,(nodes,1)).T

        # print("Oracle Odds Debug")
        # print(odds)
        # print(counter_odds)
        # print(fractional_odds)

        return fractional_odds

    def oracle_indicator_probabilities(self,node_state,live_mask,direct_state_odds_l,direct_state_odds_r,oracle_state_odds_l,oracle_state_odds_r):

        # print("Oracle sampler debug")

        states = direct_state_odds_l.shape[0]
        nodes = node_state.shape[0]
        live_nodes = np.sum(live_mask)
        total_samples = live_mask.shape[0]

        # print(states)
        # print(nodes)
        # print(live_nodes)
        # print(total_samples)

        # print(direct_state_odds_l.shape)
        # print(direct_state_odds_r.shape)


        # print(self.hidden_states)

        state_mask = np.equal(np.tile(np.arange(states),(live_nodes,1)).T,np.tile(node_state[live_mask],(states,1)))

        # print(np.sum(np.sum(state_mask,axis=0) < 1))
        # print(np.sum(np.sum(state_mask,axis=1) < 1))
        #
        # print(node_state[np.sum(state_mask,axis=0) < 1])

        if self.inf_check:
            if np.sum(state_mask,axis=0).shape[0] != live_nodes:
                print((np.sum(state_mask,axis=0) == 0).shape)
                print(live_mask.shape)
                raise Exception("Switch axies dummy")
            if (np.sum(state_mask,axis=0) == 0).any():
                print((np.sum(state_mask,axis=0) == 0).shape)
                print(node_state[live_mask][(np.sum(state_mask,axis=0) == 0)])
                raise Exception("State mask didn't work")

            if (oracle_state_odds_l[:,live_mask] == 0).any():
                raise Exception("Zero oracle state odds")
            if (oracle_state_odds_r[:,live_mask] == 0).any():
                raise Exception("Zero oracle state odds")

        #
        # print("problem_node_state")
        # print(np.tile(np.arange(direct_state_odds_l.shape[0]),(direct_state_odds_l.shape[1],1))[np.sum(state_mask,axis=0) == 0])
        # print(node_state[np.sum(state_mask,axis=0) == 0])

        direct_state_odds_l = direct_state_odds_l[:,live_mask][state_mask]
        direct_state_odds_r = direct_state_odds_r[:,live_mask][state_mask]

        # print(direct_state_odds_l.shape)
        # print(direct_state_odds_r.shape)
        # print(state_mask.shape)
        # print(state_mask.shape)
        # print(oracle_state_odds_l.shape)
        # print(oracle_state_odds_r.shape)

        oracle_state_odds_l = oracle_state_odds_l[:,live_mask][state_mask]
        oracle_state_odds_r = oracle_state_odds_r[:,live_mask][state_mask]

        oracle_probability_l = np.zeros(nodes)
        oracle_probability_r = np.zeros(nodes)

        oracle_probability_l[live_mask] = (oracle_state_odds_l / (oracle_state_odds_l + direct_state_odds_l))
        oracle_probability_r[live_mask] = (oracle_state_odds_r / (oracle_state_odds_r + direct_state_odds_r))

        if self.inf_check:
            if np.isnan(oracle_probability_l).any():
                raise Exception("Nan Oracle probability")
            if np.isnan(oracle_probability_r).any():
                raise Exception("Nan Oracle probability")
            if np.isinf(oracle_probability_l).any():
                raise Exception("Inf Oracle probability")
            if np.isinf(oracle_probability_r).any():
                raise Exception("Inf Oracle probability")

        # print("Oracle Indicator Debug")
        # print(list(oracle_probability_l))
        # print(list(oracle_probability_r))

        # if np.sum(np.isnan(oracle_probability_l[self.live_mask] + np.isnan(oracle_probability_r[self.live_mask]))) > 0:
        #     raise Exception("NaN Oracle Probability")

        return oracle_probability_l,oracle_probability_r

    def sample_oracle_indicator(live_mask,oracle_probability_l,oracle_probability_r):

        oracle_indicator_l = np.random.rand(oracle_probability_l.shape[0]) < oracle_probability_l
        oracle_indicator_r = np.random.rand(oracle_probability_r.shape[0]) < oracle_probability_r

        return oracle_indicator_l,oracle_indicator_r

    def generate_new_states(self,states,new_state_mask):

        print("Labeling new states")
        print(f"Label:{states+1}")
        # total_new_states = np.sum(new_state_mask)
        # return np.arange(states,states+total_new_states)

        return states + 1

    def lr_finite(self,state):
        return expit(np.log2(self.state_raw_emission_counts[state][:,0]/self.state_raw_emission_counts[state][:,1]))

    def state_node_odds(self,state):

        state_mask = self.state_masks[state]

        return self.state_log_odds[state,state_mask]

    def pad_root_transitions(self,raw_counts):
        for node in self.nodes:
            if node.parent is None:
                raw_counts[self.node_states[node.index],0] += 1
        return raw_counts

    def most_likely_parent_to_child(self):

        raw_counts = self.raw_transition_counts()

        raw_counts = self.pad_root_transitions(raw_counts)

        most_likely_transition_matrix = np.zeros((self.hidden_states,self.hidden_states))

        for hidden_state,transitions in enumerate(raw_counts[1:]):
            print(hidden_state)
            print(transitions)
            most_likely_parent = np.argmax(transitions)
            print(most_likely_parent)
            most_likely_transition_matrix[hidden_state+1,most_likely_parent] += 1

        return most_likely_transition_matrix.T

    def raw_transition_counts(self):

        print("Recomputing Transition Counts")

        self.establish_parameters()

        states = self.hidden_states

        transition_mask = self.live_mask

        new_transition_counts = np.zeros((states+1,states+1))

        # transition_mask = live_mask

        for ps,csl in zip(self.node_states[transition_mask],self.child_state_l[transition_mask]):
            # print(csl)
            # print(ps)
            new_transition_counts[csl,ps] += 1

        for ps,csr in zip(self.node_states[transition_mask],self.child_state_r[transition_mask]):
            # print(csr)
            # print(ps)
            new_transition_counts[csr,ps] += 1

        # print(new_transition_counts)

        return new_transition_counts

    def backup(self,location):
        self.pool = None
        with open(location,mode='bw') as f:
            pickle.dump(self,f)

    def reconstitute(location):
        with open(location,mode='br') as f:
            braider = pickle.load(f)
            braider.pool = mp.Pool()
            return braider

    def hidden_state_to_nodes(self,hidden_state):
        node_indices = np.arange(self.total_nodes)[self.state_masks[hidden_state]]
        return [self.nodes[ni] for ni in node_indices]

    def state_lr_samples(self,hidden_state):
        finite = self.lr_finite(hidden_state)
        left = np.arange(self.total_samples)[finite < .5]
        right = np.arange(self.total_samples)[finite >= .5]
        return left,right

    def state_lr_gradient(self,hidden_state):
        return self.state_sample_log_odds[hidden_state]

    #
    #
    #
    # def raw_transition_matrix(self):
    #
    #     states = self.hidden_states
    #
    #     new_transition_counts = np.zeros((states,states))
    #
    #     # transition_mask = np.logical_and(live_mask,np.logical_not(oracle_indicator))
    #
    #     transition_mask = self.live_mask
    #
    #     for ps,csl in zip(self.node_states[transition_mask],self.child_state_l[transition_mask]):
    #         # print(csl)
    #         # print(ps)
    #         new_transition_counts[csl,ps] += 1
    #
    #     for ps,csr in zip(self.node_states[transition_mask],self.child_state_r[transition_mask]):
    #         # print(csr)
    #         # print(ps)
    #         new_transition_counts[csr,ps] += 1
    #
    #     print(new_transition_counts)
    #
    #     return new_transition_counts



    #
    # def resample_node_states(self,transition_odds,sample_odds,node_states,node_divergences,children_l,children_r):
    #
    #     state_log_odds = np.zeros((self.hidden_states,self.total_nodes))
    #
    #     state_log_odds += self.state_log_odds_given_child()







##
